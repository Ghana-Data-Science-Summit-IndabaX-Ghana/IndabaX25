{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28df5579",
   "metadata": {},
   "source": [
    "# **Leveraging LLMs for Text Generation and Summarization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c363790",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "1. [Architectural Overview of LLMs](#architectural-overview-of-llms\")\n",
    "2. [Categories of LLMs](#categories-of-llms)\n",
    "3. [Advanced Text Generation Techniques](#advanced-text-generation-techniques)\n",
    "4. [Parameter Tuning for Different Needs](#parameter-tuning-for-different-needs)\n",
    "5. [Temperature tuning experiment](#temperature-tuning-experiment)\n",
    "6. [Structured output generation](#structured-output-generation)\n",
    "7. [Extractive Summarization](#extractive-summarization)\n",
    "8. [Understanding TF-IDF for Extractive Summarization](#understanding-tf-idf-for-extractive-summarization)\n",
    "8. [Evaluation Metrics for Summarization](#evaluation-metrics-for-summarization)\n",
    "9. [Abstractive Summarization & Control Parameters](#abstractive-summarization--control-parameters)\n",
    "10. [Key Models for Abstractive Summarization](#key-models-for-abstractive-summarization)\n",
    "11. [Controllable Summarization](#controllable-summarization)\n",
    "12. [Building a Multi-Stage Summarizer](#building-a-multi-stage-summarization-pipeline)\n",
    "13. [Multimodal Summarization](#multimodal-summarization)\n",
    "14. [Practical Exercise: Building Your Custom Summarization System](#practical-exercise-building-your-custom-summarization-system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddceaa6",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898b8a3",
   "metadata": {},
   "source": [
    "## Learning Objectives for Part 1:\n",
    "- Understand why LLMs revolutionized summarization\n",
    "- Master in-context learning techniques (zero-shot, few-shot)\n",
    "- Compare foundation models vs specialized approaches through hands-on experiments\n",
    "- Build practical skills for immediate application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504111f",
   "metadata": {},
   "source": [
    "# **Architectural Overview of LLMs**\n",
    "\n",
    "## The Transformer Architecture\n",
    "\n",
    "The transformer architecture revolutionized NLP when introduced in the paper \"Attention is All You Need\" (Vaswanathan et al., 2017).\n",
    "\n",
    "<!-- Transformer Architecture -->\n",
    "<div align=\"center\">\n",
    "<img src='images/transformer architecture.png' alt=\"Transformer architecture\" width=1000 height=700>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Key Components:\n",
    "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different words in context\n",
    "- **Multi-Head Attention**: Parallel attention mechanisms capturing different relationships\n",
    "- **Positional Encoding**: Helps the model understand word order\n",
    "- **Feed-Forward Networks**: Process the representations from attention layers\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "\n",
    "\n",
    "### Why Transformers Excel at Text Tasks,\n",
    "1. **Parallel Processing**: Unlike RNNs, can process entire sequences simultaneously\n",
    "2. **Long-Range Dependencies**: Attention mechanism captures distant relationships\n",
    "3. **Context Awareness**: Each token attends to all other tokens in the sequence\n",
    "4. **Scalability**: Architecture scales well with data and compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b85d1e",
   "metadata": {},
   "source": [
    "# Categories of LLMs\n",
    "\n",
    "LLMs come in different architectural variants, each with strengths for different tasks:\n",
    "\n",
    "## 1. Decoder-Only Models (Autoregressive)\n",
    "- Examples: GPT series, LLaMA, Claude\n",
    "- Trained to predict the next token\n",
    "- **Strengths for summarization**: Creative text generation, coherent narrative\n",
    "- **Weaknesses**: May hallucinate or add information not in source\n",
    "\n",
    "## 2. Encoder-Only Models\n",
    "- Examples: BERT, RoBERTa\n",
    "- Trained on masked language modeling\n",
    "- **Strengths for summarization**: Understanding document context, good for extractive summarization\n",
    "- **Weaknesses**: Not designed for generation\n",
    "\n",
    "## 3. Encoder-Decoder Models\n",
    "- Examples: T5, BART\n",
    "- Trained on sequence-to-sequence tasks\n",
    "- **Strengths for summarization**: Balanced understanding and generation, ideal for abstractive summarization\n",
    "- **Weaknesses**: Larger compute requirements\n",
    "\n",
    "### Which architecture is best for summarization?\n",
    "It depends on the task! We'll explore the tradeoffs throughout this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603ad11",
   "metadata": {},
   "source": [
    "# Advanced Text Generation Techniques\n",
    "\n",
    "### The Anatomy of Effective Prompts\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/co-star.png>\" width=700 height=500>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "| Element       | Description                              |\n",
    "| ------------- | ---------------------------------------- |\n",
    "| **C**ontext   | Provide background information           |\n",
    "| **O**bjective | State the goal of the task               |\n",
    "| **S**tyle     | Specify tone, format, or constraints     |\n",
    "| **T**ask      | What the model should actually do        |\n",
    "| **A**udience  | Who the output is intended for           |\n",
    "| **R**esponse  | Clarify what the output should look like |\n",
    "\n",
    "📌 **Prompt Example:**\n",
    "\n",
    "*Context*: You are a career advisor writing content for a university’s job preparation website. Many students are unsure how to describe their achievements on resumes, particularly in action-result format.\n",
    "\n",
    "*Objective*: Help students craft clear and impressive resume bullet points for internships in data science.\n",
    "\n",
    "*Style*: Keep the tone professional and concise. Use strong action verbs and quantify results wherever possible. Avoid first-person language.\n",
    "\n",
    "*Task*: Based on the details provided, generate 3 resume bullet points that follow best practices in resume writing.\n",
    "\n",
    "*Audience*: Undergraduate students applying for internships in data science roles.\n",
    "\n",
    "*Response*: Your output should be a bulleted list of exactly 3 resume-ready statements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1326a",
   "metadata": {},
   "source": [
    "# Parameter Tuning for Different Needs\n",
    "\n",
    "Understanding how generation parameters affect output quality:\n",
    "\n",
    "- **Temperature (0.0-2.0)**: Controls randomness\n",
    "  - 0.0-0.3: Deterministic, factual content\n",
    "  - 0.4-0.7: Balanced creativity and coherence  \n",
    "  - 0.8-1.2: Creative, varied output\n",
    "  - 1.3+: Highly creative but potentially incoherent\n",
    "\n",
    "- **Top-p (0.0-1.0)**: Nucleus sampling\n",
    "  - Selects the most probable tokens whose cumulative probability exceeds a certain threshold p\n",
    "  - Lower values: More focused, consistent\n",
    "  - Higher values: More diverse vocabulary\n",
    "\n",
    "- **Top-k**: Limits vocabulary to k most likely tokens and samples from it\n",
    "  - Lower values: More predictable\n",
    "  - Higher values: More creative word choices\n",
    "\n",
    "- **Beam-Search**\n",
    "<img src='images/beam-search.jpg' width=900 height=431>\n",
    "  -  Sequence score is cumulative sum of the log probability of every token in the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f29a6",
   "metadata": {},
   "source": [
    "# Temperature tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7de9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('OPENROUTER_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv\n",
    "  load_dotenv()\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import List\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50b30eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemini-2.0-flash-lite-001\"):\n",
    "        self.model = model_name\n",
    "        self.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=self.api_key)\n",
    "\n",
    "    def generate(self, text, temperature=0.8, max_tokens=2000, tools=None):\n",
    "        \"\"\"Generate text using autoregressive model\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                model=self.model,\n",
    "                tool_choice=\"auto\",\n",
    "                tools=tools,\n",
    "                extra_body={},\n",
    "            )\n",
    "            # Extract and return the response\n",
    "            result = response.choices[0].message.content\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "        \n",
    "    def zero_shot_summarize(self, text, instruction=\"Summarize the following text:\"):\n",
    "        \"\"\"Zero-shot summarization with customizable instructions\"\"\"\n",
    "        prompt = f\"{instruction}\\n\\n{text}\\n\\nSummary:\"\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def few_shot_summarize(self, text, examples):\n",
    "        \"\"\"Few-shot summarization with examples\"\"\"\n",
    "        prompt = \"Here are examples of good summaries:\\n\\n\"\n",
    "        \n",
    "        for i, example in enumerate(examples, 1):\n",
    "            prompt += f\"Example {i}:\\n\"\n",
    "            prompt += f\"Text: {example['text']}\\n\"\n",
    "            prompt += f\"Summary: {example['summary']}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"Now summarize this text:\\n{text}\\n\\nSummary:\"\n",
    "        return self.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde75d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parameters(prompt: str, temperatures: List[float], client: LLMClient):\n",
    "\n",
    "    results = []\n",
    "    for temperature in temperatures:\n",
    "        result = client.generate(prompt, temperature=temperature)\n",
    "        results.append(\n",
    "            {\n",
    "                \"temperature\": temperature,\n",
    "                \"response\": result,\n",
    "                \"length\": len(result.split())\n",
    "            }\n",
    "        )   \n",
    "    return results\n",
    "\n",
    "\n",
    "test_prompt = \"Write a creative opening sentence for a science fiction story about time travel.\"\n",
    "temperatures = [0.1, 0.5, 0.9, 1.2]\n",
    "\n",
    "results = compare_parameters(test_prompt, temperatures, LLMClient()),\n",
    "for result in results[0]:\n",
    "    print(f\"Temperature: {result['temperature']}\"),\n",
    "    print(f\"Response: {result['response']}\"),\n",
    "    print(f\"Word count: {result['length']}\"),\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed51980",
   "metadata": {},
   "source": [
    "# Structured output generation\n",
    "\n",
    "Getting LLMs to produce consistent, parseable output formats is crucial for integration with downstream systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d06996",
   "metadata": {},
   "source": [
    "### Prompt based structured generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"name\": \"string\",\n",
    "    \"age\": \"integer\",\n",
    "    \"profession\": \"string\",\n",
    "    \"language\": \"string\",\n",
    "    \"origin\": \"string\"\n",
    "}\n",
    "\n",
    "character_bio = \"\"\"\n",
    "\"Dr. Amara Liu is a 35-year-old astrophysicist from Shanghai. She researches dark matter and enjoys stargazing and sketching. Fluent in Mandarin and English.\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = (\n",
    "    f\"{character_bio}\\n\\n\"\n",
    "    \"Extract the following structured data as JSON:\\n\"\n",
    "    f\"{json.dumps(schema, indent=2)}\\n\\n\"\n",
    "    \"Respond ONLY with valid JSON that matches the schema above.\"\n",
    ")\n",
    "llm_model = LLMClient()\n",
    "response = llm_model.generate(prompt)\n",
    "\n",
    "# Try to extract JSON\n",
    "try:\n",
    "    structured_data = json.loads(response)\n",
    "except json.JSONDecodeError:\n",
    "    import re\n",
    "    match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    structured_data = json.loads(match.group()) if match else {\"error\": \"invalid format\"}\n",
    "\n",
    "print(structured_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db69e1b",
   "metadata": {},
   "source": [
    "### Schema Aware structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3985503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure Output via function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"extract_profile\",\n",
    "            \"description\": \"Extracts a person's profile.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"age\": {\"type\": \"integer\"},\n",
    "                    \"profession\": {\"type\": \"string\"},\n",
    "                    \"language\": {\"type\": \"string\"},\n",
    "                    \"origin\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"name\", \"age\", \"profession\", \"language\", \"origin\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = f\"What is the name, age, profession, language, and origin of the person in the following text:\\n {character_bio}\"\n",
    "response = llm_model.generate(prompt, tools=tools)\n",
    "print(response.choices[0].message.tool_calls[0].function.arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40594c3",
   "metadata": {},
   "source": [
    "## The LLM Revolution in Summarization\n",
    "\n",
    "### Quick Context: What Changed?\n",
    "\n",
    "**Pre-2020**: Building a summarization system required:\n",
    "- Manual feature engineering (TF-IDF, sentence scoring)\n",
    "- Task-specific model training\n",
    "- Domain-specific datasets\n",
    "- Complex evaluation pipelines\n",
    "\n",
    "**Post-2020**: A simple prompt to GPT-3:\n",
    "```\n",
    "\"Summarize this article in 3 sentences: [article]\"\n",
    "```\n",
    "Often outperforms specialized models trained for months.\n",
    "\n",
    "**Research Finding**: Human evaluators prefer GPT-3 summaries over specialized models 77% of the time, despite GPT-3 never being explicitly trained on summarization data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bfed6",
   "metadata": {},
   "source": [
    "<img src=\"images/text summarization.jpg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6ec0c",
   "metadata": {},
   "source": [
    "# Extractive Summarization\n",
    "\n",
    "Extractive summarization selects the most important sentences from the orginal text to create the summary.\n",
    "\n",
    "## The Basic Process:\n",
    "1. Score sentences based on importance\n",
    "2. Select top-scoring sentences using ranking algorithm (TF IDF, SVM, etc)\n",
    "3. Arrange in coherent order (usually original order)\n",
    "\n",
    "## Advantages:\n",
    "- Factually accurate (uses original text)\n",
    "- Computationally efficient\n",
    "- Works well for objective content\n",
    "\n",
    "## Disadvantages:\n",
    "- May be disconnected or redundant\n",
    "- Cannot reformulate or simplify complex content\n",
    "- Limited by quality of source material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093db879",
   "metadata": {},
   "source": [
    "# Understanding TF-IDF for Extractive Summarization\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that reflects how important a word is to a document within a collection of documents. It's widely used in information retrieval and text mining, making it perfect for identifying the most important sentences in a document for summarization.\n",
    "\n",
    "## The Mathematical Foundation\n",
    "\n",
    "### Term Frequency (TF)\n",
    "**Definition**: How frequently a word appears in a specific sentence/document.\n",
    "\n",
    "$$TF(word, sentence) = \\frac{\\text{Number of times word appears in sentence}}{\\text{Total number of words in sentence}}$$\n",
    "\n",
    "**Example**: In the sentence \"The cat sat on the mat\", the word \"the\" has TF = 2/6 = 0.33\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "**Definition**: How rare or common a word is across all sentences in the document.\n",
    "\n",
    "$$IDF(word) = \\log\\left(\\frac{\\text{Total number of sentences}}{\\text{Number of sentences containing the word}}\\right)$$\n",
    "\n",
    "**Intuition**: \n",
    "- Common words (like \"the\", \"and\") appear in many sentences → Low IDF → Less important\n",
    "- Rare words (like \"quantum\", \"photosynthesis\") appear in few sentences → High IDF → More important\n",
    "\n",
    "### TF-IDF Score\n",
    "**Final Formula**:\n",
    "$$TF\\text{-}IDF(word, sentence) = TF(word, sentence) \\times IDF(word)$$\n",
    "\n",
    "**Sentence Score**: Sum of TF-IDF scores for all words in the sentence\n",
    "$$Score(sentence) = \\sum_{word \\in sentence} TF\\text{-}IDF(word, sentence)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef4670",
   "metadata": {},
   "source": [
    "## Visual Example\n",
    "\n",
    "Let's work through a simple example:\n",
    "\n",
    "**Document**: \n",
    "- Sentence 1: \"The cat sat on the mat\"\n",
    "- Sentence 2: \"The dog ran quickly\"  \n",
    "- Sentence 3: \"Cats and dogs are pets\"\n",
    "\n",
    "### Step 1: Calculate TF for each word in Sentence 1\n",
    "\n",
    "| Word | Count in S1 | Total words in S1 | TF |\n",
    "|------|-------------|-------------------|-----|\n",
    "| the  | 2           | 6                 | 0.33|\n",
    "| cat  | 1           | 6                 | 0.17|\n",
    "| sat  | 1           | 6                 | 0.17|\n",
    "| on   | 1           | 6                 | 0.17|\n",
    "| mat  | 1           | 6                 | 0.17|\n",
    "\n",
    "### Step 2: Calculate IDF for each word\n",
    "\n",
    "| Word | Sentences containing word | Total sentences | IDF |\n",
    "|------|--------------------------|----------------|-----|\n",
    "| the  | 2 (S1, S2)              | 3              | log(3/2) = 0.18|\n",
    "| cat  | 1 (S1)                  | 3              | log(3/1) = 0.48|\n",
    "| sat  | 1 (S1)                  | 3              | log(3/1) = 0.48|\n",
    "\n",
    "### Step 3: Calculate TF-IDF scores\n",
    "\n",
    "| Word | TF   | IDF  | TF-IDF |\n",
    "|------|------|------|--------|\n",
    "| the  | 0.33 | 0.18 | 0.06   |\n",
    "| cat  | 0.17 | 0.48 | 0.08   |\n",
    "| sat  | 0.17 | 0.48 | 0.08   |\n",
    "\n",
    "**Sentence 1 Score** = 0.06 + 0.08 + 0.08 + ... = Final Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eed9d4",
   "metadata": {},
   "source": [
    "### Why TF-IDF Works for Summarization\n",
    "\n",
    "### 1. **Identifies Content Words**\n",
    "- Function words (\"the\", \"and\", \"is\") get low scores\n",
    "- Content words (\"algorithm\", \"photosynthesis\", \"democracy\") get high scores\n",
    "\n",
    "### 2. **Balances Frequency and Rarity**\n",
    "- A word mentioned often in one sentence (high TF) but rare overall (high IDF) = very important\n",
    "- A word mentioned everywhere (low IDF) = less distinctive\n",
    "\n",
    "### 3. **Context Awareness**\n",
    "- Same word gets different importance scores in different documents\n",
    "- Adapts to the specific content being summarized\n",
    "\n",
    "### Issues with TF-IDF\n",
    "1. \n",
    "- **Problem**: TF-IDF ignores where sentences appear in the document.\n",
    "- **Solution**: Add position-based scoring.\n",
    "\n",
    "2. \n",
    "- **Problem**: Longer sentences automatically get higher TF-IDF scores.\n",
    "- **Solution**: Normalize by optimal sentence length.\n",
    "\n",
    "3. \n",
    "- **Problem**: Multiple sentences might convey the same information.\n",
    "- **Solution**: Remove semantically similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class ExtractiveSummarizer:\n",
    "    \"\"\"Enhanced extractive summarizer using proper TF-IDF and additional features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and prepare text for processing\"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def extract_sentences(self, text):\n",
    "        \"\"\"Extract and clean sentences\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Filter out very short sentences (less than 5 words)\n",
    "        sentences = [s for s in sentences if len(s.split()) >= 5]\n",
    "        return sentences\n",
    "    \n",
    "    def calculate_tfidf_scores(self, sentences):\n",
    "        \"\"\"Calculate TF-IDF scores for sentences\"\"\"\n",
    "        # Use sklearn's TfidfVectorizer for proper TF-IDF calculation\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)  # Include bigrams for better context\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "            # Sum TF-IDF scores for each sentence\n",
    "            sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "            return sentence_scores\n",
    "        except ValueError:\n",
    "            print('failed to get tf-idf')\n",
    "            # Fallback to simple word frequency if TF-IDF fails\n",
    "\n",
    "    def calculate_position_scores(self, sentences):\n",
    "        \"\"\"Calculate position-based scores (first and last sentences are important)\"\"\"\n",
    "        num_sentences = len(sentences)\n",
    "        position_scores = np.zeros(num_sentences)\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            if i == 0:  # First sentence\n",
    "                position_scores[i] = 0.3\n",
    "            elif i == num_sentences - 1:  # Last sentence\n",
    "                position_scores[i] = 0.2\n",
    "            elif i < num_sentences * 0.1:  # Early sentences\n",
    "                position_scores[i] = 0.15\n",
    "            elif i > num_sentences * 0.9:  # Late sentences\n",
    "                position_scores[i] = 0.1\n",
    "            else:\n",
    "                position_scores[i] = 0.05\n",
    "        \n",
    "        return position_scores\n",
    "                \n",
    "    def calculate_sentence_length_scores(self, sentences):\n",
    "        \"\"\"Normalize scores by sentence length to avoid bias toward longer sentences\"\"\"\n",
    "        length_scores = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            # Optimal sentence length is around 15-25 words\n",
    "            if 15 <= len(words) <= 25:\n",
    "                length_scores.append(1.0)\n",
    "            elif 10 <= len(words) < 15 or 25 < len(words) <= 35:\n",
    "                length_scores.append(0.8)\n",
    "            else:\n",
    "                length_scores.append(0.6)\n",
    "        \n",
    "        return np.array(length_scores)\n",
    "\n",
    "    def remove_redundant_sentences(self, sentences, selected_indices, threshold=0.7):\n",
    "        \"\"\"Remove sentences that are too similar to already selected ones\"\"\"\n",
    "        if len(selected_indices) <= 1:\n",
    "            return selected_indices\n",
    "        \n",
    "        # Use simple word overlap for similarity\n",
    "        filtered_indices = [selected_indices[0]]  # Keep the first (highest scoring)\n",
    "        \n",
    "        for idx in selected_indices[1:]:\n",
    "            current_sentence = sentences[idx]\n",
    "            current_words = set(word.lower() for word in word_tokenize(current_sentence)\n",
    "                              if word.lower() not in self.stop_words and word.isalnum())\n",
    "            \n",
    "            is_redundant = False\n",
    "            for selected_idx in filtered_indices:\n",
    "                selected_sentence = sentences[selected_idx]\n",
    "                selected_words = set(word.lower() for word in word_tokenize(selected_sentence)\n",
    "                                   if word.lower() not in self.stop_words and word.isalnum())\n",
    "                \n",
    "                # Calculate Jaccard similarity\n",
    "                if len(current_words) > 0 and len(selected_words) > 0:\n",
    "                    intersection = current_words.intersection(selected_words)\n",
    "                    union = current_words.union(selected_words)\n",
    "                    similarity = len(intersection) / len(union)\n",
    "                    \n",
    "                    if similarity > threshold:\n",
    "                        is_redundant = True\n",
    "                        break\n",
    "            \n",
    "            if not is_redundant:\n",
    "                filtered_indices.append(idx)\n",
    "        \n",
    "        return filtered_indices\n",
    "    \n",
    "    def summarize(self, text, ratio=0.3, max_sentences=None):\n",
    "        \"\"\"\n",
    "        Generate extractive summary using improved TF-IDF approach\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to summarize\n",
    "            ratio: Proportion of sentences to include (0.1 to 0.5)\n",
    "            max_sentences: Maximum number of sentences (overrides ratio if specified)\n",
    "        \"\"\"\n",
    "        # Preprocess and extract sentences\n",
    "        text = self.preprocess_text(text)\n",
    "        sentences = self.extract_sentences(text)\n",
    "        \n",
    "        if len(sentences) <= 2:\n",
    "            return ' '.join(sentences)\n",
    "        \n",
    "        # Calculate different scoring components\n",
    "        tfidf_scores = self.calculate_tfidf_scores(sentences)\n",
    "        position_scores = self.calculate_position_scores(sentences)\n",
    "        length_scores = self.calculate_sentence_length_scores(sentences)\n",
    "        \n",
    "        # Normalize TF-IDF scores to 0-1 range\n",
    "        if tfidf_scores.max() > 0:\n",
    "            tfidf_scores = tfidf_scores / tfidf_scores.max()\n",
    "        \n",
    "        # Combine scores with weights\n",
    "        final_scores = (\n",
    "            0.7 * tfidf_scores +      # Content importance (70%)\n",
    "            0.2 * position_scores +   # Position importance (20%)\n",
    "            0.1 * length_scores       # Length preference (10%)\n",
    "        )\n",
    "        \n",
    "        # Determine number of sentences to select\n",
    "        if max_sentences:\n",
    "            num_sentences = min(max_sentences, len(sentences))\n",
    "        else:\n",
    "            num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        \n",
    "        # Select top sentences\n",
    "        top_indices = np.argsort(final_scores)[-num_sentences:][::-1]\n",
    "        \n",
    "        # Remove redundant sentences\n",
    "        filtered_indices = self.remove_redundant_sentences(sentences, top_indices)\n",
    "        \n",
    "        # Sort by original order in text\n",
    "        filtered_indices.sort()\n",
    "        \n",
    "        # Create summary\n",
    "        summary_sentences = [sentences[i] for i in filtered_indices]\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        \n",
    "        return summary, {\n",
    "            'selected_indices': filtered_indices,\n",
    "            'scores': final_scores,\n",
    "            'num_original_sentences': len(sentences),\n",
    "            'num_selected_sentences': len(filtered_indices)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Load a Ghanaian news article\n",
    "with open('articles/article.txt', 'r', encoding='utf-8') as f:\n",
    "    article = f.read()\n",
    "    \n",
    "# Print article length\n",
    "print(f\"Article contains {len(sent_tokenize(article))} sentences and {len(article.split())} words\")\n",
    "\n",
    "extractive_summarizer = ExtractiveSummarizer()\n",
    "generated_summary= extractive_summarizer.summarize(article, ratio=0.3)\n",
    "print(f\"\\nExtractive Summary\\n{generated_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LLM summary\n",
    "\n",
    "llm_summary = llm_model.zero_shot_summarize(article)\n",
    "print(\"\\nLLM Zero-Shot Summary:\")\n",
    "print(llm_summary)\n",
    "print(f\"\\nLength: {len(llm_summary.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d43ce2",
   "metadata": {},
   "source": [
    "###  Quick Analysis\n",
    "Compare the two summaries above:\n",
    "1. Which feels more natural to read?\n",
    "2. Which captures the main ideas better?\n",
    "3. Which is more concise while retaining key information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f13ebb",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Summarization\n",
    "\n",
    "How do we know if our summaries are good? Let's implement some common evaluation metrics:\n",
    "\n",
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- Measures overlap between machine-generated summary and reference summary\n",
    "- ROUGE-N: N-gram recall\n",
    "- ROUGE-L: Longest Common Subsequence \n",
    "\n",
    "<img src='images/Rouge L.jpeg' width=700>\n",
    "\n",
    "LCS is the longest set of ordered tokens that occurs in both sequences (Ref, Gen)\n",
    "\n",
    "## BLEU (Bilingual Evaluation Understudy)\n",
    "- Originally designed for translation, but used for summarization\n",
    "- Precision-focused (how many generated n-grams appear in reference)\n",
    "\n",
    "## BERTScore\n",
    "- Uses contextual embeddings to compute similarity\n",
    "- Better semantic understanding than n-gram methods\n",
    "\n",
    "## Human Evaluation Dimensions\n",
    "- **Relevance**: How well does the summary capture the main points?\n",
    "- **Coherence**: Does it flow logically?\n",
    "- **Fluency**: Is it grammatically correct?\n",
    "- **Factuality**: Does it contain errors or hallucinations?\n",
    "- **Accuracy**: Does the summary accurately represent the original content?\n",
    "- **Readability**: Is the summary well-written and easy to understand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom ROUGE socre\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Reference summary for comparison\n",
    "with open(\"articles/reference summary.txt\", 'r', encoding='utf-8') as f:\n",
    "    reference_summary = f.read()\n",
    "\n",
    "\n",
    "def calculate_rouge_n(reference_summary, generated_summary, n):\n",
    "    \"\"\"Calculate ROUGE-N score\"\"\"\n",
    "    # Tokenize into words\n",
    "    ref_tokens = word_tokenize(reference_summary.lower())\n",
    "    cand_tokens = word_tokenize(generated_summary.lower())\n",
    "\n",
    "    # Generate n-grams\n",
    "    ref_ngrams = list(zip(*[ref_tokens[i:] for i in range(n)]))\n",
    "    cand_ngrams = list(zip(*[cand_tokens[i:] for i in range(n)]))\n",
    "    \n",
    "    # Count ngrams\n",
    "    ref_counter = Counter(ref_ngrams)\n",
    "    cand_counter = Counter(cand_ngrams)\n",
    "    \n",
    "    # Count matches\n",
    "    overlap = sum((ref_counter & cand_counter).values()) # intersection of the two counters\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = overlap / max(1, sum(cand_counter.values()))\n",
    "    recall = overlap / max(1, sum(ref_counter.values()))\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * precision * recall / max(1, precision + recall)\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdcaefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple function to evaluate our summaries for ROUGE-1 and ROUGE-2\n",
    "def evaluate_summary(reference, candidate):\n",
    "    \"\"\"Evaluate a summary using multiple metrics\"\"\"\n",
    "    scores = {\n",
    "        'ROUGE-1': calculate_rouge_n(reference, candidate, 1),\n",
    "        'ROUGE-2': calculate_rouge_n(reference, candidate, 2),\n",
    "    }\n",
    "\n",
    "    # Add readability metric: average words per sentence\n",
    "    cand_sentences = sent_tokenize(candidate)\n",
    "    avg_sentence_length = len(word_tokenize(candidate)) / max(1, len(cand_sentences))\n",
    "    scores['Avg Words/Sentence'] = avg_sentence_length\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try evaluating our summary against a reference summary\n",
    "\n",
    "# Evaluate our extractive summary against the reference\n",
    "evaluation_scores = evaluate_summary(reference_summary, generated_summary)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, score in evaluation_scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87d459",
   "metadata": {},
   "source": [
    "# Abstractive Summarization & Control Parameters\n",
    "\n",
    "\n",
    "## What is Abstractive Summarization?\n",
    "\n",
    "Abstractive summarization involves:\n",
    "- Understanding the source content deeply\n",
    "- Identifying key concepts and relationships\n",
    "- Generating new text by paraphrasing existing text\n",
    "- Condensing information in ways that extraction cannot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c271d",
   "metadata": {},
   "source": [
    "# Controllable Summarization\n",
    "\n",
    "One of the major advantages of modern summarization systems is the ability to control various aspects of the generated summaries:\n",
    "\n",
    "## Common Control Parameters:\n",
    "\n",
    "1. **Length**: Controlling how long or short the summary should be\n",
    "2. **Style**: Formal vs. casual, simple vs. technical\n",
    "3. **Focus**: Emphasizing particular topics or aspects\n",
    "4. **Structure**: Bullet points, narrative, or question-answering\n",
    "\n",
    "## How to Implement Controls:\n",
    "\n",
    "1. **Model-specific parameters**: Using built-in generation controls\n",
    "2. **Prompt engineering**: Adding instructional prefixes\n",
    "3. **Output filtering**: Post-processing generated summaries\n",
    "4. **Fine-tuning**: Training the model with examples of desired style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96331874",
   "metadata": {},
   "source": [
    "Prompt Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc92288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_prompts():\n",
    "    \"\"\"Experiment with different instruction styles\"\"\"\n",
    "    \n",
    "    instructions = {\n",
    "        \"basic\": \"Summarize the following text:\",\n",
    "        \"concise\": \"Provide a concise summary of the main points:\",\n",
    "        \"detailed\": \"Write a comprehensive summary covering all key aspects:\",\n",
    "        \"bullet\": \"Summarize the key points as bullet points:\",\n",
    "        \"executive\": \"Write an executive summary focusing on implications and next steps:\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for style, instruction in instructions.items():\n",
    "        print(f\"\\n--- {style.upper()} STYLE ---\")\n",
    "        summary = llm_model.zero_shot_summarize(article, instruction)\n",
    "        results[style] = summary\n",
    "        print(f\"Instruction: {instruction}\")\n",
    "        print(f\"Summary: {summary}\")\n",
    "        print(f\"Length: {len(summary.split())} words\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "instruction_results = experiment_with_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e25ba",
   "metadata": {},
   "source": [
    "Fewshot vs Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8822e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create few-shot examples for different domains\n",
    "news_examples = [\n",
    "    {\n",
    "        \"text\": \"Scientists at MIT have developed a new battery technology that could revolutionize electric vehicles. The lithium-metal batteries can charge to 80% capacity in just 10 minutes and last for over 500,000 miles. The breakthrough addresses two major concerns about electric vehicles: charging time and battery longevity. Commercial production is expected to begin in 2026.\",\n",
    "        \"summary\": \"MIT scientists created fast-charging, long-lasting batteries for electric vehicles, addressing key concerns about charging time and durability, with commercial production planned for 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The Federal Reserve announced a 0.25% interest rate cut today, citing concerns about slowing economic growth and inflation falling below target levels. This marks the third rate cut this year. Stock markets responded positively, with the S&P 500 gaining 2.1% in after-hours trading. Economists predict this could stimulate business investment and consumer spending.\",\n",
    "        \"summary\": \"The Federal Reserve cut interest rates by 0.25% due to economic concerns, prompting positive market reactions and expectations of increased business and consumer activity.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "research_examples = [\n",
    "    {\n",
    "        \"text\": \"This study examined the effects of meditation on stress hormones in 200 participants over 8 weeks. Participants who meditated daily showed a 23% reduction in cortisol levels compared to the control group. The research also found improvements in sleep quality and self-reported well-being. These findings suggest meditation could be an effective intervention for stress management.\",\n",
    "        \"summary\": \"An 8-week study of 200 participants found daily meditation reduced stress hormone levels by 23% and improved sleep and well-being, supporting meditation as a stress management tool.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def compare_few_shot_vs_zero_shot(text, examples):\n",
    "    \"\"\"Compare zero-shot and few-shot performance\"\"\"\n",
    "    \n",
    "    # Zero-shot\n",
    "    zero_shot = llm_model.zero_shot_summarize(text)\n",
    "    \n",
    "    # Few-shot\n",
    "    few_shot = llm_model.few_shot_summarize(text, examples)\n",
    "    \n",
    "    print(\"ZERO-SHOT SUMMARY:\")\n",
    "    print(zero_shot)\n",
    "    print(f\"Length: {len(zero_shot.split())} words\\n\")\n",
    "    \n",
    "    print(\"FEW-SHOT SUMMARY:\")\n",
    "    print(few_shot)\n",
    "    print(f\"Length: {len(few_shot.split())} words\\n\")\n",
    "    \n",
    "    return zero_shot, few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a new research article\n",
    "new_research = \"\"\"\n",
    "A comprehensive analysis of social media usage patterns among teenagers reveals concerning trends in mental health outcomes. The study, following 1,500 participants aged 13-18 over two years, found that teens spending more than 3 hours daily on social platforms showed increased rates of anxiety and depression. Particularly concerning was the correlation between late-night social media use and sleep disorders. However, the research also identified positive outcomes, including enhanced social connections and access to mental health resources. The researchers recommend implementing digital wellness programs in schools and encouraging mindful social media usage rather than complete avoidance.\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_result, few_shot_result = compare_few_shot_vs_zero_shot(new_research, research_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42a6d6",
   "metadata": {},
   "source": [
    "###  Discussion Point\n",
    "After running the above experiment:\n",
    "1. Which summary feels more consistent with the examples provided?\n",
    "2. How did the few-shot examples influence the style and content selection?\n",
    "3. What are the trade-offs between zero-shot (faster) and few-shot (more controlled)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4399137",
   "metadata": {},
   "source": [
    "# Part II: Advanced Prompting Strategies & Controllable Summarization\n",
    "\n",
    "## Learning Objectives for Part 2:\n",
    "- Master advanced prompting techniques (chain-of-thought, role-playing, structured output)\n",
    "- Implement controllable summarization for different audiences and formats\n",
    "- Handle long documents and multi-document scenarios\n",
    "- Build robust evaluation frameworks using modern techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919154b",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques\n",
    "\n",
    "### Chain-of-Thought (CoT) for Complex Summarization\n",
    "\n",
    "When dealing with complex documents with multiple topics or intricate relationships, breaking down the summarization process helps LLMs produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0934ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSummarizer(LLMSummarizer):\n",
    "    \"\"\"Extended summarizer with advanced prompting capabilities\"\"\"\n",
    "    \n",
    "    def chain_of_thought_summarize(self, text, focus_areas=None):\n",
    "        \"\"\"Use step-by-step reasoning for complex summarization\"\"\"\n",
    "        \n",
    "        if focus_areas is None:\n",
    "            focus_areas = [\"main topic\", \"key findings\", \"implications\"]\n",
    "        \n",
    "        cot_prompt = f\"\"\"\n",
    "I need to summarize this complex text systematically. Let me break this down step by step:\n",
    "\n",
    "1. First, I'll identify the main topics and themes\n",
    "2. Then, I'll extract the key points for each theme\n",
    "3. Next, I'll identify relationships between different points\n",
    "4. Finally, I'll synthesize this into a coherent summary\n",
    "\n",
    "Text to analyze:\n",
    "{text}\n",
    "\n",
    "Let me work through this step by step:\n",
    "\n",
    "Step 1 - Main topics I can identify:\n",
    "\"\"\"\n",
    "        \n",
    "        return self.generate(cot_prompt, max_tokens=500)\n",
    "    \n",
    "    def role_based_summarize(self, text, role=\"expert\", audience=\"general\"):\n",
    "        \"\"\"Summarize from a specific role perspective\"\"\"\n",
    "        \n",
    "        role_prompts = {\n",
    "            \"expert\": f\"As a domain expert, provide a technical summary for {audience} audience:\",\n",
    "            \"journalist\": f\"As a journalist, write a news summary for {audience} readers:\",\n",
    "            \"teacher\": f\"As an educator, explain this clearly for {audience} students:\",\n",
    "            \"consultant\": f\"As a business consultant, provide strategic insights for {audience}:\",\n",
    "            \"researcher\": f\"As a researcher, highlight methodology and findings for {audience}:\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"{role_prompts.get(role, role_prompts['expert'])}\\n\\n{text}\\n\\nSummary:\"\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def structured_summarize(self, text, structure=\"executive\"):\n",
    "        \"\"\"Generate structured summaries with specific formats\"\"\"\n",
    "        \n",
    "        structures = {\n",
    "            \"executive\": \"\"\"\n",
    "Create an executive summary with exactly these sections:\n",
    "- Executive Summary (2-3 sentences)\n",
    "- Key Findings (3-4 bullet points)\n",
    "- Recommendations (2-3 bullet points)\n",
    "- Next Steps (1-2 bullet points)\n",
    "\"\"\",\n",
    "            \"scientific\": \"\"\"\n",
    "Summarize in scientific paper format:\n",
    "- Background & Objective (1 sentence)\n",
    "- Methods (1 sentence)\n",
    "- Results (2-3 sentences)\n",
    "- Conclusions (1-2 sentences)\n",
    "\"\"\",\n",
    "            \"news\": \"\"\"\n",
    "Create a news summary with:\n",
    "- Lead (Who, What, When, Where in first sentence)\n",
    "- Key Details (2-3 supporting facts)\n",
    "- Context/Background (1-2 sentences)\n",
    "- Impact/Implications (1 sentence)\n",
    "\"\"\",\n",
    "            \"meeting\": \"\"\"\n",
    "Format as meeting minutes:\n",
    "- Key Decisions Made\n",
    "- Action Items Assigned\n",
    "- Topics Discussed\n",
    "- Next Meeting/Follow-up\n",
    "\"\"\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"{structures.get(structure, structures['executive'])}\\n\\nText:\\n{text}\\n\\nStructured Summary:\"\n",
    "        return self.generate(prompt, max_tokens=400)\n",
    "\n",
    "# Initialize advanced summarizer\n",
    "advanced_summarizer = AdvancedSummarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456325e5",
   "metadata": {},
   "source": [
    "###  Experiment 1: Chain-of-Thought vs Direct Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex multi-topic article for testing\n",
    "with open(\"articles/complex_article.txt\", 'r', encoding='utf-8') as f:\n",
    "    complex_article = f.read()\n",
    "\n",
    "def compare_cot_vs_direct(text):\n",
    "    \"\"\"Compare chain-of-thought vs direct summarization\"\"\"\n",
    "    \n",
    "    print(\"🔗 CHAIN-OF-THOUGHT APPROACH:\")\n",
    "    print(\"=\" * 50)\n",
    "    cot_summary = advanced_summarizer.chain_of_thought_summarize(text)\n",
    "    print(cot_summary)\n",
    "    print(f\"\\nLength: {len(cot_summary.split())} words\")\n",
    "    \n",
    "    print(\"\\n📝 DIRECT APPROACH:\")\n",
    "    print(\"=\" * 50)\n",
    "    direct_summary = llm_model.zero_shot_summarize(\n",
    "        text, \n",
    "        \"Provide a comprehensive summary of this article covering all main points:\"\n",
    "    )\n",
    "    print(direct_summary)\n",
    "    print(f\"\\nLength: {len(direct_summary.split())} words\")\n",
    "    \n",
    "    return cot_summary, direct_summary\n",
    "\n",
    "cot_result, direct_result = compare_cot_vs_direct(complex_article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fa9a2",
   "metadata": {},
   "source": [
    "###  Analysis: Which Approach Worked Better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_summary_quality(cot_summary, direct_summary, original_text):\n",
    "    \"\"\"Analyze different aspects of summary quality\"\"\"\n",
    "    \n",
    "    # Extract key topics from original (simple keyword extraction)\n",
    "    import re\n",
    "    \n",
    "    key_topics = [\n",
    "        \"artificial intelligence\", \"AI\", \"healthcare\", \"hospitals\", \"diagnostic\",\n",
    "        \"privacy\", \"FDA\", \"economic\", \"costs\", \"personalized medicine\", \n",
    "        \"ethical\", \"bias\", \"demographics\"\n",
    "    ]\n",
    "    \n",
    "    def count_topic_coverage(summary, topics):\n",
    "        summary_lower = summary.lower()\n",
    "        covered = [topic for topic in topics if topic.lower() in summary_lower]\n",
    "        return len(covered), covered\n",
    "    \n",
    "    # Analyze both summaries\n",
    "    cot_coverage, cot_topics = count_topic_coverage(cot_summary, key_topics)\n",
    "    direct_coverage, direct_topics = count_topic_coverage(direct_summary, key_topics)\n",
    "    \n",
    "    analysis = {\n",
    "        \"Approach\": [\"Chain-of-Thought\", \"Direct\"],\n",
    "        \"Word Count\": [len(cot_summary.split()), len(direct_summary.split())],\n",
    "        \"Topic Coverage\": [f\"{cot_coverage}/{len(key_topics)}\", f\"{direct_coverage}/{len(key_topics)}\"],\n",
    "        \"Structure Clarity\": [\"High\" if \"step\" in cot_summary.lower() else \"Medium\", \"Medium\"],\n",
    "        \"Reasoning Visible\": [\"Yes\" if \"identify\" in cot_summary.lower() or \"step\" in cot_summary.lower() else \"No\", \"No\"]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(analysis)\n",
    "    print(\" SUMMARY QUALITY COMPARISON:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n Topics covered by CoT: {cot_topics}\")\n",
    "    print(f\" Topics covered by Direct: {direct_topics}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "quality_analysis = analyze_summary_quality(cot_result, direct_result, complex_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067178f",
   "metadata": {},
   "source": [
    "## Controllable Summarization: Audience and Purpose\n",
    "\n",
    "### Role-Based Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_role_based_summarization():\n",
    "    \"\"\"Show how different roles produce different summaries\"\"\"\n",
    "    \n",
    "    roles_and_audiences = [\n",
    "        (\"journalist\", \"general public\"),\n",
    "        (\"researcher\", \"academic peers\"),\n",
    "        (\"consultant\", \"business executives\"),\n",
    "        (\"teacher\", \"high school students\")\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for role, audience in roles_and_audiences:\n",
    "        print(f\"\\n👤 ROLE: {role.upper()} | AUDIENCE: {audience.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        summary = advanced_summarizer.role_based_summarize(\n",
    "            complex_article, \n",
    "            role=role, \n",
    "            audience=audience\n",
    "        )\n",
    "        \n",
    "        results[f\"{role}_{audience}\"] = summary\n",
    "        print(summary)\n",
    "        print(f\"Length: {len(summary.split())} words\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "role_based_results = demonstrate_role_based_summarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea4c02",
   "metadata": {},
   "source": [
    "###  Exercise: Audience Adaptation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def analyze_audience_adaptation(results):\n",
    "    \"\"\"Analyze how summaries adapt to different audiences\"\"\"\n",
    "    \n",
    "    # Define audience-specific characteristics to look for\n",
    "    characteristics = {\n",
    "        \"technical_terms\": [\"AI\", \"algorithm\", \"diagnostic\", \"optimization\", \"regulatory\"],\n",
    "        \"business_terms\": [\"investment\", \"ROI\", \"market\", \"revenue\", \"cost savings\", \"efficiency\"],\n",
    "        \"simple_language\": [\"help\", \"better\", \"improve\", \"save money\", \"easier\"],\n",
    "        \"educational_elements\": [\"learn\", \"understand\", \"example\", \"means\", \"because\"]\n",
    "    }\n",
    "    \n",
    "    analysis_results = []\n",
    "    \n",
    "    for summary_key, summary in results.items():\n",
    "        role, audience = summary_key.split('_', 1)\n",
    "        summary_lower = summary.lower()\n",
    "        \n",
    "        char_counts = {}\n",
    "        for char_type, terms in characteristics.items():\n",
    "            count = sum(1 for term in terms if term.lower() in summary_lower)\n",
    "            char_counts[char_type] = count\n",
    "        \n",
    "        analysis_results.append({\n",
    "            'role': role,\n",
    "            'audience': audience.replace('_', ' '),\n",
    "            'word_count': len(summary.split()),\n",
    "            'technical_terms': char_counts['technical_terms'],\n",
    "            'business_terms': char_counts['business_terms'],\n",
    "            'simple_language': char_counts['simple_language'],\n",
    "            'educational_elements': char_counts['educational_elements']\n",
    "        })\n",
    "    \n",
    "    adaptation_df = pd.DataFrame(analysis_results)\n",
    "    print(\"🎭 AUDIENCE ADAPTATION ANALYSIS:\")\n",
    "    print(adaptation_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize adaptation patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Summary Characteristics by Role and Audience', fontsize=16)\n",
    "    \n",
    "    # Technical terms usage\n",
    "    axes[0,0].bar(range(len(adaptation_df)), adaptation_df['technical_terms'])\n",
    "    axes[0,0].set_title('Technical Terms Usage')\n",
    "    axes[0,0].set_xticks(range(len(adaptation_df)))\n",
    "    axes[0,0].set_xticklabels([f\"{row['role']}\\n({row['audience']})\" for _, row in adaptation_df.iterrows()], rotation=45)\n",
    "    \n",
    "    # Business terms usage\n",
    "    axes[0,1].bar(range(len(adaptation_df)), adaptation_df['business_terms'])\n",
    "    axes[0,1].set_title('Business Terms Usage')\n",
    "    axes[0,1].set_xticks(range(len(adaptation_df)))\n",
    "    axes[0,1].set_xticklabels([f\"{row['role']}\\n({row['audience']})\" for _, row in adaptation_df.iterrows()], rotation=45)\n",
    "    \n",
    "    # Simple language usage\n",
    "    axes[1,0].bar(range(len(adaptation_df)), adaptation_df['simple_language'])\n",
    "    axes[1,0].set_title('Simple Language Usage')\n",
    "    axes[1,0].set_xticks(range(len(adaptation_df)))\n",
    "    axes[1,0].set_xticklabels([f\"{row['role']}\\n({row['audience']})\" for _, row in adaptation_df.iterrows()], rotation=45)\n",
    "    \n",
    "    # Word count comparison\n",
    "    axes[1,1].bar(range(len(adaptation_df)), adaptation_df['word_count'])\n",
    "    axes[1,1].set_title('Summary Length')\n",
    "    axes[1,1].set_xticks(range(len(adaptation_df)))\n",
    "    axes[1,1].set_xticklabels([f\"{row['role']}\\n({row['audience']})\" for _, row in adaptation_df.iterrows()], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return adaptation_df\n",
    "\n",
    "audience_analysis = analyze_audience_adaptation(role_based_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401cd76",
   "metadata": {},
   "source": [
    "###  Building Structured Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_structured_outputs():\n",
    "    \"\"\"Show different structured output formats\"\"\"\n",
    "    \n",
    "    # Use a business-focused article for this demo\n",
    "    business_article = \"\"\"\n",
    "    TechCorp announced its Q3 earnings today, reporting revenue of $2.8 billion, up 15% from the previous quarter. The company's cloud services division led growth with 32% year-over-year increase, while its traditional software licensing revenue declined by 8%. \n",
    "    \n",
    "    CEO Sarah Johnson highlighted the successful launch of their AI-powered analytics platform, which has already attracted 50,000 enterprise customers in its first month. The platform uses machine learning to help businesses optimize their operations and reduce costs by an average of 18%.\n",
    "    \n",
    "    However, the company faces challenges in the competitive landscape, with new entrants offering similar services at lower prices. TechCorp plans to invest $500 million in R&D next year to maintain its technological edge. The company also announced plans to acquire DataInsights Inc., a startup specializing in real-time data processing, for $150 million.\n",
    "    \n",
    "    Looking ahead, TechCorp expects Q4 revenue to reach $3.1 billion, driven by increased adoption of cloud services and the holiday shopping season. The company raised its full-year guidance to $11.2 billion, exceeding analyst expectations of $10.8 billion.\n",
    "    \"\"\"\n",
    "    \n",
    "    structures = [\"executive\", \"news\", \"meeting\"]\n",
    "    structured_results = {}\n",
    "    \n",
    "    for structure in structures:\n",
    "        print(f\"\\n📋 {structure.upper()} FORMAT:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        summary = advanced_summarizer.structured_summarize(business_article, structure)\n",
    "        structured_results[structure] = summary\n",
    "        print(summary)\n",
    "    \n",
    "    return structured_results\n",
    "\n",
    "structured_results = demonstrate_structured_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ae0db",
   "metadata": {},
   "source": [
    "## Handling Long Documents\n",
    "\n",
    "### Recursive Summarization Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c208435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongDocumentSummarizer(AdvancedSummarizer):\n",
    "    \"\"\"Specialized summarizer for handling long documents\"\"\"\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=1000, overlap=100):\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            if i + chunk_size >= len(words):\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def recursive_summarize(self, text, target_length=200, max_iterations=3):\n",
    "        \"\"\"Recursively summarize long text\"\"\"\n",
    "        \n",
    "        print(f\"📄 Starting recursive summarization...\")\n",
    "        print(f\"Original length: {len(text.split())} words\")\n",
    "        print(f\"Target length: {target_length} words\")\n",
    "        \n",
    "        current_text = text\n",
    "        iteration = 0\n",
    "        \n",
    "        while len(current_text.split()) > target_length and iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"\\n🔄 Iteration {iteration}:\")\n",
    "            \n",
    "            # If text is still very long, chunk it first\n",
    "            if len(current_text.split()) > 2000:\n",
    "                print(\"  📝 Chunking large text...\")\n",
    "                chunks = self.chunk_text(current_text, chunk_size=800, overlap=50)\n",
    "                print(f\"  Created {len(chunks)} chunks\")\n",
    "                \n",
    "                # Summarize each chunk\n",
    "                chunk_summaries = []\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    print(f\"  Processing chunk {i+1}/{len(chunks)}...\")\n",
    "                    chunk_summary = self.zero_shot_summarize(\n",
    "                        chunk, \n",
    "                        \"Summarize the key points from this text section:\"\n",
    "                    )\n",
    "                    chunk_summaries.append(chunk_summary)\n",
    "                \n",
    "                # Combine chunk summaries\n",
    "                current_text = ' '.join(chunk_summaries)\n",
    "                print(f\"  Combined chunks: {len(current_text.split())} words\")\n",
    "            \n",
    "            # Final summarization pass\n",
    "            if len(current_text.split()) > target_length:\n",
    "                print(\"  📋 Final summarization pass...\")\n",
    "                current_text = self.zero_shot_summarize(\n",
    "                    current_text,\n",
    "                    f\"Create a comprehensive summary of approximately {target_length} words:\"\n",
    "                )\n",
    "                print(f\"  Result: {len(current_text.split())} words\")\n",
    "        \n",
    "        print(f\"\\n✅ Final summary: {len(current_text.split())} words\")\n",
    "        return current_text\n",
    "    \n",
    "    def hierarchical_summarize(self, text, levels=[\"detailed\", \"medium\", \"brief\"]):\n",
    "        \"\"\"Create multiple summary levels\"\"\"\n",
    "        \n",
    "        level_configs = {\n",
    "            \"detailed\": {\"length\": 400, \"instruction\": \"Provide a detailed summary covering all major points:\"},\n",
    "            \"medium\": {\"length\": 200, \"instruction\": \"Create a balanced summary of key points:\"},\n",
    "            \"brief\": {\"length\": 100, \"instruction\": \"Write a concise summary of main ideas:\"},\n",
    "            \"executive\": {\"length\": 50, \"instruction\": \"Provide an executive summary in 2-3 sentences:\"}\n",
    "        }\n",
    "        \n",
    "        summaries = {}\n",
    "        \n",
    "        for level in levels:\n",
    "            if level in level_configs:\n",
    "                config = level_configs[level]\n",
    "                print(f\"\\n📊 Creating {level} summary (target: ~{config['length']} words)...\")\n",
    "                \n",
    "                summary = self.recursive_summarize(text, target_length=config['length'])\n",
    "                summaries[level] = summary\n",
    "                \n",
    "                print(f\"✅ {level.capitalize()} summary ({len(summary.split())} words):\")\n",
    "                print(summary[:200] + \"...\" if len(summary) > 200 else summary)\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "# Initialize long document summarizer\n",
    "long_doc_summarizer = LongDocumentSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test recursive summarization\n",
    "print(\"\\n🔄 Testing Recursive Summarization:\")\n",
    "recursive_result = long_doc_summarizer.recursive_summarize(long_document, target_length=150)\n",
    "\n",
    "# Test hierarchical summarization\n",
    "print(\"\\n📊 Testing Hierarchical Summarization:\")\n",
    "with open(\"articles/long_document.txt\", 'r', encoding='utf-8') as f:\n",
    "    long_document = f.read()\n",
    "    \n",
    "hierarchical_results = long_doc_summarizer.hierarchical_summarize(\n",
    "    long_document, \n",
    "    levels=[\"detailed\", \"medium\", \"brief\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3dcaf",
   "metadata": {},
   "source": [
    "## Multi-Document Summarization\n",
    "\n",
    "### Comparative and Synthesis Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a314689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDocumentSummarizer(AdvancedSummarizer):\n",
    "    \"\"\"Summarizer for handling multiple related documents\"\"\"\n",
    "    \n",
    "    def comparative_summarize(self, documents, document_labels=None):\n",
    "        \"\"\"Compare and contrast multiple documents\"\"\"\n",
    "        \n",
    "        if document_labels is None:\n",
    "            document_labels = [f\"Document {i+1}\" for i in range(len(documents))]\n",
    "        \n",
    "        # Create comparative prompt\n",
    "        comparative_prompt = \"\"\"\n",
    "        I need to analyze and compare multiple documents on related topics. \n",
    "        Please provide a comparative summary that:\n",
    "        1. Identifies common themes across documents\n",
    "        2. Highlights key differences in perspectives or findings\n",
    "        3. Synthesizes the most important information\n",
    "        4. Notes any contradictions or conflicting information\n",
    "        \n",
    "        Documents to compare:\n",
    "        \"\"\"\n",
    "        \n",
    "        for label, doc in zip(document_labels, documents):\n",
    "            comparative_prompt += f\"\\n\\n{label}:\\n{doc}\"\n",
    "        \n",
    "        comparative_prompt += \"\\n\\nComparative Summary:\"\n",
    "        \n",
    "        return self.generate(comparative_prompt, max_tokens=500)\n",
    "    \n",
    "    def synthesis_summarize(self, documents, focus_question=None):\n",
    "        \"\"\"Synthesize information from multiple sources\"\"\"\n",
    "        \n",
    "        if focus_question is None:\n",
    "            focus_question = \"What are the key insights when considering all sources together?\"\n",
    "        \n",
    "        synthesis_prompt = f\"\"\"\n",
    "        Synthesize information from the following sources to answer: {focus_question}\n",
    "        \n",
    "        Please create a synthesis that:\n",
    "        - Integrates information from all sources\n",
    "        - Identifies patterns and trends\n",
    "        - Resolves or notes conflicting information\n",
    "        - Provides a coherent unified perspective\n",
    "        \n",
    "        Sources:\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            synthesis_prompt += f\"\\n\\nSource {i}:\\n{doc}\"\n",
    "        \n",
    "        synthesis_prompt += f\"\\n\\nSynthesis addressing '{focus_question}':\"\n",
    "        \n",
    "        return self.generate(synthesis_prompt, max_tokens=500)\n",
    "\n",
    "# Initialize multi-document summarizer\n",
    "multi_doc_summarizer = MultiDocumentSummarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e22cce",
   "metadata": {},
   "source": [
    "###  Multi-Document Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi_document_summarization():\n",
    "    \"\"\"Test multi-document summarization with different perspectives\"\"\"\n",
    "    \n",
    "    # Three documents with different perspectives on remote work\n",
    "    documents = [\n",
    "        \"\"\"\n",
    "        Study A: Remote Work Productivity Analysis\n",
    "        A comprehensive study of 5,000 employees across 50 companies found that remote workers are 13% more productive than their office counterparts. The research, conducted over 18 months, measured productivity through completed tasks, project deadlines met, and output quality scores. Remote workers reported higher job satisfaction (8.2/10 vs 7.1/10) and better work-life balance. However, the study noted challenges in spontaneous collaboration and team building. Companies with strong digital infrastructure saw the highest productivity gains, while those with poor remote work policies experienced productivity declines.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Study B: Remote Work Challenges and Solutions\n",
    "        Research focusing on management perspectives reveals significant challenges in remote work implementation. Surveys of 1,200 managers indicate concerns about employee oversight (67%), team cohesion (58%), and maintaining company culture (52%). The study found that productivity varies significantly by role type, with creative and collaborative roles showing 8% decreased output while individual contributor roles improved by 12%. Communication frequency increased by 35% in remote teams, but decision-making speed decreased by 23%. Companies investing in management training for remote leadership saw better outcomes.\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Study C: Economic Impact of Remote Work\n",
    "        Economic analysis of remote work trends shows substantial cost savings for both employers and employees. Companies report average savings of $11,000 per remote employee annually through reduced office space, utilities, and facility costs. Employees save an average of $4,000 yearly on commuting, work clothing, and meals. However, the analysis reveals increased spending on home office equipment and higher utility bills for workers. The real estate market has been significantly impacted, with commercial office space demand down 30% in major cities while residential markets in suburban areas have seen 15% price increases. The shift has created an estimated $1.2 trillion economic redistribution.\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    labels = [\"Productivity Study\", \"Management Perspective\", \"Economic Analysis\"]\n",
    "    \n",
    "    print(\"📑 COMPARATIVE ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    comparative_result = multi_doc_summarizer.comparative_summarize(documents, labels)\n",
    "    print(comparative_result)\n",
    "    \n",
    "    print(\"\\n🔬 SYNTHESIS FOR POLICY MAKERS:\")\n",
    "    print(\"=\" * 60)\n",
    "    synthesis_result = multi_doc_summarizer.synthesis_summarize(\n",
    "        documents, \n",
    "        \"What policy recommendations emerge from considering all three studies?\"\n",
    "    )\n",
    "    print(synthesis_result)\n",
    "    \n",
    "    return comparative_result, synthesis_result\n",
    "\n",
    "comparative_analysis, synthesis_analysis = test_multi_document_summarization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de361c6",
   "metadata": {},
   "source": [
    "## Modern Evaluation Techniques\n",
    "\n",
    "### Beyond ROUGE: Human-Aligned Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ff256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernEvaluator:\n",
    "    \"\"\"Modern evaluation methods for summarization quality\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_summarizer):\n",
    "        self.summarizer = llm_summarizer\n",
    "    \n",
    "    def llm_as_judge(self, summary, original_text, criteria=None):\n",
    "        \"\"\"Use LLM to judge summary quality\"\"\"\n",
    "        \n",
    "        if criteria is None:\n",
    "            criteria = [\"accuracy\", \"completeness\", \"clarity\", \"conciseness\"]\n",
    "        \n",
    "        criteria_descriptions = {\n",
    "            \"accuracy\": \"Contains no false information or hallucinations\",\n",
    "            \"completeness\": \"Covers all important points from the original\",\n",
    "            \"clarity\": \"Easy to understand and well-written\",\n",
    "            \"conciseness\": \"Appropriate length without unnecessary detail\",\n",
    "            \"coherence\": \"Logical flow and good organization\",\n",
    "            \"relevance\": \"Focuses on the most important information\"\n",
    "        }\n",
    "        \n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Evaluate this summary based on the criteria below. Rate each criterion from 1-5 (5 being excellent).\n",
    "        \n",
    "        Original Text:\n",
    "        {original_text}\n",
    "        \n",
    "        Summary to Evaluate:\n",
    "        {summary}\n",
    "        \n",
    "        Evaluation Criteria:\n",
    "        \"\"\"\n",
    "        \n",
    "        for criterion in criteria:\n",
    "            desc = criteria_descriptions.get(criterion, \"Quality assessment\")\n",
    "            evaluation_prompt += f\"\\n- {criterion.capitalize()}: {desc}\"\n",
    "        \n",
    "        evaluation_prompt += \"\"\"\n",
    "        \n",
    "        Please provide:\n",
    "        1. A score (1-5) for each criterion\n",
    "        2. Brief explanation for each score\n",
    "        3. Overall assessment\n",
    "        4. Specific suggestions for improvement\n",
    "        \n",
    "        Evaluation:\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.summarizer.generate(evaluation_prompt, max_tokens=400)\n",
    "    \n",
    "    def factuality_check(self, summary, source_text):\n",
    "        \"\"\"Check summary for factual accuracy\"\"\"\n",
    "        \n",
    "        factuality_prompt = f\"\"\"\n",
    "        Check if this summary contains any factual errors or information not present in the source.\n",
    "        \n",
    "        Source Text:\n",
    "        {source_text}\n",
    "        \n",
    "        Summary to Check:\n",
    "        {summary}\n",
    "        \n",
    "        Please identify:\n",
    "        1. Any statements that contradict the source\n",
    "        2. Any information added that's not in the source  \n",
    "        3. Any numbers, dates, or names that are incorrect\n",
    "        4. Overall factual accuracy rating (1-5)\n",
    "        \n",
    "        Factuality Assessment:\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.summarizer.generate(factuality_prompt, max_tokens=300)\n",
    "    \n",
    "\n",
    "    def comparative_evaluation(self, summaries, source_text, labels=None):\n",
    "        \"\"\"Compare multiple summaries of the same source\"\"\"\n",
    "        \n",
    "        if labels is None:\n",
    "            labels = [f\"Summary {i+1}\" for i in range(len(summaries))]\n",
    "        \n",
    "        comparison_prompt = f\"\"\"\n",
    "        Compare these summaries of the same source text and rank them by quality.\n",
    "        \n",
    "        Source Text:\n",
    "        {source_text}\n",
    "        \n",
    "        Summaries to Compare:\n",
    "        \"\"\"\n",
    "        \n",
    "        for label, summary in zip(labels, summaries):\n",
    "            comparison_prompt += f\"\\n\\n{label}:\\n{summary}\"\n",
    "        \n",
    "        comparison_prompt += \"\"\"\n",
    "        \n",
    "        Please provide:\n",
    "        1. Ranking from best to worst with brief reasoning\n",
    "        2. Strengths and weaknesses of each summary\n",
    "        3. Which summary you would recommend and why\n",
    "        \n",
    "        Comparative Evaluation:\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.summarizer.generate(comparison_prompt, max_tokens=400)\n",
    "\n",
    "# Initialize modern evaluator\n",
    "modern_evaluator = ModernEvaluator(advanced_summarizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f4cd7",
   "metadata": {},
   "source": [
    "##  Hands-On Evaluation Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation_demo():\n",
    "    \"\"\"Demonstrate modern evaluation techniques\"\"\"\n",
    "    \n",
    "    # Test article for evaluation\n",
    "    eval_test_article = \"\"\"\n",
    "    The Mars Perseverance rover has successfully collected its 20th rock sample from the Martian surface, marking a significant milestone in the search for ancient microbial life. The sample, dubbed \"Beartown,\" was extracted from a sedimentary rock formation in Jezero Crater that scientists believe was formed in an ancient lake bed approximately 3.6 billion years ago.\n",
    "    \n",
    "    NASA's analysis indicates the rock contains high levels of silica and phosphate, minerals that on Earth are associated with biological processes. The rover's PIXL instrument detected organic compounds within the sample, though scientists caution that these could have non-biological origins. Dr. Sarah Chen, lead geologist on the mission, stated that the findings are \"extraordinarily promising\" for future analysis when the samples are returned to Earth.\n",
    "    \n",
    "    The sample collection process took three days due to the rock's unusual hardness, requiring multiple drilling attempts. This brings the total sample collection to 20 tubes, with plans to collect 10 more before the sample return mission launches in 2028. The European Space Agency will provide the Earth Return Orbiter, while NASA handles the Sample Return Lander, in a collaborative effort estimated to cost $7 billion.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate different summaries to evaluate\n",
    "    test_summaries = {\n",
    "        \"Basic\": advanced_summarizer.zero_shot_summarize(eval_test_article),\n",
    "        \"Chain-of-Thought\": advanced_summarizer.chain_of_thought_summarize(eval_test_article),\n",
    "        \"News Style\": advanced_summarizer.structured_summarize(eval_test_article, \"news\"),\n",
    "        \"Scientific\": advanced_summarizer.structured_summarize(eval_test_article, \"scientific\")\n",
    "    }\n",
    "    \n",
    "    print(\"📝 GENERATED SUMMARIES FOR EVALUATION:\")\n",
    "    print(\"=\" * 60)\n",
    "    for style, summary in test_summaries.items():\n",
    "        print(f\"\\n{style} Summary:\")\n",
    "        print(summary)\n",
    "        print(f\"Length: {len(summary.split())} words\")\n",
    "    \n",
    "    return test_summaries, eval_test_article\n",
    "\n",
    "evaluation_summaries, eval_source = comprehensive_evaluation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e69fa",
   "metadata": {},
   "source": [
    "### LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51826d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_judge_evaluation(summaries, source_text):\n",
    "    \"\"\"Run comprehensive LLM-based evaluation\"\"\"\n",
    "    \n",
    "    print(\"\\n🏛️ LLM-AS-JUDGE EVALUATION:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Evaluate each summary\n",
    "    evaluations = {}\n",
    "    \n",
    "    for style, summary in summaries.items():\n",
    "        print(f\"\\n📊 Evaluating {style} Summary...\")\n",
    "        evaluation = modern_evaluator.llm_as_judge(\n",
    "            summary, \n",
    "            source_text,\n",
    "            criteria=[\"accuracy\", \"completeness\", \"clarity\", \"conciseness\"]\n",
    "        )\n",
    "        evaluations[style] = evaluation\n",
    "        print(evaluation)\n",
    "    \n",
    "    # Comparative evaluation\n",
    "    print(f\"\\n🥇 COMPARATIVE RANKING:\")\n",
    "    print(\"=\" * 40)\n",
    "    comparative_eval = modern_evaluator.comparative_evaluation(\n",
    "        list(summaries.values()),\n",
    "        source_text,\n",
    "        list(summaries.keys())\n",
    "    )\n",
    "    print(comparative_eval)\n",
    "    \n",
    "    return evaluations, comparative_eval\n",
    "\n",
    "judge_evaluations, comparative_ranking = run_llm_judge_evaluation(evaluation_summaries, eval_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8810ce",
   "metadata": {},
   "source": [
    "### Factuality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_factuality_checks(summaries, source_text):\n",
    "    \"\"\"Check factual accuracy of summaries\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 FACTUALITY ASSESSMENT:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    factuality_results = {}\n",
    "    \n",
    "    for style, summary in summaries.items():\n",
    "        print(f\"\\n🧪 Checking {style} Summary for factual accuracy...\")\n",
    "        factuality_check = modern_evaluator.factuality_check(summary, source_text)\n",
    "        factuality_results[style] = factuality_check\n",
    "        print(factuality_check)\n",
    "    \n",
    "    return factuality_results\n",
    "\n",
    "factuality_assessments = run_factuality_checks(evaluation_summaries, eval_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc49ef",
   "metadata": {},
   "source": [
    "# Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69f26c",
   "metadata": {},
   "source": [
    "# Multimodal Summarization\n",
    "\n",
    "\n",
    "Multimodal summarization involves generating concise text that captures information from:\n",
    "- Text documents\n",
    "- Images\n",
    "- Tables and charts\n",
    "- Audio recordings\n",
    "- Video content\n",
    "\n",
    "## Approaches to Multimodal Summarization:\n",
    "\n",
    "1. **Pipeline Approach**: Process each modality separately, then combine\n",
    "2. **Unified Models**: Use multimodal models (like CLIP or GPT-4) that understand multiple modalities\n",
    "3. **Extraction + Description**: Extract elements from non-text modalities and describe them in text\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "- Aligning information across modalities\n",
    "- Handling inconsistencies between modalities \n",
    "- Determining relative importance of different modalities\n",
    "- Technical complexity of processing multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e721c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a multimodal summarizer for text + image data \n",
    "\n",
    "import requests\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultimodalSummarizer:\n",
    "    \"\"\"Multimodal summarizer using Llama-4 Vision capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"Initialize the multimodal summarizer with an OPENROUTER API key\"\"\"\n",
    "        # Get API key from environment variable if not provided\n",
    "        self.api_key = api_key or os.environ.get(\"OPENROUTER_API_KEY\", \"\")\n",
    "        if not self.api_key:\n",
    "            print(\"Warning: No OPENROUTER API key provided. Please set your OPENROUTER_API_KEY.\")\n",
    "        \n",
    "        self.api_url = \"https://openrouter.ai/api/v1\"\n",
    "        self.model = \"meta-llama/llama-4-maverick:free\"\n",
    "        \n",
    "    def encode_image(self, image_path):\n",
    "        \"\"\"Encode an image to base64 for API submission\"\"\"\n",
    "        # Check if it's a URL or local path\n",
    "        if image_path.startswith(('http://', 'https://')):\n",
    "            response = requests.get(image_path)\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        else:\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    def create_payload(self, text, image_paths, max_tokens=500):\n",
    "        \"\"\"Create the API payload with text and images\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that creates concise summaries from text and images.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Please create a comprehensive summary that combines information from the following text and images. Focus on integrating visual information with the text content.\\n\\nTEXT: {text}\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add images to the content\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                base64_image = self.encode_image(img_path)\n",
    "                messages[1][\"content\"].append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"high\"\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "        \n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    \n",
    "    def summarize_multimodal(self, text, image_paths, max_tokens=500):\n",
    "        \"\"\"Generate a summary from text and images using GPT-4\"\"\"\n",
    "        if not self.api_key:\n",
    "            return {\"error\": \"No API key provided. Please set your OPENROUTER_API_KEY.\"}\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = self.create_payload(text, image_paths, max_tokens)\n",
    "        \n",
    "        try:\n",
    "            print(\"Making API call to model...\")\n",
    "\n",
    "            response = requests.post(self.api_url, headers=headers, data=payload)\n",
    "            result = response.json()\n",
    "            \n",
    "            return {\n",
    "                'text_source': text,\n",
    "                'image_paths': image_paths,\n",
    "                'combined_summary': result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def display_images(self, image_paths):\n",
    "        \"\"\"Display the images used in the multimodal summary\"\"\"\n",
    "        num_images = len(image_paths)\n",
    "        \n",
    "        if num_images == 0:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
    "        \n",
    "        if num_images == 1:\n",
    "            axes = [axes]  \n",
    "            \n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                # Handle both URLs and local paths\n",
    "                if img_path.startswith(('http://', 'https://')):\n",
    "                    response = requests.get(img_path)\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "                else:\n",
    "                    img = Image.open(img_path)\n",
    "                \n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f\"Image {i+1}\")\n",
    "                axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                axes[i].text(0.5, 0.5, f\"Error loading image: {e}\", \n",
    "                             ha='center', va='center', transform=axes[i].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multimodal_input.png')\n",
    "        plt.close()\n",
    "        \n",
    "        from IPython.display import Image\n",
    "        return Image('multimodal_input.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    # \"images/image1.jpg\",\n",
    "    # \"images/image2.jpg\",\n",
    "    # \"images/image3.png\"\n",
    "]\n",
    "\n",
    "multimodal_summarizer = MultimodalSummarizer()\n",
    "\n",
    "# Generate a multimodal summary\n",
    "multimodal_result = multimodal_summarizer.summarize_multimodal(\n",
    "    #article,\n",
    "    image_paths,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6a0ad",
   "metadata": {},
   "source": [
    "# Practical Exercise: Building Your Custom Summarization System\n",
    "\n",
    "Now it's your turn to build a complete summarization system by combining techniques we've explored.\n",
    "\n",
    "## Exercise Goals:\n",
    "1. Create a pipeline that combines multiple approaches\n",
    "2. Customize control parameters for your specific needs\n",
    "3. Evaluate results using advanced metrics\n",
    "4. Compare performance across different text types\n",
    "\n",
    "## Project Ideas:\n",
    "1. **News Summarizer Bot**: Create a system that retrieves and summarizes news articles on specific topics\n",
    "2. **Meeting Minutes Generator**: Transcribe and summarize meeting audio recordings\n",
    "3. **Research Paper Summarizer**: Generate summaries of academic papers with focus on methodology and results\n",
    "4. **Medical Conversation Summarizer**: Summarize doctor-patient conversations, creating dual summaries (technical for doctors, simplified for patients)\n",
    "5. **EHR Summarizer**: Create a system that generates longitudinal patient summaries from fragmented electronic health records, retrieving and synthesizing information across multiple visits, lab results, and clinical notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483f7a1",
   "metadata": {},
   "source": [
    "### Code and Libraries:\n",
    "- [🤗 Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [BART Model Card](https://huggingface.co/facebook/bart-large-cnn)\n",
    "- [T5 Model Card](https://huggingface.co/t5-base)\n",
    "- [Text Generation Parameters](https://huggingface.co/blog/mlabonne/decoding-strategies)\n",
    "\n",
    "### Papers:\n",
    "- \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\"\n",
    "- \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
    "- \"Neural Abstractive Text Summarization with Sequence-to-Sequence Models\"\n",
    "\n",
    "\n",
    "## Useful Resources:\n",
    "\n",
    "### Datasets:\n",
    "- [CNN/Daily Mail Dataset](https://huggingface.co/datasets/cnn_dailymail)\n",
    "- [XSum Dataset](https://huggingface.co/datasets/xsum)\n",
    "- [Multi-News](https://huggingface.co/datasets/multi_news)\n",
    "- [BBC News Summary Dataset](https://www.kaggle.com/datasets/pariza/bbc-news-summary)\n",
    "\n",
    "### Evaluation Tools:\n",
    "- [ROUGE Implementation in Python](https://github.com/google-research/google-research/tree/master/rouge)\n",
    "- [BERTScore](https://github.com/Tiiiger/bert_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1794a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "def transcribe_meeting(audio_file):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_file)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f76b4",
   "metadata": {},
   "source": [
    "# **Facilitator(s) Details**\n",
    "\n",
    "**Facilitator(s):**\n",
    "\n",
    "*   Name: Nana Sam Yeboah                       \n",
    "*   Email: nanayeb34@gmail.com\n",
    "*   LinkedIn: [Nana Sam Yeboah](https://www.linkedin.com/in/nana-sam-yeboah-0b664484)\n",
    "\n",
    "# \n",
    "\n",
    "*   Name: Audrey Eyram Agbeve\n",
    "*   Email: audreyagbeve02@gmail.com\n",
    "*   LinkedIn: [Audrey (Eyram) Agbeve](https://www.linkedin.com/in/audreyagbeve02/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f827823",
   "metadata": {},
   "source": [
    "### Please rate this Tutorial\n",
    "\n",
    "<img src=\"images/Day1_feedback.png\" height=500 width=500  >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
