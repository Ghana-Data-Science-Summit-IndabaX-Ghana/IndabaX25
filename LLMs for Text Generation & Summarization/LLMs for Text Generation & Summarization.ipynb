{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28df5579",
   "metadata": {},
   "source": [
    "# **Leveraging LLMs for Text Generation and Summarization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c363790",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "1. [Architectural Overview of LLMs](#architectural-overview-of-llms\")\n",
    "2. [Categories of LLMs](#categories-of-llms)\n",
    "3. [Advanced Text Generation Techniques](#advanced-text-generation-techniques)\n",
    "4. [Parameter Tuning for Different Needs](#parameter-tuning-for-different-needs)\n",
    "5. [Temperature tuning experiment](#temperature-tuning-experiment)\n",
    "6. [Structured output generation](#structured-output-generation)\n",
    "7. [Extractive Summarization](#extractive-summarization)\n",
    "8. [Evaluation Metrics for Summarization](#evaluation-metrics-for-summarization)\n",
    "9. [Abstractive Summarization & Control Parameters](#abstractive-summarization--control-parameters)\n",
    "10. [Key Models for Abstractive Summarization](#key-models-for-abstractive-summarization)\n",
    "11. [Controllable Summarization](#controllable-summarization)\n",
    "12. [Building a Multi-Stage Summarizer](#building-a-multi-stage-summarization-pipeline)\n",
    "13. [Multimodal Summarization](#multimodal-summarization)\n",
    "14. [Practical Exercise: Building Your Custom Summarization System](#practical-exercise-building-your-custom-summarization-system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddceaa6",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898b8a3",
   "metadata": {},
   "source": [
    "### Learning Objectives for Part 1:\n",
    "- Understand the architecture of modern LLMs and their categories\n",
    "- Implement extractive summarization techniques\n",
    "- Establish evaluation metrics for summarization quality\n",
    "- Begin building a multi-stage summarization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504111f",
   "metadata": {},
   "source": [
    "# **Architectural Overview of LLMs**\n",
    "\n",
    "## The Transformer Architecture\n",
    "\n",
    "The transformer architecture revolutionized NLP when introduced in the paper \"Attention is All You Need\" (Vaswanathan et al., 2017).\n",
    "\n",
    "<!-- Transformer Architecture -->\n",
    "<div align=\"center\">\n",
    "<img src='images/transformer architecture.png' alt=\"Transformer architecture\" width=1000 height=700>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Key Components:\n",
    "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different words in context\n",
    "- **Multi-Head Attention**: Parallel attention mechanisms capturing different relationships\n",
    "- **Positional Encoding**: Helps the model understand word order\n",
    "- **Feed-Forward Networks**: Process the representations from attention layers\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "\n",
    "\n",
    "### Why Transformers Excel at Text Tasks,\n",
    "1. **Parallel Processing**: Unlike RNNs, can process entire sequences simultaneously\n",
    "2. **Long-Range Dependencies**: Attention mechanism captures distant relationships\n",
    "3. **Context Awareness**: Each token attends to all other tokens in the sequence\n",
    "4. **Scalability**: Architecture scales well with data and compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b85d1e",
   "metadata": {},
   "source": [
    "# Categories of LLMs\n",
    "\n",
    "LLMs come in different architectural variants, each with strengths for different tasks:\n",
    "\n",
    "## 1. Decoder-Only Models (Autoregressive)\n",
    "- Examples: GPT series, LLaMA, Claude\n",
    "- Trained to predict the next token\n",
    "- **Strengths for summarization**: Creative text generation, coherent narrative\n",
    "- **Weaknesses**: May hallucinate or add information not in source\n",
    "\n",
    "## 2. Encoder-Only Models\n",
    "- Examples: BERT, RoBERTa\n",
    "- Trained on masked language modeling\n",
    "- **Strengths for summarization**: Understanding document context, good for extractive summarization\n",
    "- **Weaknesses**: Not designed for generation\n",
    "\n",
    "## 3. Encoder-Decoder Models\n",
    "- Examples: T5, BART\n",
    "- Trained on sequence-to-sequence tasks\n",
    "- **Strengths for summarization**: Balanced understanding and generation, ideal for abstractive summarization\n",
    "- **Weaknesses**: Larger compute requirements\n",
    "\n",
    "### Which architecture is best for summarization?\n",
    "It depends on the task! We'll explore the tradeoffs throughout this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae68707",
   "metadata": {},
   "source": [
    "# Quick Exercise: Choosing the Right Architecture\n",
    "\n",
    "For each summarization scenario below, identify which model architecture (decoder-only, encoder-only, or encoder-decoder) would be most appropriate and why:\n",
    "\n",
    "1. Generating creative article summaries with novel phrasing\n",
    "2. Extracting key sentences from legal documents\n",
    "3. Translating and summarizing simultaneously\n",
    "4. Creating concise bullet points from meeting transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e11c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to view the answers\n",
    "print(\"1. Decoder-only models excel at creative generation but may add details.\")\n",
    "\n",
    "print(\"2. Encoder-only models like BERT are great at understanding document context.\")\n",
    "\n",
    "print(\"3. Encoder-decoder models like T5 are designed for tasks requiring both understanding and generation.\")\n",
    "\n",
    "print(\"4. This could use either encoder-only for extraction or encoder-decoder for concise reformulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603ad11",
   "metadata": {},
   "source": [
    "# Advanced Text Generation Techniques\n",
    "\n",
    "### The Anatomy of Effective Prompts\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/co-star.png>\" width=700 height=500>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "| Element       | Description                              |\n",
    "| ------------- | ---------------------------------------- |\n",
    "| **C**ontext   | Provide background information           |\n",
    "| **O**bjective | State the goal of the task               |\n",
    "| **S**tyle     | Specify tone, format, or constraints     |\n",
    "| **T**ask      | What the model should actually do        |\n",
    "| **A**udience  | Who the output is intended for           |\n",
    "| **R**esponse  | Clarify what the output should look like |\n",
    "\n",
    "ðŸ“Œ **Prompt Example:**\n",
    "\n",
    "*Context*: You are a career advisor writing content for a universityâ€™s job preparation website. Many students are unsure how to describe their achievements on resumes, particularly in action-result format.\n",
    "\n",
    "*Objective*: Help students craft clear and impressive resume bullet points for internships in data science.\n",
    "\n",
    "*Style*: Keep the tone professional and concise. Use strong action verbs and quantify results wherever possible. Avoid first-person language.\n",
    "\n",
    "*Task*: Based on the details provided, generate 3 resume bullet points that follow best practices in resume writing.\n",
    "\n",
    "*Audience*: Undergraduate students applying for internships in data science roles.\n",
    "\n",
    "*Response*: Your output should be a bulleted list of exactly 3 resume-ready statements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1326a",
   "metadata": {},
   "source": [
    "# Parameter Tuning for Different Needs\n",
    "\n",
    "Understanding how generation parameters affect output quality:\n",
    "\n",
    "- **Temperature (0.0-2.0)**: Controls randomness\n",
    "  - 0.0-0.3: Deterministic, factual content\n",
    "  - 0.4-0.7: Balanced creativity and coherence  \n",
    "  - 0.8-1.2: Creative, varied output\n",
    "  - 1.3+: Highly creative but potentially incoherent\n",
    "\n",
    "- **Top-p (0.0-1.0)**: Nucleus sampling\n",
    "  - Selects the most probable tokens whose cumulative probability exceeds a certain threshold p\n",
    "  - Lower values: More focused, consistent\n",
    "  - Higher values: More diverse vocabulary\n",
    "\n",
    "- **Top-k**: Limits vocabulary to k most likely tokens and samples from it\n",
    "  - Lower values: More predictable\n",
    "  - Higher values: More creative word choices\n",
    "\n",
    "- **Beam-Search**\n",
    "<img src='images/beam-search.jpg' width=900 height=431>\n",
    "  -  Sequence score is cumulative sum of the log probability of every token in the beam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f29a6",
   "metadata": {},
   "source": [
    "# Temperature tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7de9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the environment\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('OPENROUTER_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv\n",
    "  load_dotenv()\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import List\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b30eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemini-2.0-flash-lite-001\"):\n",
    "        self.model = model_name\n",
    "        self.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=self.api_key)\n",
    "\n",
    "    def generate(self, text, temperature=0.8, max_tokens=2000):\n",
    "        \"\"\"Generate summary using autoregressive model\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            model=self.model,\n",
    "            extra_body={},\n",
    "        )\n",
    "        \n",
    "        # Extract and return the response\n",
    "        result = response.choices[0].message.content\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde75d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parameters(prompt: str, temperatures: List[float], client: LLMClient):\n",
    "\n",
    "    results = []\n",
    "    for temperature in temperatures:\n",
    "        result = client.generate(prompt, temperature=temperature)\n",
    "        results.append(\n",
    "            {\n",
    "                \"temperature\": temperature,\n",
    "                \"response\": result,\n",
    "                \"length\": len(result.split())\n",
    "            }\n",
    "        )   \n",
    "    return results\n",
    "\n",
    "\n",
    "test_prompt = \"Write a creative opening sentence for a science fiction story about time travel.\"\n",
    "temperatures = [0.1, 0.5, 0.9, 1.2]\n",
    "\n",
    "results = compare_parameters(test_prompt, temperatures, LLMClient()),\n",
    "for result in results[0]:\n",
    "    print(f\"Temperature: {result['temperature']}\"),\n",
    "    print(f\"Response: {result['response']}\"),\n",
    "    print(f\"Word count: {result['length']}\"),\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed51980",
   "metadata": {},
   "source": [
    "# Structured output generation\n",
    "\n",
    "Getting LLMs to produce consistent, parseable output formats is crucial for integration with downstream systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d06996",
   "metadata": {},
   "source": [
    "### Prompt based structured generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ea843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Dr. Amara Liu', 'age': 35, 'profession': 'astrophysicist', 'language': 'Mandarin and English', 'origin': 'Shanghai'}\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"name\": \"string\",\n",
    "    \"age\": \"integer\",\n",
    "    \"profession\": \"string\",\n",
    "    \"language\": \"string\",\n",
    "    \"origin\": \"string\"\n",
    "}\n",
    "\n",
    "character_bio = \"\"\"\n",
    "\"Dr. Amara Liu is a 35-year-old astrophysicist from Shanghai. She researches dark matter and enjoys stargazing and sketching. Fluent in Mandarin and English.\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = (\n",
    "    f\"{character_bio}\\n\\n\"\n",
    "    \"Extract the following structured data as JSON:\\n\"\n",
    "    f\"{json.dumps(schema, indent=2)}\\n\\n\"\n",
    "    \"Respond ONLY with valid JSON that matches the schema above.\"\n",
    ")\n",
    "model = LLMClient()\n",
    "response = model.generate(prompt)\n",
    "\n",
    "# Try to extract JSON\n",
    "try:\n",
    "    structured_data = json.loads(response)\n",
    "except json.JSONDecodeError:\n",
    "    import re\n",
    "    match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    structured_data = json.loads(match.group()) if match else {\"error\": \"invalid format\"}\n",
    "\n",
    "print(structured_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db69e1b",
   "metadata": {},
   "source": [
    "### Schema Aware structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3985503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure Output via function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"extract_profile\",\n",
    "            \"description\": \"Extracts a person's profile.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"age\": {\"type\": \"integer\"},\n",
    "                    \"profession\": {\"type\": \"string\"},\n",
    "                    \"language\": {\"type\": \"string\"},\n",
    "                    \"origin\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"name\", \"age\", \"profession\", \"language\", \"origin\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=os.getenv(\"OPENROUTER_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"google/gemini-2.0-flash-lite-001\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"What is the name, age, profession, language, and origin of the person in the following text:\\n {character_bio}\"\n",
    "    }],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.tool_calls[0].function.arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cfbe7",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bfed6",
   "metadata": {},
   "source": [
    "<img src=\"images/text summarization.jpg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6ec0c",
   "metadata": {},
   "source": [
    "# Extractive Summarization\n",
    "\n",
    "Extractive summarization selects the most important sentences from the orginal text to create the summary.\n",
    "\n",
    "## The Basic Process:\n",
    "1. Score sentences based on importance\n",
    "2. Select top-scoring sentences using ranking algorithm (TF IDF, SVM, etc)\n",
    "3. Arrange in coherent order (usually original order)\n",
    "\n",
    "## Advantages:\n",
    "- Factually accurate (uses original text)\n",
    "- Computationally efficient\n",
    "- Works well for objective content\n",
    "\n",
    "## Disadvantages:\n",
    "- May be disconnected or redundant\n",
    "- Cannot reformulate or simplify complex content\n",
    "- Limited by quality of source material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "def extractive_summary(text, ratio=0.3):\n",
    "    # Tokenize the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into individual words and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()] # This removes any stop words and non-alphanumeric characters from the resulting list of words and converts them all to lowercase\n",
    "\n",
    "    # Compute the frequency of each word\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Compute the score for each sentence based on the frequency of its words\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        if len(sentence_words) < 20: # to filter short sentences\n",
    "            sentence_scores[sentence] = sentence_score\n",
    "\n",
    "    # Compute the number of sentences to include in the summary\n",
    "    num_sentences = max(1, int(len(sentences) * ratio))\n",
    "\n",
    "\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Load a Ghanaian news article\n",
    "with open('articles/article.txt', 'r', encoding='utf-8') as f:\n",
    "    ghana_article = f.read()\n",
    "    \n",
    "# Print article length\n",
    "print(f\"Article contains {len(sent_tokenize(ghana_article))} sentences and {len(ghana_article.split())} words\")\n",
    "generated_summary= extractive_summary(ghana_article, ratio=0.3)\n",
    "print(f\"\\nSUMMARY\\n{generated_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f13ebb",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Summarization\n",
    "\n",
    "How do we know if our summaries are good? Let's implement some common evaluation metrics:\n",
    "\n",
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- Measures overlap between machine-generated summary and reference summary\n",
    "- ROUGE-N: N-gram recall\n",
    "- ROUGE-L: Longest Common Subsequence \n",
    "\n",
    "<img src='images/Rouge L.jpeg' width=700>\n",
    "\n",
    "LCS is the longest set of ordered tokens that occurs in both sequences (Ref, Gen)\n",
    "\n",
    "## BLEU (Bilingual Evaluation Understudy)\n",
    "- Originally designed for translation, but used for summarization\n",
    "- Precision-focused (how many generated n-grams appear in reference)\n",
    "\n",
    "## BERTScore\n",
    "- Uses contextual embeddings to compute similarity\n",
    "- Better semantic understanding than n-gram methods\n",
    "\n",
    "## Human Evaluation Dimensions\n",
    "- **Relevance**: How well does the summary capture the main points?\n",
    "- **Coherence**: Does it flow logically?\n",
    "- **Fluency**: Is it grammatically correct?\n",
    "- **Factuality**: Does it contain errors or hallucinations?\n",
    "- **Accuracy**: Does the summary accurately represent the original content?\n",
    "- **Readability**: Is the summary well-written and easy to understand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement custom ROUGE socre\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Reference summary for comparison\n",
    "with open(\"articles/reference summary.txt\", 'r', encoding='utf-8') as f:\n",
    "    reference_summary = f.read()\n",
    "\n",
    "# Tokenize into words\n",
    "ref_tokens = word_tokenize(reference_summary.lower())\n",
    "cand_tokens = word_tokenize(generated_summary.lower())\n",
    "\n",
    "\n",
    "# Generate n-grams\n",
    "\n",
    "\n",
    "# Count ngrams\n",
    "\n",
    "\n",
    "# Calculate precision\n",
    "\n",
    "\n",
    "# Calculate recall\n",
    "\n",
    "\n",
    "# Calculate F1 score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdcaefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple function to evaluate our summaries for ROUGE-1 and ROUGE-2\n",
    "def evaluate_summary(reference, candidate):\n",
    "    \"\"\"Evaluate a summary using multiple metrics\"\"\"\n",
    "    scores = {\n",
    "        'ROUGE-1': calculate_rouge_n(reference, candidate, 1),\n",
    "        'ROUGE-2': calculate_rouge_n(reference, candidate, 2),\n",
    "    }\n",
    "\n",
    "    # Add readability metric: average words per sentence\n",
    "    cand_sentences = sent_tokenize(candidate)\n",
    "    avg_sentence_length = len(word_tokenize(candidate)) / max(1, len(cand_sentences))\n",
    "    scores['Avg Words/Sentence'] = avg_sentence_length\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try evaluating our summary against a reference summary\n",
    "\n",
    "# Evaluate our extractive summary against the reference\n",
    "evaluation_scores = evaluate_summary(reference_summary, generated_summary)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, score in evaluation_scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try different summary ratios and compare\n",
    "ratios = [0.2, 0.3, 0.4, 0.5]\n",
    "ratio_results = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    test_summary = extractive_summary(ghana_article, ratio=ratio)\n",
    "    scores = evaluate_summary(reference_summary, test_summary)\n",
    "    scores['ratio'] = ratio\n",
    "    scores['length'] = len(test_summary.split())\n",
    "    ratio_results.append(scores)\n",
    "\n",
    "# Display comparison\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(ratio_results)\n",
    "print(\"\\nComparison of Different Summary Ratios:\")\n",
    "print(results_df[['ratio', 'length', 'ROUGE-1', 'ROUGE-2']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a04f07",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87d459",
   "metadata": {},
   "source": [
    "# Abstractive Summarization & Control Parameters\n",
    "\n",
    "\n",
    "## What is Abstractive Summarization?\n",
    "\n",
    "Abstractive summarization involves:\n",
    "- Understanding the source content deeply\n",
    "- Identifying key concepts and relationships\n",
    "- Generating new text by paraphrasing existing text\n",
    "- Condensing information in ways that extraction cannot\n",
    "\n",
    "## Why Use Encoder-Decoder Models?\n",
    "\n",
    "While autoregressive (decoder-only) models like GPT can perform abstractive summarization, encoder-decoder models like T5 and BART offer specific advantages:\n",
    "\n",
    "1. **Bidirectional encoding**: The encoder comprehends the entire document before generation begins\n",
    "2. **Source-target separation**: Clear distinction between understanding and generation\n",
    "3. **Cross-attention mechanism**: Decoder can directly reference source material during generation\n",
    "4. **Training objectives**: Pre-trained specifically for tasks that include summarization\n",
    "5. **Control mechanisms**: Easier to implement length constraints and other controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b7ab5",
   "metadata": {},
   "source": [
    "# Key Models for Abstractive Summarization\n",
    "\n",
    "## BART (Bidirectional and Auto-Regressive Transformers)\n",
    "- Combines bidirectional encoder (like BERT) with autoregressive decoder\n",
    "- Pre-trained on denoising tasks, including text infilling and sentence shuffling\n",
    "- Particularly effective for summarization tasks\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer)\n",
    "- Treats all NLP tasks as \"text-to-text\" problems\n",
    "- Consistent performance across summarization, translation, classification, etc.\n",
    "- Uses a \"prefix\" to specify the task (e.g., \"summarize:\")\n",
    "\n",
    "## Pegasus\n",
    "- Specifically pre-trained for abstractive summarization\n",
    "- Uses \"gap sentences\" pre-training, masking important sentences during training\n",
    "- Optimized for news summarization tasks\n",
    "\n",
    "## Comparing with Autoregressive Models (e.g., GPT)\n",
    "\n",
    "| Aspect | Encoder-Decoder | Autoregressive |\n",
    "|--------|-----------------|----------------|\n",
    "| Source understanding | Bidirectional | Primarily left-to-right |\n",
    "| Memory of source | Direct attention | Must retain in context |\n",
    "| Training objective | Often summarization-specific | General next-token prediction |\n",
    "| Length control | Easier to implement | Requires special techniques |\n",
    "| Hallucination risk | Lower (with cross-attention) | Higher |\n",
    "| Flexibility | Task-specific | More general-purpose |\n",
    "\n",
    "In practice, the lines are blurring as models evolve. Modern autoregressive models can achieve excellent summarization through few-shot prompting and other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartForConditionalGeneration, BartTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "class T5Summarizer:\n",
    "    \"\"\"T5-based abstractive summarizer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"t5-small\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.prefix = \"summarize: \"\n",
    "        self.checkpoint_name = f\"{model_name.replace('/', '_')}_t5\"\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "            \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "        \n",
    "    def summarize(self, text, max_length=150, min_length=50, length_penalty=2.0, \n",
    "                  num_beams=4, early_stopping=True):\n",
    "        \"\"\"Generate summary with T5, adding the task prefix\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            self.load_model()\n",
    "            \n",
    "        # Add prefix for T5\n",
    "        prefixed_text = self.prefix + text\n",
    "        \n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(prefixed_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(self.device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            length_penalty=length_penalty,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "        \n",
    "        # Decode and return summary\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "class BartSummarizer:\n",
    "    \"\"\"BART-based abstractive summarizer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"facebook/bart-base\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.checkpoint_name = f\"{model_name.replace('/', '_')}_bart\"\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "            \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "    def summarize(self, text, max_length=150, min_length=50, length_penalty=2.0, \n",
    "                  num_beams=4, early_stopping=True):\n",
    "        \"\"\"Generate an abstractive summary with BART\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            self.load_model()\n",
    "            \n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(self.device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            length_penalty=length_penalty,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "        \n",
    "        # Decode and return summary\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "class AutoregressiveSumarizer:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemini-2.0-flash-lite-001\"):\n",
    "        self.model = model_name\n",
    "        self.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=self.api_key)\n",
    "\n",
    "    def summarize(self, text, system_prompt, temperature=0.8, max_tokens=500):\n",
    "        \"\"\"Generate summary using autoregressive model\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            model=self.model,\n",
    "            extra_body={},\n",
    "        )\n",
    "        \n",
    "        # Extract and return the summary\n",
    "        summary = response.choices[0].message.content\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e95cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out our abstractive summarizers\n",
    "\n",
    "# Let's create our summarizers\n",
    "t5_summarizer = T5Summarizer(\"t5-small\")  # \n",
    "bart_summarizer = BartSummarizer(\"facebook/bart-base\")\n",
    "autoregressive_summarizer = AutoregressiveSumarizer()\n",
    "\n",
    "# Generate summaries\n",
    "t5_summary = t5_summarizer.summarize(\n",
    "    ghana_article,\n",
    "    max_length=100,  # Target length in tokens\n",
    "    min_length=30,\n",
    "    num_beams=4      # Beam search for better quality\n",
    ")\n",
    "\n",
    "bart_summary = bart_summarizer.summarize(\n",
    "    ghana_article,\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "autoregressive_summary = autoregressive_summarizer.summarize(\n",
    "    ghana_article,\n",
    "    \"Summarize the following text in a concise manner.\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Let's also get our extractive summary for comparison\n",
    "extractive_summary= extractive_summary(ghana_article, ratio=0.3)\n",
    "\n",
    "# Display all summaries\n",
    "print(\"Original Text Length:\", len(ghana_article.split()))\n",
    "print(\"\\n--- T5 Summary ---\")\n",
    "print(t5_summary)\n",
    "print(\"\\nLength:\", len(t5_summary.split()))\n",
    "\n",
    "print(\"\\n--- BART Summary ---\")\n",
    "print(bart_summary)\n",
    "print(\"\\nLength:\", len(bart_summary.split()))\n",
    "\n",
    "print(\"\\n--- AutoRegressive Summary ---\")\n",
    "print(autoregressive_summary)\n",
    "print(\"\\nLength:\", len(autoregressive_summary.split()))\n",
    "\n",
    "print(\"\\n--- Extractive Summary ---\")\n",
    "print(extractive_summary)\n",
    "print(\"\\nLength:\", len(extractive_summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b104b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same news file so we use the same reference summary\n",
    "# Evaluate all summaries\n",
    "t5_scores = evaluate_summary(reference_summary, t5_summary)\n",
    "bart_scores = evaluate_summary(reference_summary, bart_summary)\n",
    "autoregressive_scores = evaluate_summary(reference_summary, autoregressive_summary)\n",
    "extractive_scores = evaluate_summary(reference_summary, extractive_summary)\n",
    "\n",
    "# Compare scores\n",
    "summary_comparison = pd.DataFrame({\n",
    "    'T5': t5_scores,\n",
    "    'BART': bart_scores,\n",
    "    'AutoRegressive': autoregressive_scores,\n",
    "    'Extractive': extractive_scores\n",
    "})\n",
    "\n",
    "print(\"Objective Metrics Comparison:\")\n",
    "print(summary_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c271d",
   "metadata": {},
   "source": [
    "# Controllable Summarization\n",
    "\n",
    "One of the major advantages of modern summarization systems is the ability to control various aspects of the generated summaries:\n",
    "\n",
    "## Common Control Parameters:\n",
    "\n",
    "1. **Length**: Controlling how long or short the summary should be\n",
    "2. **Style**: Formal vs. casual, simple vs. technical\n",
    "3. **Focus**: Emphasizing particular topics or aspects\n",
    "4. **Structure**: Bullet points, narrative, or question-answering\n",
    "\n",
    "## How to Implement Controls:\n",
    "\n",
    "1. **Model-specific parameters**: Using built-in generation controls\n",
    "2. **Prompt engineering**: Adding instructional prefixes\n",
    "3. **Output filtering**: Post-processing generated summaries\n",
    "4. **Fine-tuning**: Training the model with examples of desired style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1780e",
   "metadata": {},
   "source": [
    "Length control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50907a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing length control for our summarizers\n",
    "\n",
    "def generate_controlled_summaries(text, model, lengths=[50, 100, 200]):\n",
    "    \"\"\"Generate summaries of different controlled lengths\"\"\"\n",
    "    summaries = {}\n",
    "    \n",
    "    for length in lengths:\n",
    "        if isinstance(model, T5Summarizer):\n",
    "            summary = model.summarize(\n",
    "                text,\n",
    "                max_length=length,\n",
    "                min_length=max(10, int(length * 0.7)),  # At least 70% of max length\n",
    "                num_beams=4\n",
    "            )\n",
    "            summaries[f\"T5 (length={length})\"] = summary\n",
    "            \n",
    "        elif isinstance(model, BartSummarizer):\n",
    "            summary = model.summarize(\n",
    "                text,\n",
    "                max_length=length,\n",
    "                min_length=max(10, int(length * 0.7)),\n",
    "                num_beams=4\n",
    "            )\n",
    "            summaries[f\"BART (length={length})\"] = summary\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Let's generate summaries of different lengths\n",
    "length_controlled_t5 = generate_controlled_summaries(ghana_article, t5_summarizer, [50, 100, 150])\n",
    "length_controlled_bart = generate_controlled_summaries(ghana_article, bart_summarizer, [50, 100, 150])\n",
    "\n",
    "# Display and analyze length-controlled summaries\n",
    "print(\"Length-Controlled Summaries:\\n\")\n",
    "\n",
    "for name, summary in {**length_controlled_t5, **length_controlled_bart}.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(summary)\n",
    "    print(f\"Actual length: {len(summary.split())} words\")\n",
    "\n",
    "# Plotting the relationship between target length and actual length\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract target lengths and actual lengths\n",
    "target_lengths = []\n",
    "actual_lengths_t5 = []\n",
    "actual_lengths_bart = []\n",
    "\n",
    "for length in [50, 100, 150]:\n",
    "    target_lengths.append(length)\n",
    "    actual_lengths_t5.append(len(length_controlled_t5[f\"T5 (length={length})\"].split()))\n",
    "    actual_lengths_bart.append(len(length_controlled_bart[f\"BART (length={length})\"].split()))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(target_lengths, actual_lengths_t5, 'o-', label='T5')\n",
    "plt.plot(target_lengths, actual_lengths_bart, 'o-', label='BART')\n",
    "plt.plot(target_lengths, target_lengths, '--', label='Target=Actual', color='gray')\n",
    "plt.xlabel('Target Length (tokens)')\n",
    "plt.ylabel('Actual Length (words)')\n",
    "plt.title('Length Control Effectiveness')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig('length_control.png')\n",
    "plt.close()\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('length_control.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556545a",
   "metadata": {},
   "source": [
    "Prompt Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing focus control through prompt engineering\n",
    "\n",
    "def focus_controlled_summary(text, model, focus_area):\n",
    "    \"\"\"Generate a summary focused on a specific aspect\"\"\"\n",
    "    \n",
    "    # Create a focused prompt\n",
    "    if isinstance(model, T5Summarizer):\n",
    "        focused_prompt = f\"summarize focusing on {focus_area}: {text}\"\n",
    "        summary = model.summarize(\n",
    "            focused_prompt,\n",
    "            max_length=100,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    elif isinstance(model, BartSummarizer):\n",
    "        # For BART, we need to be more creative since it doesn't have a prefix\n",
    "        # Append the instruction to the beginning of the text\n",
    "        focused_prompt = f\"Focus on {focus_area} in your summary. {text}\"\n",
    "        summary = model.summarize(\n",
    "            focused_prompt,\n",
    "            max_length=100,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries focused on different aspects\n",
    "focus_areas = [\"cost of living comparison\", \"Brain Drain vs. Nation Building\", \"unity and collective action\"]\n",
    "focused_summaries = {}\n",
    "\n",
    "for focus in focus_areas:\n",
    "    focused_summaries[f\"T5 (focus: {focus})\"] = focus_controlled_summary(ghana_article, t5_summarizer, focus)\n",
    "    focused_summaries[f\"BART (focus: {focus})\"] = focus_controlled_summary(ghana_article, bart_summarizer, focus)\n",
    "\n",
    "# Display focused summaries\n",
    "print(\"Focus-Controlled Summaries:\\n\")\n",
    "\n",
    "for name, summary in focused_summaries.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(summary)\n",
    "\n",
    "# Let's analyze how well the focus control worked\n",
    "import re\n",
    "\n",
    "def count_focus_related_words(text, focus):\n",
    "    \"\"\"Count words related to the focus area\"\"\"\n",
    "    # Create a simple keyword list for each focus area\n",
    "    focus_keywords = {\n",
    "        \"cost of living comparison\": [\"youth emigration\", \"greener pastures\", \"high unemployment\", \"limited opportunities\", \"collective responsibility\", \"development\"],\n",
    "        \"Brain Drain vs. Nation Building\": [\"affordable housing\", \"lower living expenses\", \"imported goods\",  \"entrepreneurship opportunities\"],\n",
    "        \"unity and collective action\": [ \"unity\", \"peace\", \"collaboration\",  \"technology leverage\", \"shared vision\", \"mutual respect\",\"national development\"]\n",
    "    }\n",
    "    \n",
    "    # Count occurrences of focus keywords\n",
    "    keywords = focus_keywords.get(focus, [])\n",
    "    count = sum(1 for keyword in keywords if re.search(r'\\b' + keyword + r'\\b', text.lower()))\n",
    "    \n",
    "    return count, len(keywords)\n",
    "\n",
    "# Analyze focus effectiveness\n",
    "focus_effectiveness = {}\n",
    "\n",
    "for focus in focus_areas:\n",
    "    t5_count, total = count_focus_related_words(\n",
    "        focused_summaries[f\"T5 (focus: {focus})\"], \n",
    "        focus\n",
    "    )\n",
    "    bart_count, _ = count_focus_related_words(\n",
    "        focused_summaries[f\"BART (focus: {focus})\"], \n",
    "        focus\n",
    "    )\n",
    "    \n",
    "    # Calculate percentage of focus keywords present\n",
    "    focus_effectiveness[focus] = {\n",
    "        \"T5\": f\"{t5_count}/{total} keywords ({t5_count/total*100:.1f}%)\",\n",
    "        \"BART\": f\"{bart_count}/{total} keywords ({bart_count/total*100:.1f}%)\"\n",
    "    }\n",
    "\n",
    "# Display focus effectiveness\n",
    "print(\"\\nFocus Control Effectiveness:\\n\")\n",
    "for focus, models in focus_effectiveness.items():\n",
    "    print(f\"Focus area: {focus}\")\n",
    "    print(f\"  T5: {models['T5']}\")\n",
    "    print(f\"  BART: {models['BART']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96331874",
   "metadata": {},
   "source": [
    "Style Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc92288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing style and structure control\n",
    "\n",
    "def style_controlled_summary(text, model, style):\n",
    "    \"\"\"Generate a summary with a specific style\"\"\"\n",
    "    \n",
    "    style_prompts = {\n",
    "        \"formal\": \"Generate a formal and technical summary of the following text:\",\n",
    "        \"simple\": \"Generate a simple summary using basic vocabulary and short sentences:\",\n",
    "        \"bullet_points\": \"Generate a summary in bullet point format highlighting key points:\",\n",
    "        \"question_answering\": \"Generate a summary in question and answer format about:\",\n",
    "        \"news_headline\": \"Write a news headline style summary of:\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"{style_prompts[style]} {text}\"\n",
    "    \n",
    "    if isinstance(model, T5Summarizer):\n",
    "        # For T5, replace the standard \"summarize:\" prefix\n",
    "        summary = model.summarize(\n",
    "            prompt,\n",
    "            max_length=120,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    elif isinstance(model, BartSummarizer):\n",
    "        summary = model.summarize(\n",
    "            prompt,\n",
    "            max_length=120,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries with different styles\n",
    "styles = [\"formal\", \"simple\", \"bullet_points\", \"question_answering\", \"news_headline\"]\n",
    "styled_summaries = {}\n",
    "\n",
    "for style in styles:\n",
    "    # Let's just use T5 for this demonstration\n",
    "    styled_summaries[f\"T5 (style: {style})\"] = style_controlled_summary(ghana_article, t5_summarizer, style)\n",
    "\n",
    "# Display styled summaries\n",
    "print(\"Style-Controlled Summaries:\\n\")\n",
    "\n",
    "for name, summary in styled_summaries.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(summary)\n",
    "\n",
    "# Simple readability assessment\n",
    "def assess_readability(text):\n",
    "    \"\"\"Calculate a simple readability score (average words per sentence)\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    \n",
    "    words = text.split()\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    \n",
    "    return avg_words_per_sentence\n",
    "\n",
    "# Analyze style effectiveness\n",
    "style_assessment = {}\n",
    "\n",
    "for style, summary_key in zip(styles, styled_summaries.keys()):\n",
    "    summary = styled_summaries[summary_key]\n",
    "    \n",
    "    # Check if bullet points are present\n",
    "    has_bullets = \"â€¢\" in summary or \"-\" in summary.split() or any(line.strip().startswith(\"-\") for line in summary.split(\"\\n\"))\n",
    "    \n",
    "    # Check if questions are present\n",
    "    has_questions = \"?\" in summary\n",
    "    \n",
    "    # Assess readability\n",
    "    readability = assess_readability(summary)\n",
    "    \n",
    "    style_assessment[style] = {\n",
    "        \"Readability (words/sentence)\": f\"{readability:.1f}\",\n",
    "        \"Has bullet points\": \"Yes\" if has_bullets else \"No\",\n",
    "        \"Has questions\": \"Yes\" if has_questions else \"No\"\n",
    "    }\n",
    "\n",
    "# Display style assessment\n",
    "print(\"\\nStyle Control Assessment:\\n\")\n",
    "style_df = pd.DataFrame(style_assessment).T\n",
    "print(style_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7b8da",
   "metadata": {},
   "source": [
    "# Building a Multi-Stage Summarization Pipeline\n",
    "\n",
    "Now let's combine the best of extractive and abstractive approaches to create a more effective summarization pipeline:\n",
    "\n",
    "## Benefits of a Multi-Stage Approach:\n",
    "\n",
    "1. **Handling longer documents**: Extractive methods can select relevant content from long texts\n",
    "2. **Improving factual accuracy**: Extractive first step preserves key facts\n",
    "3. **Computational efficiency**: Processing only relevant portions with resource-intensive models\n",
    "4. **Enhanced control**: Apply different strategies at different stages\n",
    "\n",
    "## Our Pipeline Design:\n",
    "\n",
    "1. **Stage 1**: Extractive selection of most relevant sentences\n",
    "2. **Stage 2**: Abstractive summarization of the extracted content\n",
    "3. **Stage 3**: Post-processing for quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc87208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a multi-stage summarization pipeline\n",
    "\n",
    "class MultiStageSummarizer:\n",
    "    \"\"\"Multi-stage pipeline combining extractive and abstractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 extractive_summarizer,\n",
    "                 abstractive_summarizer,\n",
    "                 extractive_ratio=0.5):\n",
    "        \"\"\"Initialize the pipeline with component summarizers\"\"\"\n",
    "        self.extractive_summarizer = extractive_summarizer\n",
    "        self.abstractive_summarizer = abstractive_summarizer\n",
    "        self.extractive_ratio = extractive_ratio\n",
    "        \n",
    "    def summarize(self, text, max_length=100, min_length=30):\n",
    "        \"\"\"Generate a summary using the multi-stage pipeline\"\"\"\n",
    "        # Stage 1: Extractive summarization\n",
    "        print(\"Stage 1: Extractive summarization...\")\n",
    "        extractive_summary = self.extractive_summarizer(\n",
    "            text, \n",
    "            ratio=self.extractive_ratio\n",
    "        )\n",
    "        \n",
    "        # Check the intermediate result\n",
    "        print(f\"  Extracted ({len(extractive_summary.split())} words)\")\n",
    "        \n",
    "        # Stage 2: Abstractive summarization\n",
    "        print(\"Stage 2: Abstractive summarization...\")\n",
    "        if isinstance(self.abstractive_summarizer, T5Summarizer):\n",
    "            abstractive_summary = self.abstractive_summarizer.summarize(\n",
    "                extractive_summary,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4\n",
    "            )\n",
    "        else:\n",
    "            abstractive_summary = self.abstractive_summarizer.summarize(\n",
    "                extractive_summary,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4\n",
    "            )\n",
    "            \n",
    "        # Stage 3: Post-processing\n",
    "        print(\"Stage 3: Post-processing...\")\n",
    "        final_summary = self.post_process(abstractive_summary)\n",
    "        \n",
    "        return {\n",
    "            \"extractive_summary\": extractive_summary,\n",
    "            \"abstractive_summary\": abstractive_summary,\n",
    "            \"final_summary\": final_summary\n",
    "        }\n",
    "    \n",
    "    def post_process(self, summary):\n",
    "        \"\"\"Apply post-processing to improve summary quality\"\"\"\n",
    "        # Remove repeated phrases or sentences (a common issue)\n",
    "        sentences = sent_tokenize(summary)\n",
    "        unique_sentences = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            # Skip nearly identical sentences (simple approach)\n",
    "            if not any(self.sentence_similarity(s, us) > 0.7 for us in unique_sentences):\n",
    "                unique_sentences.append(s)\n",
    "        \n",
    "        # Rejoin the unique sentences\n",
    "        processed_summary = ' '.join(unique_sentences)\n",
    "        \n",
    "        return processed_summary\n",
    "    \n",
    "    def sentence_similarity(self, s1, s2):\n",
    "        \"\"\"Calculate simple word overlap similarity between sentences\"\"\"\n",
    "        words1 = set(s1.lower().split())\n",
    "        words2 = set(s2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0\n",
    "            \n",
    "        overlap = words1.intersection(words2)\n",
    "        return len(overlap) / max(len(words1), len(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28825ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multistage_summarizer=MultiStageSummarizer(\n",
    "    extractive_summarizer=extractive_summary,\n",
    "    abstractive_summarizer=t5_summarizer,\n",
    "    extractive_ratio=0.3\n",
    ")\n",
    "result = multistage_summarizer.summarize(ghana_article)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc49ef",
   "metadata": {},
   "source": [
    "# Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69f26c",
   "metadata": {},
   "source": [
    "# Multimodal Summarization\n",
    "\n",
    "\n",
    "Multimodal summarization involves generating concise text that captures information from:\n",
    "- Text documents\n",
    "- Images\n",
    "- Tables and charts\n",
    "- Audio recordings\n",
    "- Video content\n",
    "\n",
    "## Approaches to Multimodal Summarization:\n",
    "\n",
    "1. **Pipeline Approach**: Process each modality separately, then combine\n",
    "2. **Unified Models**: Use multimodal models (like CLIP or GPT-4) that understand multiple modalities\n",
    "3. **Extraction + Description**: Extract elements from non-text modalities and describe them in text\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "- Aligning information across modalities\n",
    "- Handling inconsistencies between modalities \n",
    "- Determining relative importance of different modalities\n",
    "- Technical complexity of processing multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e721c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a multimodal summarizer for text + image data \n",
    "\n",
    "import requests\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultimodalSummarizer:\n",
    "    \"\"\"Multimodal summarizer using Llama-4 Vision capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"Initialize the multimodal summarizer with an OPENROUTER API key\"\"\"\n",
    "        # Get API key from environment variable if not provided\n",
    "        self.api_key = api_key or os.environ.get(\"OPENROUTER_API_KEY\", \"\")\n",
    "        if not self.api_key:\n",
    "            print(\"Warning: No OPENROUTER API key provided. Please set your OPENROUTER_API_KEY.\")\n",
    "        \n",
    "        self.api_url = \"https://openrouter.ai/api/v1\"\n",
    "        self.model = \"meta-llama/llama-4-maverick:free\"\n",
    "        \n",
    "    def encode_image(self, image_path):\n",
    "        \"\"\"Encode an image to base64 for API submission\"\"\"\n",
    "        # Check if it's a URL or local path\n",
    "        if image_path.startswith(('http://', 'https://')):\n",
    "            response = requests.get(image_path)\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        else:\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    def create_gpt4_payload(self, text, image_paths, max_tokens=500):\n",
    "        \"\"\"Create the API payload with text and images\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that creates concise summaries from text and images.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Please create a comprehensive summary that combines information from the following text and images. Focus on integrating visual information with the text content.\\n\\nTEXT: {text}\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add images to the content\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                base64_image = self.encode_image(img_path)\n",
    "                messages[1][\"content\"].append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"high\"\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "        \n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    \n",
    "    def summarize_multimodal(self, text, image_paths, max_tokens=500):\n",
    "        \"\"\"Generate a summary from text and images using GPT-4\"\"\"\n",
    "        if not self.api_key:\n",
    "            return {\"error\": \"No API key provided. Please set your OPENROUTER_API_KEY.\"}\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = self.create_gpt4_payload(text, image_paths, max_tokens)\n",
    "        \n",
    "        try:\n",
    "            # Simulate API call for workshop purposes\n",
    "            print(\"Making API call to model...\")\n",
    "\n",
    "            response = requests.post(self.api_url, headers=headers, data=payload)\n",
    "            result = response.json()\n",
    "            \n",
    "            return {\n",
    "                'text_source': text,\n",
    "                'image_paths': image_paths,\n",
    "                'combined_summary': result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def display_images(self, image_paths):\n",
    "        \"\"\"Display the images used in the multimodal summary\"\"\"\n",
    "        num_images = len(image_paths)\n",
    "        \n",
    "        if num_images == 0:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
    "        \n",
    "        if num_images == 1:\n",
    "            axes = [axes]  # Make it iterable for single image case\n",
    "            \n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                # Handle both URLs and local paths\n",
    "                if img_path.startswith(('http://', 'https://')):\n",
    "                    response = requests.get(img_path)\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "                else:\n",
    "                    img = Image.open(img_path)\n",
    "                \n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f\"Image {i+1}\")\n",
    "                axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                axes[i].text(0.5, 0.5, f\"Error loading image: {e}\", \n",
    "                             ha='center', va='center', transform=axes[i].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multimodal_input.png')\n",
    "        plt.close()\n",
    "        \n",
    "        from IPython.display import Image\n",
    "        return Image('multimodal_input.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    # \"images/image1.jpg\",\n",
    "    # \"images/image2.jpg\",\n",
    "    # \"images/image3.png\"\n",
    "]\n",
    "\n",
    "multimodal_summarizer = MultimodalSummarizer()\n",
    "\n",
    "# Generate a multimodal summary\n",
    "multimodal_result = multimodal_summarizer.summarize_multimodal(\n",
    "    #article,\n",
    "    image_paths,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6a0ad",
   "metadata": {},
   "source": [
    "# Practical Exercise: Building Your Custom Summarization System\n",
    "\n",
    "Now it's your turn to build a complete summarization system by combining techniques we've explored.\n",
    "\n",
    "## Exercise Goals:\n",
    "1. Create a pipeline that combines multiple approaches\n",
    "2. Customize control parameters for your specific needs\n",
    "3. Evaluate results using advanced metrics\n",
    "4. Compare performance across different text types\n",
    "\n",
    "## Project Ideas:\n",
    "1. **News Summarizer Bot**: Create a system that retrieves and summarizes news articles on specific topics\n",
    "2. **Meeting Minutes Generator**: Transcribe and summarize meeting audio recordings\n",
    "3. **Research Paper Summarizer**: Generate summaries of academic papers with focus on methodology and results\n",
    "4. **Medical Conversation Summarizer**: Summarize doctor-patient conversations, creating dual summaries (technical for doctors, simplified for patients)\n",
    "5. **EHR Summarizer**: Create a system that generates longitudinal patient summaries from fragmented electronic health records, retrieving and synthesizing information across multiple visits, lab results, and clinical notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483f7a1",
   "metadata": {},
   "source": [
    "### Code and Libraries:\n",
    "- [ðŸ¤— Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [BART Model Card](https://huggingface.co/facebook/bart-large-cnn)\n",
    "- [T5 Model Card](https://huggingface.co/t5-base)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [Text Generation Parameters](https://huggingface.co/blog/mlabonne/decoding-strategies)\n",
    "\n",
    "### Papers:\n",
    "- \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\"\n",
    "- \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
    "- \"Neural Abstractive Text Summarization with Sequence-to-Sequence Models\"\n",
    "\n",
    "\n",
    "## Useful Resources:\n",
    "\n",
    "### Datasets:\n",
    "- [CNN/Daily Mail Dataset](https://huggingface.co/datasets/cnn_dailymail)\n",
    "- [XSum Dataset](https://huggingface.co/datasets/xsum)\n",
    "- [Multi-News](https://huggingface.co/datasets/multi_news)\n",
    "\n",
    "### Evaluation Tools:\n",
    "- [ROUGE Implementation in Python](https://github.com/google-research/google-research/tree/master/rouge)\n",
    "- [BERTScore](https://github.com/Tiiiger/bert_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f76b4",
   "metadata": {},
   "source": [
    "# **Facilitator(s) Details**\n",
    "\n",
    "**Facilitator(s):**\n",
    "\n",
    "*   Name: Nana Sam Yeboah                       \n",
    "*   Email: nanayeb34@gmail.com\n",
    "*   LinkedIn: [Nana Sam Yeboah](https://www.linkedin.com/in/nana-sam-yeboah-0b664484)\n",
    "\n",
    "# \n",
    "\n",
    "*   Name: Audrey Eyram Agbeve\n",
    "*   Email: audreyagbeve02@gmail.com\n",
    "*   LinkedIn: [Audrey (Eyram) Agbeve](https://www.linkedin.com/in/audreyagbeve02/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f827823",
   "metadata": {},
   "source": [
    "### Please rate this Tutorial\n",
    "\n",
    "<img src=\"images/Day1_feedback.png\" height=500 width=500  >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
