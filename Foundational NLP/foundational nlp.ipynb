{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14557826",
   "metadata": {},
   "source": [
    "# **Foundational NLP**\n",
    "\n",
    "## **Pre-training and Key Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1084d5",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "1.   [Introduction](#Introduction)\n",
    "2.   [Prerequisites](#Prerequisites)\n",
    "3.   [Step-by-Step-Guide](#Step-by-Step-Guide)\n",
    "4.   [Code Examples](#Code-Examples)\n",
    "5.   [Troubleshooting](#Troubleshooting)\n",
    "6.   [Conclusion](#Conclusion)\n",
    "7.   [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87443492",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "The principle of distributional semantics is encapsulated in J.R. Firth’s famous quote   <pre> ```“You shall know a word by the company it keeps”``` </pre> \n",
    " this quote highlights the significance of contextual information in determining   \n",
    " word meaning and captures the importance of contextual information in defining word meanings.   \n",
    " This principle is a cornerstone in the development of word embeddings.\n",
    "\n",
    "Word embeddings, also known as word vectors, provide a dense, continuous, and compact representation of words,  \n",
    "encapsulating their semantic and syntactic attributes.   \n",
    "They are essentially real-valued vectors, and the proximity of these vectors in a multidimensional   \n",
    "space is indicative of the linguistic relationships between words \n",
    "\n",
    "The term  <pre> “embedding” </pre>  in this context refers to the transformation of discrete words into   \n",
    "continuous vectors,   \n",
    "achieved through word embedding algorithms. These algorithms are designed to convert   \n",
    "words into vectors that encapsulate a significant portion of their semantic content.   \n",
    "An example of the effectiveness of these embeddings is the vector arithmetic that yields meaningful analogies such as <pre> \"uncle\" - \"man\" + \"woman\" ≈ \"aunt\" </pre>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4dccf",
   "metadata": {},
   "source": [
    "## **Prerequisites**\n",
    "\n",
    "- Programming fundamentals (Python is the standard language for NLP)\n",
    "\n",
    "- Basic probability and statistics as well as linear algebra concepts\n",
    "\n",
    "- Machine learning concepts\n",
    "\n",
    "- Text preprocessing techniques\n",
    "\n",
    "- Linguistic Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4b46d",
   "metadata": {},
   "source": [
    "<a id='guide'></a>\n",
    "## **Step-by-Step Guide**\n",
    "\n",
    "## Word Embedding Techniques\n",
    "\n",
    "- Count-Based Techniques (TF-IDF and BM25)  \n",
    "- Co-occurrence Based/Static Embedding Techniques  \n",
    "- Contextualized/Dynamic Representation Techniques (BERT, ELMo) \n",
    "\n",
    "\n",
    "### Bag of Words (BoW) \n",
    "\n",
    "Tokenization:\n",
    " - Split the text into words (tokens).  \n",
    "\n",
    "Vocabulary Building:\n",
    " - Create a vocabulary list of all unique words in the corpus.\n",
    "\n",
    "Vector Representation:\n",
    "   - For each document, create a vector where each element corresponds to a word in the vocabulary. \n",
    "     The value is the count of occurrences of that word in the document.\n",
    "\n",
    "\n",
    "\n",
    "**Example** \n",
    "\n",
    "Consider a corpus with the following two documents:\n",
    "1. “The cat sat on the mat.”\n",
    "2. “The dog sat on the log.”\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Tokenization:\n",
    "   - Document 1: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "   - Document 2: [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]\n",
    "\n",
    "\n",
    "2. Vocabulary Building:\n",
    "    - Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n",
    "\n",
    "\n",
    "3. Vector Representation:\n",
    "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "    The resulting BoW vectors are:\n",
    "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "\n",
    "\n",
    "###  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a \n",
    "word to a document in a collection or corpus. \n",
    "It is a fundamental technique in text processing that ranks the \n",
    "relevance of documents to a specific query, commonly applied in tasks such as document classification, search engine ranking, \n",
    "information retrieval, and text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493b6c9",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b289d",
   "metadata": {},
   "source": [
    "## Term Frequency (TF)\n",
    "\n",
    "- Term Frequency measures how frequently a term occurs in a document. \n",
    "Since every document is different in length, it is possible that a term would \n",
    "appear much more times in long documents than shorter ones.\n",
    "Thus, the term frequency is often divided by the document length (the total number of terms in the document) as a way of normalization:\n",
    "\n",
    " <pre> TF(t)=Number of times term t appears in a documentTotal number of terms in the document     \n",
    "           --------------------------------------------------------------------------------------\n",
    "                             Total number of terms in the document\n",
    "\n",
    "  </pre>\n",
    "\n",
    "\n",
    "\n",
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "- Inverse Document Frequency measures how important a term is. \n",
    " While computing TF, all terms are considered equally important.   \n",
    " However, certain terms, like “is”, “of”, and “that”, may appear a lot of times but have little importance.   \n",
    " Thus, we need to weigh down the frequent terms while scaling up the rare ones, by computing the following:\n",
    "\n",
    "   <pre>  IDF(t)=log(Total number of documents)  \n",
    "   -------------------------------------------\n",
    "    (Number of documents with term t in it)  </pre>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Steps to Calculate TF-IDF\n",
    "\n",
    "Step 1: TF (Term Frequency): Number of times a word appears in a document divided by the total number of words in that document.  \n",
    "Step 2: IDF (Inverse Document Frequency): Calculated as log(N / df), where: \n",
    "N is the total number of documents in the collection.   \n",
    "df is the number of documents containing the word.  \n",
    "Step 3: TF-IDF: The product of TF and IDF.\n",
    "\n",
    "Document Collection\n",
    "- Doc 1: “The sky is blue.”\n",
    "- Doc 2: “The sun is bright.”\n",
    "- Total documents (N): 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd72d66",
   "metadata": {},
   "source": [
    "## **CODE EXAMPLE**\n",
    "## **RAG APPLICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fb31b",
   "metadata": {},
   "source": [
    "In many real-world scenarios, organizations maintain extensive collections of \n",
    "proprietary documents, \n",
    "such as technical manuals, from which precise information must be extracted. \n",
    "This challenge is often \n",
    "analogous to locating a needle in a haystack, given the sheer volume and complexity of the content.\n",
    "While recent advancements, such as OpenAI’s introduction of GPT-4 Turbo, offer improved capabilities \n",
    "for processing lengthy documents, they are not without limitations. Notably, these models exhibit a \n",
    "tendency known as the “Lost in the Middle” phenomenon, wherein information positioned near the center \n",
    "of the context window is more likely to be overlooked or forgotten. This issue is akin to reading a \n",
    "comprehensive text such as the Bible, yet struggling to recall specific content from its middle chapters.\n",
    "To address this shortcoming, the RAG approach has been introduced. This method involves segmenting documents \n",
    "into discrete units—typically paragraphs—and creating an index for each. Upon receiving a query, the system \n",
    "efficiently identifies and retrieves the most relevant segments, which are then supplied to the language model. \n",
    "By narrowing the input to only the most pertinent information, this strategy mitigates cognitive overload within \n",
    "the model and substantially improves the relevance and accuracy of its responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ff58c",
   "metadata": {},
   "source": [
    "<img src=\"/Users/nanakwame/Downloads/indaba/IndabaX251/Foundational NLP/rag.png\" width=\"400\" alt=\"Rag Pipe\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea5bcd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Iterator, AsyncIterator, List\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "class KNUSTCsvDataLoader(BaseLoader):\n",
    "    \"\"\"A document loader that loads CSV documents.\"\"\"\n",
    "\n",
    "    def __init__(self, directory: str, encoding: str = 'latin1') -> None:\n",
    "        \"\"\"Initialize the loader with a directory.\n",
    "\n",
    "        Args:\n",
    "            directory: The path to the directory containing CSV files.\n",
    "            encoding: The encoding to use for reading CSV files (default: 'latin1').\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        return list(self.lazy_load())\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"A lazy loader that reads CSV files row by row.\"\"\"\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                try:\n",
    "                    # Load CSV file with specified encoding\n",
    "                    df = pd.read_csv(file_path, encoding=self.encoding)\n",
    "\n",
    "                    # Validate required columns\n",
    "                    required_columns = {\"Subject\", \"Question\", \"Response\"}\n",
    "                    if not required_columns.issubset(df.columns):\n",
    "                        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "                    # Iterate over rows in chunks\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=1000, encoding=self.encoding):\n",
    "                        for row in chunk.itertuples():\n",
    "                            yield Document(\n",
    "                                page_content=f\"Subject: {row.Subject}\\nQuestion: {row.Question}\\nResponse: {row.Response}\",\n",
    "                                metadata={\"subject\": row.Subject}\n",
    "                            )\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Encoding error in {file_path}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    async def alazy_load(self) -> AsyncIterator[Document]:\n",
    "        \"\"\"An async lazy loader that reads CSV files row by row.\"\"\"\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                try:\n",
    "                    # Read CSV file synchronously with specified encoding\n",
    "                    df = pd.read_csv(file_path, encoding=self.encoding)\n",
    "\n",
    "                    # Validate required columns\n",
    "                    required_columns = {\"Subject\", \"Question\", \"Response\"}\n",
    "                    if not required_columns.issubset(df.columns):\n",
    "                        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "                    # Yield documents asynchronously\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=1000, encoding=self.encoding):\n",
    "                        for row in chunk.itertuples():\n",
    "                            yield Document(\n",
    "                                page_content=f\"Subject: {row.Subject}\\nQuestion: {row.Question}\\nResponse: {row.Response}\",\n",
    "                                metadata={\"subject\": row.Subject}\n",
    "                            )\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Encoding error in {file_path}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "# Usage\n",
    "directory = \"/Users/nanakwame/Downloads/ghana-data-science/IndabaX25/data\"\n",
    "loader = KNUSTCsvDataLoader(directory, encoding='latin1')\n",
    "documents = list(loader.lazy_load())\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d724897",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
