{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14557826",
   "metadata": {},
   "source": [
    "# **Foundational NLP**\n",
    "\n",
    "## **Pre-training and Key Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1084d5",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "1.   [Introduction](#Introduction)\n",
    "2.   [Prerequisites](#Prerequisites)\n",
    "3.   [Step-by-Step-Guide](#Step-by-Step-Guide)\n",
    "4.   [Code Examples](#Code-Examples)\n",
    "5.   [Troubleshooting](#Troubleshooting)\n",
    "6.   [Conclusion](#Conclusion)\n",
    "7.   [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87443492",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "The principle of distributional semantics is encapsulated in J.R. Firth’s famous quote   <pre> ```“You shall know a word by the company it keeps”``` </pre> \n",
    " this quote highlights the significance of contextual information in determining   \n",
    " word meaning and captures the importance of contextual information in defining word meanings.   \n",
    " This principle is a cornerstone in the development of word embeddings.\n",
    "\n",
    "Word embeddings, also known as word vectors, provide a dense, continuous, and compact representation of words,  \n",
    "encapsulating their semantic and syntactic attributes.   \n",
    "They are essentially real-valued vectors, and the proximity of these vectors in a multidimensional   \n",
    "space is indicative of the linguistic relationships between words \n",
    "\n",
    "The term  <pre> “embedding” </pre>  in this context refers to the transformation of discrete words into   \n",
    "continuous vectors,   \n",
    "achieved through word embedding algorithms. These algorithms are designed to convert   \n",
    "words into vectors that encapsulate a significant portion of their semantic content.   \n",
    "An example of the effectiveness of these embeddings is the vector arithmetic that yields meaningful analogies such as <pre> \"uncle\" - \"man\" + \"woman\" ≈ \"aunt\" </pre>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4dccf",
   "metadata": {},
   "source": [
    "## **Prerequisites**\n",
    "\n",
    "- Programming fundamentals (Python is the standard language for NLP)\n",
    "\n",
    "- Basic probability and statistics as well as linear algebra concepts\n",
    "\n",
    "- Machine learning concepts\n",
    "\n",
    "- Text preprocessing techniques\n",
    "\n",
    "- Linguistic Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4b46d",
   "metadata": {},
   "source": [
    "<a id='guide'></a>\n",
    "## **Step-by-Step Guide**\n",
    "\n",
    "## Word Embedding Techniques\n",
    "\n",
    "- Count-Based Techniques (TF-IDF and BM25)  \n",
    "- Co-occurrence Based/Static Embedding Techniques  \n",
    "- Contextualized/Dynamic Representation Techniques (BERT, ELMo) \n",
    "\n",
    "\n",
    "### Bag of Words (BoW) \n",
    "\n",
    "Tokenization:\n",
    " - Split the text into words (tokens).  \n",
    "\n",
    "Vocabulary Building:\n",
    " - Create a vocabulary list of all unique words in the corpus.\n",
    "\n",
    "Vector Representation:\n",
    "   - For each document, create a vector where each element corresponds to a word in the vocabulary. \n",
    "     The value is the count of occurrences of that word in the document.\n",
    "\n",
    "\n",
    "\n",
    "**Example** \n",
    "\n",
    "Consider a corpus with the following two documents:\n",
    "1. “The cat sat on the mat.”\n",
    "2. “The dog sat on the log.”\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Tokenization:\n",
    "   - Document 1: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "   - Document 2: [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]\n",
    "\n",
    "\n",
    "2. Vocabulary Building:\n",
    "    - Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n",
    "\n",
    "\n",
    "3. Vector Representation:\n",
    "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "    The resulting BoW vectors are:\n",
    "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "\n",
    "\n",
    "###  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a \n",
    "word to a document in a collection or corpus. \n",
    "It is a fundamental technique in text processing that ranks the \n",
    "relevance of documents to a specific query, commonly applied in tasks such as document classification, search engine ranking, \n",
    "information retrieval, and text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493b6c9",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b289d",
   "metadata": {},
   "source": [
    "## Term Frequency (TF)\n",
    "\n",
    "- Term Frequency measures how frequently a term occurs in a document. \n",
    "Since every document is different in length, it is possible that a term would \n",
    "appear much more times in long documents than shorter ones.\n",
    "Thus, the term frequency is often divided by the document length (the total number of terms in the document) as a way of normalization:\n",
    "\n",
    " <pre> TF(t)=Number of times term t appears in a documentTotal number of terms in the document     \n",
    "           --------------------------------------------------------------------------------------\n",
    "                             Total number of terms in the document\n",
    "\n",
    "  </pre>\n",
    "\n",
    "\n",
    "\n",
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "- Inverse Document Frequency measures how important a term is. \n",
    " While computing TF, all terms are considered equally important.   \n",
    " However, certain terms, like “is”, “of”, and “that”, may appear a lot of times but have little importance.   \n",
    " Thus, we need to weigh down the frequent terms while scaling up the rare ones, by computing the following:\n",
    "\n",
    "   <pre>  IDF(t)=log(Total number of documents)  \n",
    "   -------------------------------------------\n",
    "    (Number of documents with term t in it)  </pre>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Steps to Calculate TF-IDF\n",
    "\n",
    "Step 1: TF (Term Frequency): Number of times a word appears in a document divided by the total number of words in that document.  \n",
    "Step 2: IDF (Inverse Document Frequency): Calculated as log(N / df), where: \n",
    "N is the total number of documents in the collection.   \n",
    "df is the number of documents containing the word.  \n",
    "Step 3: TF-IDF: The product of TF and IDF.\n",
    "\n",
    "Document Collection\n",
    "- Doc 1: “The sky is blue.”\n",
    "- Doc 2: “The sun is bright.”\n",
    "- Total documents (N): 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd72d66",
   "metadata": {},
   "source": [
    "## **CODE EXAMPLE**\n",
    "## **RAG APPLICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fb31b",
   "metadata": {},
   "source": [
    "In many real-world scenarios, organizations maintain extensive collections of \n",
    "proprietary documents, \n",
    "such as technical manuals, from which precise information must be extracted. \n",
    "This challenge is often \n",
    "analogous to locating a needle in a haystack, given the sheer volume and complexity of the content.\n",
    "While recent advancements, such as OpenAI’s introduction of GPT-4 Turbo, offer improved capabilities \n",
    "for processing lengthy documents, they are not without limitations. Notably, these models exhibit a \n",
    "tendency known as the “Lost in the Middle” phenomenon, wherein information positioned near the center \n",
    "of the context window is more likely to be overlooked or forgotten. This issue is akin to reading a \n",
    "comprehensive text such as the Bible, yet struggling to recall specific content from its middle chapters.\n",
    "To address this shortcoming, the RAG approach has been introduced. This method involves segmenting documents \n",
    "into discrete units—typically paragraphs—and creating an index for each. Upon receiving a query, the system \n",
    "efficiently identifies and retrieves the most relevant segments, which are then supplied to the language model. \n",
    "By narrowing the input to only the most pertinent information, this strategy mitigates cognitive overload within \n",
    "the model and substantially improves the relevance and accuracy of its responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ff58c",
   "metadata": {},
   "source": [
    "<img src=\"/Users/nanakwame/Downloads/indaba/IndabaX251/Foundational NLP/rag.png\" width=\"400\" alt=\"Rag Pipe\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Iterator, AsyncIterator, List\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "class KNUSTCsvDataLoader(BaseLoader):\n",
    "    \"\"\"A document loader that loads CSV documents.\"\"\"\n",
    "\n",
    "    def __init__(self, directory: str, encoding: str = 'latin1') -> None:\n",
    "        \"\"\"Initialize the loader with a directory.\n",
    "\n",
    "        Args:\n",
    "            directory: The path to the directory containing CSV files.\n",
    "            encoding: The encoding to use for reading CSV files (default: 'latin1').\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        return list(self.lazy_load())\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"A lazy loader that reads CSV files row by row.\"\"\"\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                try:\n",
    "                    # Load CSV file with specified encoding\n",
    "                    df = pd.read_csv(file_path, encoding=self.encoding)\n",
    "\n",
    "                    # Validate required columns\n",
    "                    required_columns = {\"Subject\", \"Question\", \"Response\"}\n",
    "                    if not required_columns.issubset(df.columns):\n",
    "                        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "                    # Iterate over rows in chunks\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=1000, encoding=self.encoding):\n",
    "                        for row in chunk.itertuples():\n",
    "                            yield Document(\n",
    "                                page_content=f\"Subject: {row.Subject}\\nQuestion: {row.Question}\\nResponse: {row.Response}\",\n",
    "                                metadata={\"subject\": row.Subject}\n",
    "                            )\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Encoding error in {file_path}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    async def alazy_load(self) -> AsyncIterator[Document]:\n",
    "        \"\"\"An async lazy loader that reads CSV files row by row.\"\"\"\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                try:\n",
    "                    # Read CSV file synchronously with specified encoding\n",
    "                    df = pd.read_csv(file_path, encoding=self.encoding)\n",
    "\n",
    "                    # Validate required columns\n",
    "                    required_columns = {\"Subject\", \"Question\", \"Response\"}\n",
    "                    if not required_columns.issubset(df.columns):\n",
    "                        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "                    # Yield documents asynchronously\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=1000, encoding=self.encoding):\n",
    "                        for row in chunk.itertuples():\n",
    "                            yield Document(\n",
    "                                page_content=f\"Subject: {row.Subject}\\nQuestion: {row.Question}\\nResponse: {row.Response}\",\n",
    "                                metadata={\"subject\": row.Subject}\n",
    "                            )\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Encoding error in {file_path}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "# Usage\n",
    "directory = \"/Users/nanakwame/Downloads/indaba/IndabaX251/Foundational NLP/data\"\n",
    "loader = KNUSTCsvDataLoader(directory, encoding='latin1')\n",
    "documents = list(loader.lazy_load())\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f495f7",
   "metadata": {},
   "source": [
    "## **CODE EXAMPLE**\n",
    "## **NAMED-ENTITY RECOGNITION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc93139",
   "metadata": {},
   "source": [
    "Name Entity recognition(NER) is a subtask of Natural language process(NLP) which focuses on identifying and grouping entities within a text or document.   \n",
    "Entities present specific objects or names such as Persons, organizations, dates and times, countries, drugs, and various unique information within a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39735c2",
   "metadata": {},
   "source": [
    "## Types-of-NER-models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32708a",
   "metadata": {},
   "source": [
    "NER is applied in various sectors of our daily lives; so of these applied areas are:\n",
    "\n",
    "- Rule based NER models\n",
    "- Machine learning (ML) models\n",
    "- Deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7c28e",
   "metadata": {},
   "source": [
    "Rule-based-model\n",
    "A rule-based NER model is a .....\n",
    "\n",
    "Examples of rule based approaches\n",
    "\n",
    "a. Spacy Entity\n",
    "\n",
    "b. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d894ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import svgling\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good. can arthur go to the Ghana\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97245c5d",
   "metadata": {},
   "source": [
    "## Code-Examples-For-Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "%%capture\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_rb_ner(patterns,text,model_name='en'):\n",
    "\n",
    "  #create a blank model\n",
    "  nlp = spacy.blank(model_name)\n",
    "\n",
    "  #create an new entity to for NER\n",
    "  ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "  ruler.add_patterns(patterns)\n",
    "\n",
    "  #extract components from the text\n",
    "  doc = nlp(text)\n",
    "  # print(doc)\n",
    "  for ent in doc.ents:\n",
    "      print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [{\"label\": \"AGE\", \"pattern\": [{\"like_num\": True}, {\"lower\": \"years\"}, {\"lower\": \"old\"}]}]\n",
    "text = \"John is 25 years old\"\n",
    "spacy_rb_ner(patterns,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e2d7d",
   "metadata": {},
   "source": [
    "## Exercises 1  \n",
    "\n",
    "Please list some advantages and disadvantages as you try out these rule based name entity recognition models.\n",
    "\n",
    "Advantages 1. 2.\n",
    "\n",
    "Disadvantages 1. 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy\n",
    "#customize your own pattern and provide your text for testing\n",
    "pattern =[]\n",
    "text = \"\"\n",
    "spacy_rb_ner(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76987783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard\n",
    "\n",
    "#1. dataset extraction from huggingface\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"remakia/drugs-dictionary\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "#2. load the json dictionary\n",
    "import json\n",
    "def read_json(json_file):\n",
    "\n",
    "  return 0\n",
    "\n",
    "#3. convert the drug dict into patterns\n",
    "def generate_patern(drug_dict):\n",
    "\n",
    "  return 0\n",
    "\n",
    "json_file = \"drug.json\"\n",
    "drug_dict = read_json(json_file)\n",
    "pattern =generate_patern(drug_dict)\n",
    "text = \"Perfusion d'une ampoule de prexidine de lithium et introduction d'un antihistaminique par Cétirizine 10 mg x 2 par jour, avec diminution puis disparition de l'oedème.\"\n",
    "text = text.lower()\n",
    "spacy_rb_ner(pattern,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce6357",
   "metadata": {},
   "source": [
    "## Machine Learning-model\n",
    "Conditional Random Fields - (provide explanation)  \n",
    "SVM - (provide explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d14e31a",
   "metadata": {},
   "source": [
    "Example-code\n",
    "Below is an example code of Spacy, a machine learning NER model train with the theory of conditional random fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "content =\"Esi is a 27-years-old individual who came back home from school. she is meant to go back to school soon to see her friends. Have you heard from Kwame because the last time i spoke to him, he said he was going to the Ghana, Kigali\"\n",
    "\n",
    "doc = nlp(content)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11490b34",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "We will train our own Spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf784f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_data():\n",
    "     return 0 \n",
    "\n",
    "def normalization():\n",
    "    return 0\n",
    "\n",
    "def dataset_preprocessing(TRAIN_DATA,ner):\n",
    "\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_iter= 30 #number of times you want the model to train \n",
    "model= \"name-of-blank-model\"\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "\n",
    "#create and set up a pipeline \n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "#data preprocessing\n",
    "TRAIN_DATA=processed_data()\n",
    "dataset_preprocessing(TRAIN_DATA)\n",
    "\n",
    "#training \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in tqdm(TRAIN_DATA):\n",
    "            nlp.update(\n",
    "                [text],  \n",
    "                [annotations],  \n",
    "                drop=0.5,  \n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026a6ee",
   "metadata": {},
   "source": [
    "## **Conclusion and Comments From Participants**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44145ab2",
   "metadata": {},
   "source": [
    "# **Facilitator(s) Details**\n",
    "\n",
    "**Facilitator(s):**\n",
    "\n",
    "*   Name: FELIX TETTEH AKWERH\n",
    "*   Email: felix.akwerh@knust.edu.gh\n",
    "\n",
    "\n",
    "\n",
    "*   Name: ADWOA ASANTEWAA BREMANG\n",
    "*   Email: adwoabremang@gmail.com\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
