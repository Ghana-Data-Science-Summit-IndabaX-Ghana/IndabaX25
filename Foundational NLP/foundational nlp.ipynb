{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14557826",
   "metadata": {},
   "source": [
    "# **Foundational NLP**\n",
    "\n",
    "## **Pre-training and Key Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1084d5",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "1.   [Introduction](#Introduction)\n",
    "2.   [Prerequisites](#Prerequisites)\n",
    "3.   [Step-by-Step-Guide](#Step-by-Step-Guide)\n",
    "4.   [Code Examples](#Code-Examples)\n",
    "5.   [Troubleshooting](#Troubleshooting)\n",
    "6.   [Conclusion](#Conclusion)\n",
    "7.   [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b995a",
   "metadata": {},
   "source": [
    "## PRE-TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b175a48",
   "metadata": {},
   "source": [
    "### DEFINITION\n",
    "Pre-training is the initial phase of training a machine learning model, like a neural network, on a large, diverse   \n",
    "general dataset before fine-tuning it for a specific task. This approach helps the model learn broad patterns   \n",
    "that can be adapted, saving time and resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc452fb4",
   "metadata": {},
   "source": [
    "### Process of Pre-Training\n",
    "\n",
    "\n",
    "The pre-training process can be broken down into several key steps, as outlined in recent literature:\n",
    "\n",
    "1. Training on a Large Dataset: The model is trained on a vast, diverse dataset, such as internet text for LLMs or large image datasets for computer vision tasks. For example, datasets like FineWeb, with 15 trillion tokens, are used for LLMs, ensuring the model learns general language patterns .\n",
    "  > - Data Sources: Common sources include web crawls like CommonCrawl, archiving billions of web pages since 2007, such as the April 2024 crawl with 2.7 billion pages and  386 TiB of uncompressed HTML .\n",
    "  \n",
    "  > - Data Processing: This involves filtering (e.g., URL blocklists for adult content, spam), text extraction (removing HTML, JavaScript), language filtering (e.g., fastText for English ≥ 0.65 confidence), and deduplication (e.g., MinHash), resulting in high-quality, diverse data.\n",
    "\n",
    "\n",
    "2. Tokenization: For LLMs, text is converted into tokens, discrete units processed by the neural network. Byte Pair Encoding (BPE) is commonly used, reducing sequence length while expanding vocabulary (e.g., GPT-4 with 100,277 tokens). Tools like Tiktokenizer facilitate this process, optimizing input for efficiency.\n",
    "\n",
    "\n",
    "\n",
    "3. Neural Network Training: The model, typically Transformer-based (e.g., nano-GPT with 85,000 parameters), is trained to predict the next token or perform a general task, using backpropagation and optimization techniques like gradient descent. The training involves computing loss (e.g., cross-entropy) and iteratively refining parameters. \n",
    "\n",
    "\n",
    "4. Saving the Model: Once trained, the model's parameters (weights) are saved, creating a base model that can be used for subsequent tasks.\n",
    "\n",
    "\n",
    "5. Fine-Tuning for Specific Tasks: For a new task, the pre-trained model's parameters initialize a new model, which is then trained on the specific task's dataset.  \n",
    "This fine-tuning adjusts the model to the new task, often requiring less data and computational resources than training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70231fe0",
   "metadata": {},
   "source": [
    "### Theoretical Underpinnings of Pre-training\n",
    "\n",
    "Recent research, such as the 2023 Nature Machine Intelligence article, provides insight into why pre-training improves downstream task performance. \n",
    "It suggests that pre-training induces a structured latent space within the model, capturing task-relevant relationships.The Structure-Inducing Pre-Training (SIPT) \n",
    "framework explicitly imposes deep, structural constraints during pre-training, enhancing the latent space's geometry. This framework theoretically guarantees \n",
    "improved fine-tuning performance, with empirical results showing SIPT outperforming baselines in 10/15 cases across modalities like Proteins, \n",
    "Abstracts, and Networks, with gains such as 17% improvement on ACL-ARC and 6% on SciERC relation extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87443492",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "The principle of distributional semantics is encapsulated in J.R. Firth’s famous quote   <pre> ```“You shall know a word by the company it keeps”``` </pre> \n",
    " this quote highlights the significance of contextual information in determining   \n",
    " word meaning and captures the importance of contextual information in defining word meanings.   \n",
    " This principle is a cornerstone in the development of word embeddings.\n",
    "\n",
    "Word embeddings, also known as word vectors, provide a dense, continuous, and compact representation of words,  \n",
    "encapsulating their semantic and syntactic attributes.   \n",
    "They are essentially real-valued vectors, and the proximity of these vectors in a multidimensional   \n",
    "space is indicative of the linguistic relationships between words \n",
    "\n",
    "The term  <pre> “embedding” </pre>  in this context refers to the transformation of discrete words into   \n",
    "continuous vectors,   \n",
    "achieved through word embedding algorithms. These algorithms are designed to convert   \n",
    "words into vectors that encapsulate a significant portion of their semantic content.   \n",
    "An example of the effectiveness of these embeddings is the vector arithmetic that yields meaningful analogies such as <pre> \"uncle\" - \"man\" + \"woman\" ≈ \"aunt\" </pre>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4dccf",
   "metadata": {},
   "source": [
    "## **Prerequisites**\n",
    "\n",
    "- Programming fundamentals (Python is the standard language for NLP)\n",
    "\n",
    "- Basic probability and statistics as well as linear algebra concepts\n",
    "\n",
    "- Machine learning concepts\n",
    "\n",
    "- Text preprocessing techniques\n",
    "\n",
    "- Linguistic Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4b46d",
   "metadata": {},
   "source": [
    "<a id='guide'></a>\n",
    "## **Step-by-Step Guide**\n",
    "\n",
    "## Word Embedding Techniques\n",
    "\n",
    "- Count-Based Techniques (TF-IDF and BM25)  \n",
    "- Co-occurrence Based/Static Embedding Techniques  \n",
    "- Contextualized/Dynamic Representation Techniques (BERT, ELMo) \n",
    "\n",
    "\n",
    "### Bag of Words (BoW) \n",
    "\n",
    "Tokenization:\n",
    " - Split the text into words (tokens).  \n",
    "\n",
    "Vocabulary Building:\n",
    " - Create a vocabulary list of all unique words in the corpus.\n",
    "\n",
    "Vector Representation:\n",
    "   - For each document, create a vector where each element corresponds to a word in the vocabulary. \n",
    "     The value is the count of occurrences of that word in the document.\n",
    "\n",
    "\n",
    "\n",
    "**Example** \n",
    "\n",
    "Consider a corpus with the following two documents:\n",
    "1. “The cat sat on the mat.”\n",
    "2. “The dog sat on the log.”\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Tokenization:\n",
    "   - Document 1: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "   - Document 2: [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]\n",
    "\n",
    "\n",
    "2. Vocabulary Building:\n",
    "    - Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n",
    "\n",
    "\n",
    "3. Vector Representation:\n",
    "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "    The resulting BoW vectors are:\n",
    "   - Document 1: [2, 1, 1, 1, 1, 0, 0]\n",
    "   - Document 2: [2, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "\n",
    "\n",
    "###  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a \n",
    "word to a document in a collection or corpus. \n",
    "It is a fundamental technique in text processing that ranks the \n",
    "relevance of documents to a specific query, commonly applied in tasks such as document classification, search engine ranking, \n",
    "information retrieval, and text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493b6c9",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b289d",
   "metadata": {},
   "source": [
    "## Term Frequency (TF)\n",
    "\n",
    "- Term Frequency measures how frequently a term occurs in a document. \n",
    "Since every document is different in length, it is possible that a term would \n",
    "appear much more times in long documents than shorter ones.\n",
    "Thus, the term frequency is often divided by the document length (the total number of terms in the document) as a way of normalization:\n",
    "\n",
    " <pre> TF(t)=Number of times term t appears in a documentTotal number of terms in the document     \n",
    "           --------------------------------------------------------------------------------------\n",
    "                             Total number of terms in the document\n",
    "\n",
    "  </pre>\n",
    "\n",
    "\n",
    "\n",
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "- Inverse Document Frequency measures how important a term is. \n",
    " While computing TF, all terms are considered equally important.   \n",
    " However, certain terms, like “is”, “of”, and “that”, may appear a lot of times but have little importance.   \n",
    " Thus, we need to weigh down the frequent terms while scaling up the rare ones, by computing the following:\n",
    "\n",
    "   <pre>  IDF(t)=log(Total number of documents)  \n",
    "   -------------------------------------------\n",
    "    (Number of documents with term t in it)  </pre>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Steps to Calculate TF-IDF\n",
    "\n",
    "Step 1: TF (Term Frequency): Number of times a word appears in a document divided by the total number of words in that document.  \n",
    "Step 2: IDF (Inverse Document Frequency): Calculated as log(N / df), where: \n",
    "N is the total number of documents in the collection.   \n",
    "df is the number of documents containing the word.  \n",
    "Step 3: TF-IDF: The product of TF and IDF.\n",
    "\n",
    "Document Collection\n",
    "- Doc 1: “The sky is blue.”\n",
    "- Doc 2: “The sun is bright.”\n",
    "- Total documents (N): 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6d6bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b57ed4",
   "metadata": {},
   "source": [
    "## WORD2VEC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d2047",
   "metadata": {},
   "source": [
    "### Motivation Behind Word2Vec: the Need for Context-based Semantic Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b40ccf",
   "metadata": {},
   "source": [
    "- TF-IDF and BM25 are methods used in information retrieval to rank documents based on their relevance to a query.  \n",
    " While they provide useful measures for text analysis, they do not offer context-based “semantic” embeddings   \n",
    " (in the same way that Word2Vec or BERT embeddings do). Here’s why:\n",
    "\n",
    "- TF-IDF: This method calculates a weight for each word in a document, which increases with the number of times   \n",
    "the word appears in the document but decreases based on the frequency of the word across all documents. TF-IDF is  \n",
    " good at identifying important words in a document but doesn’t capture the meaning of the words or their   \n",
    " relationships with each other.   \n",
    " It’s more about word importance than word meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859ec08",
   "metadata": {},
   "source": [
    "- In contrast, semantic embeddings (like those from Word2Vec, BERT, etc.) are designed to capture the  \n",
    " meanings of words and their relationships to each other. These embeddings represent words as vectors in a  \n",
    "  way that words with similar meanings are located close to each other in the vector space, enabling the capture   \n",
    "  of semantic relationships and nuances in language.  \n",
    "\n",
    "\n",
    "- Therefore, while TF-IDF and BM25 are valuable tools for information retrieval and determining document   \n",
    "relevance,they do not provide semantic embeddings of words or phrases. They are more focused on word  \n",
    "occurrence and   \n",
    "frequency rather than on capturing the underlying meanings and relationships of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb65c8",
   "metadata": {},
   "source": [
    "### CORE IDEA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9fb13",
   "metadata": {},
   "source": [
    "- Word2Vec employs a shallow neural network, trained on a large textual corpus, to predict the context surrounding a given word.   \n",
    "The essence of Word2Vec lies in its ability to convert words into high-dimensional vectors. This representation allows the   \n",
    "algorithm to capture the meaning, semantic similarity, and relationships with surrounding text. A notable feature of Word2Vec   \n",
    "is its capacity to perform arithmetic operations with these vectors to reveal linguistic patterns, such as the famous analogy king - man + woman = queen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69727c45",
   "metadata": {},
   "source": [
    "### Word2Vec Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384daa5f",
   "metadata": {},
   "source": [
    "- Word2Vec offers two distinct architectures for training:\n",
    "\n",
    "  - Continuous Bag-of-Words (CBOW):\n",
    "      This model predicts a target word based on its context words.   \n",
    "        CBOW computes the conditional probability of a target word given the context words surrounding it across a window of size k.   \n",
    "        The input is a summation of the word vectors of the surrounding context words, with the output being the current word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd72d66",
   "metadata": {},
   "source": [
    "## **CODE EXAMPLE**\n",
    "## **RAG APPLICATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fb31b",
   "metadata": {},
   "source": [
    "In many real-world scenarios, organizations maintain extensive collections of \n",
    "proprietary documents, \n",
    "such as technical manuals, from which precise information must be extracted. \n",
    "This challenge is often \n",
    "analogous to locating a needle in a haystack, given the sheer volume and complexity of the content.\n",
    "While recent advancements, such as OpenAI’s introduction of GPT-4 Turbo, offer improved capabilities \n",
    "for processing lengthy documents, they are not without limitations. Notably, these models exhibit a \n",
    "tendency known as the “Lost in the Middle” phenomenon, wherein information positioned near the center \n",
    "of the context window is more likely to be overlooked or forgotten. This issue is akin to reading a \n",
    "comprehensive text such as the Bible, yet struggling to recall specific content from its middle chapters.\n",
    "To address this shortcoming, the RAG approach has been introduced. This method involves segmenting documents \n",
    "into discrete units—typically paragraphs—and creating an index for each. Upon receiving a query, the system \n",
    "efficiently identifies and retrieves the most relevant segments, which are then supplied to the language model. \n",
    "By narrowing the input to only the most pertinent information, this strategy mitigates cognitive overload within \n",
    "the model and substantially improves the relevance and accuracy of its responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ff58c",
   "metadata": {},
   "source": [
    "<img src=\"/Users/nanakwame/Downloads/indaba/IndabaX251/Foundational NLP/rag.png\" width=\"400\" alt=\"Rag Pipe\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ea5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Iterator, AsyncIterator, List\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "class KNUSTCsvDataLoader(BaseLoader):\n",
    "    \"\"\"A document loader that loads CSV documents.\"\"\"\n",
    "\n",
    "    def __init__(self, directory: str, encoding: str = 'latin1') -> None:\n",
    "        \"\"\"Initialize the loader with a directory.\n",
    "\n",
    "        Args:\n",
    "            directory: The path to the directory containing CSV files.\n",
    "            encoding: The encoding to use for reading CSV files (default: 'latin1').\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        return list(self.lazy_load())\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"A lazy loader that reads CSV files row by row.\"\"\"\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                try:\n",
    "                    # Load CSV file with specified encoding\n",
    "                    df = pd.read_csv(file_path, encoding=self.encoding)\n",
    "\n",
    "                    # Validate required columns\n",
    "                    required_columns = {\"Subject\", \"Question\", \"Response\"}\n",
    "                    if not required_columns.issubset(df.columns):\n",
    "                        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "                    # Iterate over rows in chunks\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=1000, encoding=self.encoding):\n",
    "                        for row in chunk.itertuples():\n",
    "                            yield Document(\n",
    "                                page_content=f\"Subject: {row.Subject}\\nQuestion: {row.Question}\\nResponse: {row.Response}\",\n",
    "                                metadata={\"subject\": row.Subject}\n",
    "                            )\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Encoding error in {file_path}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    async def alazy_load(self) -> AsyncIterator[Document]:\n",
    "        \"\"\"An async lazy loader that reads CSV files row by row.\"\"\"\n",
    "        for filename in os.listdir(self.directory):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(self.directory, filename)\n",
    "                try:\n",
    "                    # Read CSV file synchronously with specified encoding\n",
    "                    df = pd.read_csv(file_path, encoding=self.encoding)\n",
    "\n",
    "                    # Validate required columns\n",
    "                    required_columns = {\"Subject\", \"Question\", \"Response\"}\n",
    "                    if not required_columns.issubset(df.columns):\n",
    "                        raise ValueError(f\"Missing required columns in {file_path}\")\n",
    "\n",
    "                    # Yield documents asynchronously\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=1000, encoding=self.encoding):\n",
    "                        for row in chunk.itertuples():\n",
    "                            yield Document(\n",
    "                                page_content=f\"Subject: {row.Subject}\\nQuestion: {row.Question}\\nResponse: {row.Response}\",\n",
    "                                metadata={\"subject\": row.Subject}\n",
    "                            )\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Encoding error in {file_path}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "# Usage\n",
    "directory = \"/Users/nanakwame/Downloads/indaba/IndabaX251/Foundational NLP/data\"\n",
    "loader = KNUSTCsvDataLoader(directory, encoding='latin1')\n",
    "documents = list(loader.lazy_load())\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a83ec89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Subject: Time Travel Ethics\\nQuestion: Should time travelers be allowed to invest in the stock market?\\nResponse: No, it creates temporal imbalance and unfair economic advantage, violating ChronoCode 42B.'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Subject: Time Travel Ethics\\nQuestion: Can altering a minor historical event avoid paradoxes?\\nResponse: Even small changes can ripple unpredictably, risking causality collapse.'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Subject: Human Psychology\\nQuestion: Why do people procrastinate despite knowing the consequences?\\nResponse: ItÕs often due to fear of failure, task aversion, or dopamine-driven preference for instant rewards.'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Subject: Human Psychology\\nQuestion: How does social media affect self-esteem?\\nResponse: Frequent use can lead to social comparison, which may lower self-esteem over time.'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Subject: Programming\\nQuestion: WhatÕs the difference between a list and a tuple in Python?\\nResponse: Lists are mutable and dynamic; tuples are immutable and usually faster for fixed data.'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Subject: Programming\\nQuestion: Why is recursion considered risky in programming?\\nResponse: It can lead to stack overflow if not properly managed with base cases or depth limits.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32844ed",
   "metadata": {},
   "source": [
    "## SPLITTING DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf3a03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 35 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Split documents (optional, as your documents are likely short)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=50,\n",
    "        chunk_overlap=10,\n",
    "        length_function=len\n",
    "    )\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(split_docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc27cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Subject: Time Travel Ethics'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Question: Should time travelers be allowed to'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='to invest in the stock market?'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Response: No, it creates temporal imbalance and'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='and unfair economic advantage, violating'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='violating ChronoCode 42B.'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Subject: Time Travel Ethics'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Question: Can altering a minor historical event'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='event avoid paradoxes?'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='Response: Even small changes can ripple'),\n",
       " Document(metadata={'subject': 'Time Travel Ethics'}, page_content='ripple unpredictably, risking causality collapse.'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Subject: Human Psychology'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Question: Why do people procrastinate despite'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='despite knowing the consequences?'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Response: ItÕs often due to fear of failure, task'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='task aversion, or dopamine-driven preference for'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='for instant rewards.'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Subject: Human Psychology'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Question: How does social media affect'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='affect self-esteem?'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='Response: Frequent use can lead to social'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='to social comparison, which may lower self-esteem'),\n",
       " Document(metadata={'subject': 'Human Psychology'}, page_content='over time.'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Subject: Programming'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Question: WhatÕs the difference between a list'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='a list and a tuple in Python?'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Response: Lists are mutable and dynamic; tuples'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='tuples are immutable and usually faster for fixed'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='for fixed data.'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Subject: Programming'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Question: Why is recursion considered risky in'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='risky in programming?'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='Response: It can lead to stack overflow if not'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='if not properly managed with base cases or depth'),\n",
       " Document(metadata={'subject': 'Programming'}, page_content='or depth limits.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0ac1b",
   "metadata": {},
   "source": [
    " ## CREATE EMBEDDINGS AND VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3fbaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store_file = FAISS.from_documents(split_docs, embeddings)\n",
    "print(\"Vector store created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d3cff05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x121475550>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x121475e80>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "879c3cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x121671400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748f2fb",
   "metadata": {},
   "source": [
    "## SETUP RETRIEVER AND LANGUAGE MODEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a0b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "retriever = vector_store_file.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        temperature=0.7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b07a1",
   "metadata": {},
   "source": [
    "## SETUP PROMPT TEMPLATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer, say so.\n",
    "    Context: {context}\n",
    "    Question: {input}\n",
    "    Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"input\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  CREATE DOCUMENT CHAIN AND RAG CHAIN\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e220269",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9898f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(rag_chain, question: str):\n",
    "    \"\"\"Query the RAG pipeline and return the answer with sources.\"\"\"\n",
    "    result = rag_chain.invoke({\"input\": question})\n",
    "    answer = result[\"answer\"]\n",
    "    sources = result[\"context\"]\n",
    "    return answer, sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47a734d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: WHO IS THE PRESIDENT OF GHANA\n",
      "Answer: I don't know the answer.\n",
      "Sources:\n",
      "1. Subject: Human Psychology...\n",
      "2. Subject: Human Psychology...\n",
      "3. Subject: Programming...\n"
     ]
    }
   ],
   "source": [
    "# Example queries\n",
    "questions = [\n",
    "        \"WHO IS THE PRESIDENT OF GHANA\",\n",
    "    ]\n",
    "\n",
    "for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        answer, sources = query_rag(rag_chain, question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"Sources:\")\n",
    "        for i, doc in enumerate(sources, 1):\n",
    "            print(f\"{i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bce3184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: SHOULD TIME TRAVELERS BE ALLOWED TO INVEST IN STOCK MARKET\n",
      "Answer: The question of whether time travelers should be allowed to invest in the stock market raises significant ethical and practical considerations. On one hand, allowing time travelers to invest could lead to market manipulation, as they would have access to future information that could give them an unfair advantage over regular investors. This could destabilize financial markets and undermine the principles of fairness and equality in investing.\n",
      "\n",
      "On the other hand, proponents might argue that time travelers, like any other individuals, should have the right to participate in the market. However, the potential consequences of their investments could have far-reaching effects on the economy and society.\n",
      "\n",
      "Ultimately, the decision would depend on the ethical frameworks and regulations established in a society that accommodates time travel. It may be prudent to implement strict guidelines or prohibitions to prevent any adverse impacts on the financial system.\n",
      "Sources:\n",
      "1. Question: Should time travelers be allowed to...\n",
      "2. to invest in the stock market?...\n",
      "3. Subject: Time Travel Ethics...\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "        \"SHOULD TIME TRAVELERS BE ALLOWED TO INVEST IN STOCK MARKET\",\n",
    "    ]\n",
    "\n",
    "for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        answer, sources = query_rag(rag_chain, question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"Sources:\")\n",
    "        for i, doc in enumerate(sources, 1):\n",
    "            print(f\"{i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f495f7",
   "metadata": {},
   "source": [
    "## **CODE EXAMPLE**\n",
    "## **NAMED-ENTITY RECOGNITION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc93139",
   "metadata": {},
   "source": [
    "Name Entity recognition(NER) is a subtask of Natural language process(NLP) which focuses on identifying and grouping entities within a text or document.   \n",
    "Entities present specific objects or names such as Persons, organizations, dates and times, countries, drugs, and various unique information within a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39735c2",
   "metadata": {},
   "source": [
    "## Types-of-NER-models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32708a",
   "metadata": {},
   "source": [
    "NER is applied in various sectors of our daily lives; so of these applied areas are:\n",
    "\n",
    "- Rule based NER models\n",
    "- Machine learning (ML) models\n",
    "- Deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7c28e",
   "metadata": {},
   "source": [
    "Rule-based-model\n",
    "A rule-based NER model is a .....\n",
    "\n",
    "Examples of rule based approaches\n",
    "\n",
    "a. Spacy Entity\n",
    "\n",
    "b. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d894ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import svgling\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good. can arthur go to the Ghana\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97245c5d",
   "metadata": {},
   "source": [
    "## Code-Examples-For-Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "%%capture\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_rb_ner(patterns,text,model_name='en'):\n",
    "\n",
    "  #create a blank model\n",
    "  nlp = spacy.blank(model_name)\n",
    "\n",
    "  #create an new entity to for NER\n",
    "  ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "  ruler.add_patterns(patterns)\n",
    "\n",
    "  #extract components from the text\n",
    "  doc = nlp(text)\n",
    "  # print(doc)\n",
    "  for ent in doc.ents:\n",
    "      print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [{\"label\": \"AGE\", \"pattern\": [{\"like_num\": True}, {\"lower\": \"years\"}, {\"lower\": \"old\"}]}]\n",
    "text = \"John is 25 years old\"\n",
    "spacy_rb_ner(patterns,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e2d7d",
   "metadata": {},
   "source": [
    "## Exercises 1  \n",
    "\n",
    "Please list some advantages and disadvantages as you try out these rule based name entity recognition models.\n",
    "\n",
    "Advantages 1. 2.\n",
    "\n",
    "Disadvantages 1. 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy\n",
    "#customize your own pattern and provide your text for testing\n",
    "pattern =[]\n",
    "text = \"\"\n",
    "spacy_rb_ner(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76987783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard\n",
    "\n",
    "#1. dataset extraction from huggingface\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"remakia/drugs-dictionary\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "#2. load the json dictionary\n",
    "import json\n",
    "def read_json(json_file):\n",
    "\n",
    "  return 0\n",
    "\n",
    "#3. convert the drug dict into patterns\n",
    "def generate_patern(drug_dict):\n",
    "\n",
    "  return 0\n",
    "\n",
    "json_file = \"drug.json\"\n",
    "drug_dict = read_json(json_file)\n",
    "pattern =generate_patern(drug_dict)\n",
    "text = \"Perfusion d'une ampoule de prexidine de lithium et introduction d'un antihistaminique par Cétirizine 10 mg x 2 par jour, avec diminution puis disparition de l'oedème.\"\n",
    "text = text.lower()\n",
    "spacy_rb_ner(pattern,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce6357",
   "metadata": {},
   "source": [
    "## Machine Learning-model\n",
    "Conditional Random Fields - (provide explanation)  \n",
    "SVM - (provide explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d14e31a",
   "metadata": {},
   "source": [
    "Example-code\n",
    "Below is an example code of Spacy, a machine learning NER model train with the theory of conditional random fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "content =\"Esi is a 27-years-old individual who came back home from school. she is meant to go back to school soon to see her friends. Have you heard from Kwame because the last time i spoke to him, he said he was going to the Ghana, Kigali\"\n",
    "\n",
    "doc = nlp(content)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11490b34",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "We will train our own Spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf784f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_data():\n",
    "     return 0 \n",
    "\n",
    "def normalization():\n",
    "    return 0\n",
    "\n",
    "def dataset_preprocessing(TRAIN_DATA,ner):\n",
    "\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_iter= 30 #number of times you want the model to train \n",
    "model= \"name-of-blank-model\"\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "\n",
    "#create and set up a pipeline \n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "#data preprocessing\n",
    "TRAIN_DATA=processed_data()\n",
    "dataset_preprocessing(TRAIN_DATA)\n",
    "\n",
    "#training \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in tqdm(TRAIN_DATA):\n",
    "            nlp.update(\n",
    "                [text],  \n",
    "                [annotations],  \n",
    "                drop=0.5,  \n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026a6ee",
   "metadata": {},
   "source": [
    "## **Conclusion and Comments From Participants**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44145ab2",
   "metadata": {},
   "source": [
    "# **Facilitator(s) Details**\n",
    "\n",
    "**Facilitator(s):**\n",
    "\n",
    "*   Name: FELIX TETTEH AKWERH\n",
    "*   Email: felix.akwerh@knust.edu.gh\n",
    "\n",
    "\n",
    "\n",
    "*   Name: ADWOA ASANTEWAA BREMANG\n",
    "*   Email: adwoabremang@gmail.com\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdssenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
