{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8bd3a486-36c6-4c20-94d7-a30271ad011d",
      "metadata": {
        "id": "8bd3a486-36c6-4c20-94d7-a30271ad011d"
      },
      "source": [
        "# **NAME ENTITY RECOGNITION**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4348ff2-2146-4305-8999-191a8e9c4cc7",
      "metadata": {
        "id": "a4348ff2-2146-4305-8999-191a8e9c4cc7"
      },
      "source": [
        "# **Table of Contents**\n",
        "\n",
        "1.   [Introduction](#Introduction)\n",
        "2.   [Prerequisites](#Prerequisites)\n",
        "3.   [Applications](#Applications)\n",
        "4.   [Rule-based-model](#Rule-based-models)\n",
        "5.   [Machinelearning-model](#Machinelearning-model)\n",
        "6.   [References](#References)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b8a541-a2e4-4886-b42e-4126d9fb9d7a",
      "metadata": {
        "id": "a1b8a541-a2e4-4886-b42e-4126d9fb9d7a"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "Name Entity recognition(NER) is a subtask of Natural language process(NLP) which focuses on identifying and grouping entities within a text or document.\n",
        "Entities present specific objects or names such as Persons, organizations, dates and times, countries, drugs, and various unique information within a document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b84706-8034-464e-aeb9-3a40249370df",
      "metadata": {
        "id": "70b84706-8034-464e-aeb9-3a40249370df"
      },
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "Before diving into coding exercises and examples, one should have basic knowledge in the following:\n",
        "1. python programming\n",
        "2. writing and creating algorithms\n",
        "3. problem solving, critical thinking and creative skills"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a37af23-9129-4e73-88a0-b8714f639416",
      "metadata": {
        "id": "3a37af23-9129-4e73-88a0-b8714f639416"
      },
      "source": [
        "<a id='guide'></a>\n",
        "## **Applications**\n",
        "\n",
        "NER is applied in various sectors of our daily lives; so of these applied areas are:\n",
        "1. Spam detection in emails\n",
        "2. Use for search engines\n",
        "3. Finance and Banking\n",
        "4. Chatbots generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ceb56a",
      "metadata": {
        "id": "26ceb56a"
      },
      "source": [
        "<a id='guide'></a>\n",
        "## **Types-of-NER-models**\n",
        "\n",
        "NER is applied in various sectors of our daily lives; so of these applied areas are:\n",
        "1. Rule based NER models\n",
        "2. Machine learning (ML) models\n",
        "3. Deep learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b82436ec-e867-4841-966b-9c3fd6280850",
      "metadata": {
        "id": "b82436ec-e867-4841-966b-9c3fd6280850"
      },
      "source": [
        "## **Rule-based-model**\n",
        "A rule-based NER model is a system that relies on manually crafted linguistic rules, such as regular expressions, token patterns, and lexicons, to identify and classify named entities in text without the need for training data.\n",
        "\n",
        "Examples of rule based approaches\n",
        "\n",
        "a. Spacy Entity\n",
        "\n",
        "b. NLTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mohSp-SK2kVw",
      "metadata": {
        "id": "mohSp-SK2kVw"
      },
      "source": [
        "### **code-examples-for-NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V5AdgzpmCEjM",
      "metadata": {
        "id": "V5AdgzpmCEjM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# review_comment: !pip install svgling nltk kagglehub\n",
        "!pip install svgling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "52e1a00c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "52e1a00c",
        "outputId": "300364f7-5ce3-4428-9bb7-4d023822a075"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": [
              "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,976.0,168.0\" width=\"976px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"3.27869%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">At</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.63934%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.7377%\" x=\"3.27869%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">eight</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.14754%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.37705%\" x=\"9.01639%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">o'clock</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.7049%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"16.3934%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.0328%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.19672%\" x=\"19.6721%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Thursday</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.7705%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.37705%\" x=\"27.8689%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">morning</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.5574%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.55738%\" x=\"35.2459%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Arthur</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.5246%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"41.8033%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">did</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.8525%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"45.9016%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">n't</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.9508%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">feel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.459%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"54.918%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">very</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.377%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"59.8361%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">good</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.2951%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.45902%\" x=\"64.7541%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.9836%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"67.2131%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">can</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.2623%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.55738%\" x=\"71.3115%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">arthur</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.5902%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"77.8689%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">go</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5082%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"81.1475%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.7869%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"84.4262%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.4754%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.4754%\" x=\"88.5246%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Ghana</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.2623%\" y1=\"20px\" y2=\"48px\" /></svg>"
            ],
            "text/plain": [
              "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.'), ('can', 'MD'), ('arthur', 'VB'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), Tree('ORGANIZATION', [('Ghana', 'NNP')])])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import svgling\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good. can arthur go to the Ghana\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "\n",
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ef8ce6",
      "metadata": {
        "id": "f3ef8ce6"
      },
      "source": [
        "### **code-examples-for-Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c90da5ca",
      "metadata": {
        "id": "c90da5ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%%capture` not found.\n"
          ]
        }
      ],
      "source": [
        "#review_comment: the magic word should be at the first line of the cell %%capture\n",
        "#installations\n",
        "%%capture\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8b16d546",
      "metadata": {
        "id": "8b16d546"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "# import EntityRuler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d53eed00",
      "metadata": {
        "id": "d53eed00"
      },
      "outputs": [],
      "source": [
        "\n",
        "def spacy_rb_ner(patterns,text,model_name='en'):\n",
        "\n",
        "  #create a blank model\n",
        "  nlp = spacy.blank(model_name)\n",
        "\n",
        "  #create an new entity to for NER\n",
        "  ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "  ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "  #extract components from the text\n",
        "  doc = nlp(text)\n",
        "  print(doc)\n",
        "  for ent in doc.ents:\n",
        "      print(ent.text, ent.label_)\n",
        "\n",
        "  displacy.render(doc, style=\"ent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f8a1b4a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "f8a1b4a7",
        "outputId": "83f29995-8c33-42bc-9f49-d1ab0137e930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John is 25 years old\n",
            "25 years old AGE\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/codespace/.local/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m patterns = [{\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAGE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpattern\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mlike_num\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}, {\u001b[33m\"\u001b[39m\u001b[33mlower\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33myears\u001b[39m\u001b[33m\"\u001b[39m}, {\u001b[33m\"\u001b[39m\u001b[33mlower\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mold\u001b[39m\u001b[33m\"\u001b[39m}]}]\n\u001b[32m      2\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mJohn is 25 years old\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mspacy_rb_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mspacy_rb_ner\u001b[39m\u001b[34m(patterns, text, model_name)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ent.text, ent.label_)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/codespace/.local/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "patterns = [{\"label\": \"AGE\", \"pattern\": [{\"like_num\": True}, {\"lower\": \"years\"}, {\"lower\": \"old\"}]}]\n",
        "text = \"John is 25 years old\"\n",
        "spacy_rb_ner(patterns,text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80aedc0f",
      "metadata": {
        "id": "80aedc0f"
      },
      "source": [
        "### **Exercises 1**\n",
        "\n",
        "Please list some advantages and disadvantages as you try out these rule based name entity recognition models.\n",
        "\n",
        "Advantages\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "1.\n",
        "\n",
        "2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9cd2de",
      "metadata": {
        "id": "1d9cd2de"
      },
      "source": [
        "### **Exercises 2**\n",
        "\n",
        "Let's try our hands on sample exercises for better understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "388b0feb",
      "metadata": {
        "id": "388b0feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/pipeline/entityruler.py:366: UserWarning: [W036] The component 'entity_ruler' does not have any patterns defined.\n",
            "  warnings.warn(Warnings.W036.format(name=self.name))\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/displacy/__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  warnings.warn(Warnings.W006)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/codespace/.local/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m pattern =[]\n\u001b[32m      4\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mspacy_rb_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mspacy_rb_ner\u001b[39m\u001b[34m(patterns, text, model_name)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ent.text, ent.label_)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/codespace/.local/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "# Easy\n",
        "#customize your own pattern and provide your text for testing\n",
        "pattern =[]\n",
        "text = \"\"\n",
        "spacy_rb_ner(pattern,text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1cfecc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "collapsed": true,
        "id": "9d1cfecc",
        "outputId": "914368c7-4936-46b6-ad2b-b90d0ba7ab54"
      },
      "outputs": [],
      "source": [
        "#Hard\n",
        "\n",
        "#1. dataset extraction from huggingface\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"remakia/drugs-dictionary\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "!cp -r /root/.cache/kagglehub/datasets/remakia/drugs-dictionary/versions/1/ .\n",
        "!mv 1/drugs.json drug.json\n",
        "\n",
        "#2. load the json dictionary\n",
        "import json\n",
        "\n",
        "# review_comment: you were supposed to define a class according to the code in the main function\n",
        "# class SpacyNEREntity:\n",
        "#   def __init__(self,json_file):\n",
        "#     self.json_file = json_file\n",
        "\n",
        "#   def read_json(self):\n",
        "\n",
        "#     return 0\n",
        "\n",
        "#   def generate_pattern(self,collections):\n",
        "    \n",
        "#     return 0\n",
        "\n",
        "def read_json(json_file):\n",
        "\n",
        "  return 0\n",
        "\n",
        "#3. convert the pattern dictionary into patterns\n",
        "def generate_patern(collection):\n",
        "\n",
        "  return 0\n",
        "\n",
        "#4. test out generated pattern in main\n",
        "def main():\n",
        "  json_file = \"/content/entities_dataset.json\"\n",
        "  spacy_enr_entity =SpacyNEREntity()\n",
        "\n",
        "  pattern_dict = spacy_enr_entity.read_json(json_file)\n",
        "  pattern =spacy_enr_entity.generate_pattern(pattern_dict)\n",
        "  text = \"Asantewaa went to kotoka international airport\"\n",
        "  text = text.lower()\n",
        "  spacy_rb_ner(pattern,text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rk9kas6ztSAV",
      "metadata": {
        "id": "Rk9kas6ztSAV"
      },
      "source": [
        "solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XRBcZT4-tT3C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "XRBcZT4-tT3C",
        "outputId": "35909228-965a-4177-8304-e23a7f1df0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'GPE01', 'pattern': [{'lower': 'ghana'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'nigeria'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'kenya'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'france'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'united'}, {'lower': 'states'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'sahara'}, {'lower': 'desert'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'mount'}, {'lower': 'kilimanjaro'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'lake'}, {'lower': 'victoria'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'atlantic'}, {'lower': 'ocean'}]}, {'label': 'PER01', 'pattern': [{'lower': 'kwame'}, {'lower': 'nkrumah'}]}, {'label': 'PER01', 'pattern': [{'lower': 'ngũgĩ'}, {'lower': 'wa'}, {'lower': \"thiong'o\"}]}, {'label': 'PER01', 'pattern': [{'lower': 'chinua'}, {'lower': 'achebe'}]}, {'label': 'PER01', 'pattern': [{'lower': 'yaa'}, {'lower': 'asantewaa'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'united'}, {'lower': 'nations'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'world'}, {'lower': 'health'}, {'lower': 'organization'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'google'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'ashesi'}, {'lower': 'university'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'world'}, {'lower': 'war'}, {'lower': 'ii'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'african'}, {'lower': 'union'}, {'lower': 'summit'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'cop27'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'olympic'}, {'lower': 'games'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'swahili'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'zulu'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'yoruba'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'igbo'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'twi'}]}, {'label': 'WOA01', 'pattern': [{'lower': 'things'}, {'lower': 'fall'}, {'lower': 'apart'}]}, {'label': 'WOA01', 'pattern': [{'lower': 'petals'}, {'lower': 'of'}, {'lower': 'blood'}]}, {'label': 'WOA01', 'pattern': [{'lower': 'the'}, {'lower': 'beautiful'}, {'lower': 'ones'}, {'lower': 'are'}, {'lower': 'not'}, {'lower': 'yet'}, {'lower': 'born'}]}, {'label': 'LAW01', 'pattern': [{'lower': 'data'}, {'lower': 'protection'}, {'lower': 'act'}]}, {'label': 'LAW01', 'pattern': [{'lower': 'patriot'}, {'lower': 'act'}]}, {'label': 'LAW01', 'pattern': [{'lower': 'right'}, {'lower': 'to'}, {'lower': 'information'}, {'lower': 'bill'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'iphone'}, {'lower': '14'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'samsung'}, {'lower': 'galaxy'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'tesla'}, {'lower': 'model'}, {'lower': 's'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'playstation'}, {'lower': '5'}]}, {'label': 'FAC01', 'pattern': [{'lower': 'kotoka'}, {'lower': 'international'}, {'lower': 'airport'}]}, {'label': 'FAC01', 'pattern': [{'lower': 'accra'}, {'lower': 'sports'}, {'lower': 'stadium'}]}, {'label': 'FAC01', 'pattern': [{'lower': 'nairobi'}, {'lower': 'terminal'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'independence'}, {'lower': 'day'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'christmas'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'africa'}, {'lower': 'day'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'new'}, {'lower': \"year's\"}, {'lower': 'day'}]}]\n",
            "asantewaa went to kotoka international airport\n",
            "kotoka international airport FAC01\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (/home/codespace/.local/lib/python3.12/site-packages/IPython/core/display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     spacy_rb_ner(pattern,text)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mAsantewaa went to kotoka international airport\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m text = text.lower()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mspacy_rb_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mspacy_rb_ner\u001b[39m\u001b[34m(patterns, text, model_name)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ent.text, ent.label_)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/spacy/displacy/__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (/home/codespace/.local/lib/python3.12/site-packages/IPython/core/display.py)"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "class SpacyNEREntity:\n",
        "  def __init__(self,json_file):\n",
        "    self.json_file = json_file\n",
        "\n",
        "  def read_json(self):\n",
        "\n",
        "    try:\n",
        "        with open(self.json_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            # print(data)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'file.json' not found. Make sure the file exists in the correct path.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error: Could not decode JSON from the file. Make sure the file is a valid JSON.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "  def generate_pattern(self,collections):\n",
        "    pattern=[]\n",
        "    patterns=[]\n",
        "    for collection in collections.items():\n",
        "      # print(collection[0])\n",
        "      for i in collection[1]:\n",
        "        # print(i)\n",
        "        components = i.split()\n",
        "        if len(components)>1:\n",
        "\n",
        "          pattern=[{\"lower\":items.lower()} for items in components]\n",
        "          # print(pattern)\n",
        "        elif len(components)==1:\n",
        "          # print(components)\n",
        "\n",
        "          pattern=[{\"lower\":components[0].lower()}]\n",
        "        patterns.append({\"label\":collection[0],\"pattern\":pattern})\n",
        "    print(patterns)\n",
        "    return patterns\n",
        "\n",
        "def main():\n",
        "    json_file = \"./entities_dataset.json\"\n",
        "    # review_comment: you were supposed to put json_file as argument\n",
        "    # spacy_enr_entity =SpacyNEREntity(json_file)\n",
        "    spacy_enr_entity =SpacyNEREntity()\n",
        "\n",
        "    # review_comment: the .read_json() method does not require any argument\n",
        "    #pattern_dict = spacy_enr_entity.read_json()\n",
        "    pattern_dict = spacy_enr_entity.read_json(json_file)\n",
        "    pattern =spacy_enr_entity.generate_pattern(pattern_dict)\n",
        "    text = \"Asantewaa went to kotoka international airport\"\n",
        "    text = text.lower()\n",
        "    spacy_rb_ner(pattern,text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2FnYxQbKtinN",
      "metadata": {
        "collapsed": true,
        "id": "2FnYxQbKtinN"
      },
      "outputs": [],
      "source": [
        "#review_comment: the read_json function has to be defined\n",
        "# def read_json(file):\n",
        "#     try:\n",
        "#         with open(file, 'r') as f:\n",
        "#             data = json.load(f)\n",
        "#             # print(data)\n",
        "#     except FileNotFoundError:\n",
        "#         print(\"Error: 'file.json' not found. Make sure the file exists in the correct path.\")\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(\"Error: Could not decode JSON from the file. Make sure the file is a valid JSON.\")\n",
        "\n",
        "#     return data\n",
        "\n",
        "collections=read_json(\"entities_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "s37Iwd16t0ze",
      "metadata": {
        "id": "s37Iwd16t0ze"
      },
      "outputs": [],
      "source": [
        "def generate_patern(collections):\n",
        "  pattern=[]\n",
        "  patterns=[]\n",
        "  for collection in collections.items():\n",
        "    print(collection[0])\n",
        "    for i in collection[1]:\n",
        "      components = i.split()\n",
        "      if len(components)>1:\n",
        "        pattern=[{\"lower\":items} for items in components]\n",
        "      else:\n",
        "        pattern=[{\"lower\":components}]\n",
        "    patterns.append({\"label\":collection[0],\"pattern\":pattern})\n",
        "    print(pattern)\n",
        "\n",
        "  return patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "mMNZI2QouGzm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mMNZI2QouGzm",
        "outputId": "35895095-6202-46f1-cfb4-fc6f6788cdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPE01\n",
            "[{'lower': 'UNITED'}, {'lower': 'STATES'}]\n",
            "LOC01\n",
            "[{'lower': 'ATLANTIC'}, {'lower': 'OCEAN'}]\n",
            "PER01\n",
            "[{'lower': 'YAA'}, {'lower': 'ASANTEWAA'}]\n",
            "ORG01\n",
            "[{'lower': 'ASHESI'}, {'lower': 'UNIVERSITY'}]\n",
            "EVENT01\n",
            "[{'lower': 'OLYMPIC'}, {'lower': 'GAMES'}]\n",
            "LANG01\n",
            "[{'lower': ['TWI']}]\n",
            "WOA01\n",
            "[{'lower': 'THE'}, {'lower': 'BEAUTIFUL'}, {'lower': 'ONES'}, {'lower': 'ARE'}, {'lower': 'NOT'}, {'lower': 'YET'}, {'lower': 'BORN'}]\n",
            "LAW01\n",
            "[{'lower': 'RIGHT'}, {'lower': 'TO'}, {'lower': 'INFORMATION'}, {'lower': 'BILL'}]\n",
            "PROD01\n",
            "[{'lower': 'PLAYSTATION'}, {'lower': '5'}]\n",
            "FAC01\n",
            "[{'lower': 'NAIROBI'}, {'lower': 'TERMINAL'}]\n",
            "DATE01\n",
            "[{'lower': 'NEW'}, {'lower': \"YEAR'S\"}, {'lower': 'DAY'}]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'GPE01', 'pattern': [{'lower': 'UNITED'}, {'lower': 'STATES'}]},\n",
              " {'label': 'LOC01', 'pattern': [{'lower': 'ATLANTIC'}, {'lower': 'OCEAN'}]},\n",
              " {'label': 'PER01', 'pattern': [{'lower': 'YAA'}, {'lower': 'ASANTEWAA'}]},\n",
              " {'label': 'ORG01', 'pattern': [{'lower': 'ASHESI'}, {'lower': 'UNIVERSITY'}]},\n",
              " {'label': 'EVENT01', 'pattern': [{'lower': 'OLYMPIC'}, {'lower': 'GAMES'}]},\n",
              " {'label': 'LANG01', 'pattern': [{'lower': ['TWI']}]},\n",
              " {'label': 'WOA01',\n",
              "  'pattern': [{'lower': 'THE'},\n",
              "   {'lower': 'BEAUTIFUL'},\n",
              "   {'lower': 'ONES'},\n",
              "   {'lower': 'ARE'},\n",
              "   {'lower': 'NOT'},\n",
              "   {'lower': 'YET'},\n",
              "   {'lower': 'BORN'}]},\n",
              " {'label': 'LAW01',\n",
              "  'pattern': [{'lower': 'RIGHT'},\n",
              "   {'lower': 'TO'},\n",
              "   {'lower': 'INFORMATION'},\n",
              "   {'lower': 'BILL'}]},\n",
              " {'label': 'PROD01', 'pattern': [{'lower': 'PLAYSTATION'}, {'lower': '5'}]},\n",
              " {'label': 'FAC01', 'pattern': [{'lower': 'NAIROBI'}, {'lower': 'TERMINAL'}]},\n",
              " {'label': 'DATE01',\n",
              "  'pattern': [{'lower': 'NEW'}, {'lower': \"YEAR'S\"}, {'lower': 'DAY'}]}]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_patern(collections)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18658ab7",
      "metadata": {
        "id": "18658ab7"
      },
      "source": [
        "## **Machinelearning-model**\n",
        "\n",
        "1. Conditional Random Fields - Conditional Random Fields is a probalistic model that determines or predicts labels based on a given sequence/Input.\n",
        "\n",
        "It models the conditional probability of a label sequence Y, given an input sequence X:\n",
        "\n",
        "P(Y | X)\n",
        "\n",
        "\n",
        "2.\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a supervised machine learning algorithm that finds the best hyperplane to separate data points into different classes.\n",
        "\n",
        "In **Named Entity Recognition (NER)**, SVM classifies each token (word) independently, based on its features.\n",
        "Unlike **Conditional Random Fields (CRFs)**, SVMs do **not** consider the previous or future tags when making predictions.\n",
        "This can limit their ability to model the context and dependencies between labels in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce78b08",
      "metadata": {
        "id": "0ce78b08"
      },
      "source": [
        "### **Example-code**\n",
        "\n",
        "Below is an example code of Spacy, a machine learning NER model train with the theory of conditional random fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ab200a63",
      "metadata": {
        "id": "ab200a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27-years-old 9 21 DATE\n",
            "Kwame 144 149 PERSON\n",
            "Ghana 216 221 GPE\n",
            "Kigali 223 229 GPE\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import requests\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "content =\"Esi is a 27-years-old individual who came back home from school. she is meant to go back to school soon to see her friends. Have you heard from Kwame because the last time i spoke to him, he said he was going to the Ghana, Kigali\"\n",
        "\n",
        "doc = nlp(content)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3da591e",
      "metadata": {
        "id": "b3da591e"
      },
      "source": [
        "### **Exercise 1**\n",
        "\n",
        "We will train our own Spacy model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NuCd8_f8OKtX",
      "metadata": {
        "id": "NuCd8_f8OKtX"
      },
      "source": [
        "Installations & downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "LEw2MBgJLhwj",
      "metadata": {
        "id": "LEw2MBgJLhwj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nervaluate in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.3.1)\n",
            "Requirement already satisfied: pandas>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nervaluate) (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.3.0->nervaluate) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.3.0->nervaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.3.0->nervaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=2.3.0->nervaluate) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.3.0->nervaluate) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "--2025-06-06 11:21:33--  https://sls.csail.mit.edu/downloads/restaurant/restauranttest.bio\n",
            "Resolving sls.csail.mit.edu (sls.csail.mit.edu)... 128.52.131.233\n",
            "Connecting to sls.csail.mit.edu (sls.csail.mit.edu)|128.52.131.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155722 (152K)\n",
            "Saving to: ‘restauranttest.bio.1’\n",
            "\n",
            "restauranttest.bio. 100%[===================>] 152.07K   690KB/s    in 0.2s    \n",
            "\n",
            "2025-06-06 11:21:34 (690 KB/s) - ‘restauranttest.bio.1’ saved [155722/155722]\n",
            "\n",
            "--2025-06-06 11:21:34--  https://sls.csail.mit.edu/downloads/restaurant/restauranttrain.bio\n",
            "Resolving sls.csail.mit.edu (sls.csail.mit.edu)... 128.52.131.233\n",
            "Connecting to sls.csail.mit.edu (sls.csail.mit.edu)|128.52.131.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 759757 (742K)\n",
            "Saving to: ‘restauranttrain.bio.2’\n",
            "\n",
            "restauranttrain.bio 100%[===================>] 741.95K  2.07MB/s    in 0.3s    \n",
            "\n",
            "2025-06-06 11:21:35 (2.07 MB/s) - ‘restauranttrain.bio.2’ saved [759757/759757]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nervaluate\n",
        "!wget https://sls.csail.mit.edu/downloads/restaurant/restauranttest.bio\n",
        "!wget https://sls.csail.mit.edu/downloads/restaurant/restauranttrain.bio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g6xMlsKJONeS",
      "metadata": {
        "id": "g6xMlsKJONeS"
      },
      "source": [
        "Importations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Ezl1qmGSui3G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ezl1qmGSui3G",
        "outputId": "9d661503-d7ae-4de3-e903-df96dd606a78"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training.example import Example # Correct import for Example\n",
        "from spacy.util import minibatch\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from nervaluate import Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RB73DWNtOSb5",
      "metadata": {
        "id": "RB73DWNtOSb5"
      },
      "source": [
        "Helper class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "wfcW4xhaOSF0",
      "metadata": {
        "id": "wfcW4xhaOSF0"
      },
      "outputs": [],
      "source": [
        "# anything that is **not** a letter or digit → replace\n",
        "chars_to_ignore_regex = r\"[^A-Za-z0-9]+\"\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"\n",
        "    Replace non-alphanumeric runs with a single space and lowercase the result.\n",
        "    \"\"\"\n",
        "    cleaned = re.sub(chars_to_ignore_regex, \" \", text)   # keep letters & digits\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)               # squeeze repeated spaces\n",
        "    return cleaned.lower().strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-4EPi8BOWIZ",
      "metadata": {
        "id": "V-4EPi8BOWIZ"
      },
      "source": [
        "Data processing class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nKr39DsLSFaO",
      "metadata": {
        "id": "nKr39DsLSFaO"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Sample format for a spacy NER dataset\n",
        "TRAIN_DATA = [\n",
        "    (\n",
        "        \"a four star restaurant with a bar\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "                (2, 11, \"Rating\"),   # \"four star\"\n",
        "                (23, 29, \"Location\"), # \"with a\"\n",
        "                (30, 33, \"Amenity\")   # \"bar\"\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "{'O': {'start': 12, 'end': 23},\n",
        "'Rating': {'start': 2, 'end': 12, 'text': 'a four star '},\n",
        "'Location': {'start': 23, 'end': 30},\n",
        " 'Amenity': {'start': 30, 'end': 34}}\n",
        "---------------------------------------------------------------------------\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UQc-BVD61THh",
      "metadata": {
        "id": "UQc-BVD61THh"
      },
      "source": [
        "Dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "JIJkc78cOD4p",
      "metadata": {
        "id": "JIJkc78cOD4p"
      },
      "outputs": [],
      "source": [
        "class NerDataProcessor:\n",
        "    \"\"\"\n",
        "    Class to handle reading and processing of .bio (CoNLL-style) NER data.\n",
        "    \"\"\"\n",
        "    def __init__(self, bio_file_path):\n",
        "        self.bio_file_path = Path(bio_file_path)\n",
        "        if not self.bio_file_path.is_file():\n",
        "            raise FileNotFoundError(f\"BIO file not found at: {self.bio_file_path}\")\n",
        "\n",
        "\n",
        "    def read_bio_file(self):\n",
        "      \"\"\"\n",
        "      Reads a CoNLL-style .bio file and returns a list of\n",
        "      (text, {\"entities\": [(start, end, label), …]}) tuples.\n",
        "      \"\"\"\n",
        "      TRAIN_DATA , entities =[], []\n",
        "      entity_start_char_time, entity_end_char_time = 0,0\n",
        "      sentence ,tag_idx=\"\", \"\"\n",
        "      stored_entities={}\n",
        "\n",
        "      with open(self.bio_file_path, mode='r',encoding=\"utf8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "\n",
        "      for num,line in enumerate(lines):\n",
        "        line= line.strip()\n",
        "\n",
        "        part =line.split()\n",
        "        if len(part)==2:\n",
        "\n",
        "          token, tag = line.split()\n",
        "          # review_comment: sentence += token + \" \" it should add token not tag pls check\n",
        "          sentence = sentence + tag + \" \"\n",
        "\n",
        "          if tag:\n",
        "            entity_end_char_time= entity_start_char_time + len(tag.strip())\n",
        "            if token.startswith(\"O\"):\n",
        "              entity_start_char_time=entity_end_char_time+1\n",
        "              continue\n",
        "            elif token.split(\"-\"):\n",
        "              idx= token.split(\"-\")[0]\n",
        "\n",
        "              if token.split(\"-\")[1] in stored_entities:\n",
        "                stored_entities[token.split(\"-\")[1]][\"end\"]=entity_end_char_time\n",
        "                if idx==\"B\": #new changes\n",
        "                    stored_entities[token.split(\"-\")[1]][\"start\"]=entity_start_char_time\n",
        "\n",
        "              else:\n",
        "                  stored_entities[token.split(\"-\")[1]]={\"start\":entity_start_char_time,\"end\":entity_end_char_time}\n",
        "\n",
        "              entity_start_char_time=entity_end_char_time+1\n",
        "        if not line: #this means if it reads an empty line\n",
        "          entity_start_char_time, entity_end_char_time = 0,0\n",
        "\n",
        "\n",
        "          entities= [(values[1]['start'],values[1]['end'],values[0]) for values in stored_entities.items()]\n",
        "\n",
        "          #we normalize the text before passing it\n",
        "          sentence = remove_special_characters(sentence)\n",
        "          TRAIN_DATA.append((sentence.strip(),{\"entities\":entities}))\n",
        "          sentence ,tag_idx=\"\", \"\"\n",
        "\n",
        "          stored_entities={}\n",
        "\n",
        "      return TRAIN_DATA\n",
        "\n",
        "    def preprocess_data(self, raw_data):\n",
        "        \"\"\"\n",
        "        Placeholder for any further filtering or augmentation.\n",
        "        For now, just returns it unchanged.\n",
        "        Can be extended to, e.g., filter out examples with no entities.\n",
        "        \"\"\"\n",
        "        # Example: Filter out sentences with no entities\n",
        "        processed_data = [(text, annots) for text, annots in raw_data if annots.get(\"entities\")]\n",
        "        return processed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pVLxPmruOZyf",
      "metadata": {
        "id": "pVLxPmruOZyf"
      },
      "source": [
        "Spacy trainer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "q_xMklUwM5oa",
      "metadata": {
        "id": "q_xMklUwM5oa"
      },
      "outputs": [],
      "source": [
        "class SpacyNerTrainer:\n",
        "    \"\"\"\n",
        "    Class to handle training a spaCy NER model.\n",
        "    \"\"\"\n",
        "    def __init__(self, blank_model_name=\"en\", n_iterations=20, dropout_rate=0.2, batch_size=128, output_dir=\"./custom_ner_model\"):\n",
        "        self.blank_model_name = blank_model_name\n",
        "        self.n_iterations = n_iterations\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.nlp = spacy.blank(self.blank_model_name)\n",
        "        self._setup_ner_pipe()\n",
        "\n",
        "    def _setup_ner_pipe(self):\n",
        "        \"\"\"Initializes or gets the NER pipe.\"\"\"\n",
        "        if \"ner\" not in self.nlp.pipe_names:\n",
        "            self.ner = self.nlp.add_pipe(\"ner\", last=True)\n",
        "        else:\n",
        "            self.ner = self.nlp.get_pipe(\"ner\")\n",
        "\n",
        "    def _add_labels_to_ner(self, train_data):\n",
        "        \"\"\"Adds all unique entity labels from training data to the NER component.\"\"\"\n",
        "        all_labels = set()\n",
        "        for _, annotations in train_data:\n",
        "            for ent_start, ent_end, label in annotations.get(\"entities\", []):\n",
        "                all_labels.add(label)\n",
        "        for label in all_labels:\n",
        "            self.ner.add_label(label)\n",
        "        print(f\"Added labels to NER: {sorted(list(all_labels))}\")\n",
        "\n",
        "    def train_model(self, train_data):\n",
        "        \"\"\"Trains the NER model.\"\"\"\n",
        "        self._add_labels_to_ner(train_data)\n",
        "\n",
        "        # Prepare example data for initialization (can use a subset)\n",
        "        # Ensure TRAIN_DATA is not empty\n",
        "        if not train_data:\n",
        "            print(\"Error: TRAIN_DATA is empty. Cannot initialize model.\")\n",
        "            return\n",
        "\n",
        "        example_data_for_init = [\n",
        "            Example.from_dict(self.nlp.make_doc(text), annots)\n",
        "            for text, annots in train_data[:min(100, len(train_data))] # Use a subset or all\n",
        "        ]\n",
        "        if not example_data_for_init:\n",
        "            print(\"Error: Could not create example data for initialization from TRAIN_DATA.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != \"ner\"]\n",
        "        with self.nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "            optimizer = self.nlp.initialize(get_examples=lambda: example_data_for_init)\n",
        "\n",
        "            for itn in range(self.n_iterations):\n",
        "                random.shuffle(train_data)\n",
        "                epoch_losses = {}\n",
        "                batches = minibatch(train_data, size=self.batch_size)\n",
        "\n",
        "                for batch in tqdm(batches, desc=f\"Epoch {itn + 1}/{self.n_iterations}\"):\n",
        "                    examples_for_update = []\n",
        "                    for text, annotations in batch:\n",
        "                        doc = self.nlp.make_doc(text)\n",
        "                        examples_for_update.append(Example.from_dict(doc, annotations))\n",
        "\n",
        "                    if examples_for_update:\n",
        "                        self.nlp.update(\n",
        "                            examples_for_update,\n",
        "                            drop=self.dropout_rate,\n",
        "                            sgd=optimizer,\n",
        "                            losses=epoch_losses\n",
        "                        )\n",
        "                if 'ner' in epoch_losses and examples_for_update: # Check if examples_for_update is not empty\n",
        "                    avg_loss = epoch_losses['ner'] / len(examples_for_update) # More accurate avg per batch step in last batch\n",
        "                    print(f\"Epoch {itn + 1} finished. Average NER Loss (last batch): {avg_loss:.4f} (Total Epoch: {epoch_losses['ner']:.2f})\")\n",
        "                else:\n",
        "                    print(f\"Epoch {itn + 1} finished. Losses: {epoch_losses}\")\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Saves the trained model to the output directory.\"\"\"\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.nlp.to_disk(self.output_dir)\n",
        "        print(f\"Saved model to {self.output_dir}\")\n",
        "\n",
        "    def test_model(self, test_text):\n",
        "        \"\"\"Loads the saved model and tests it on a sample text.\"\"\"\n",
        "        print(\"\\n--- Loading and Testing Model ---\")\n",
        "        try:\n",
        "            nlp_loaded = spacy.load(self.output_dir)\n",
        "            doc = nlp_loaded(test_text)\n",
        "            print(f\"Test text: '{test_text}'\")\n",
        "            print(\"Entities found:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or testing model: {e}\")\n",
        "    def make_dicts_from_offsets(self,text, spans):\n",
        "      \"\"\"\n",
        "      Convert a span list to the dictionaries nervaluate expects.\n",
        "      \"\"\"\n",
        "      out = []\n",
        "\n",
        "      for span in spans:\n",
        "\n",
        "          start, end, label = span\n",
        "          ent_text = text[start:end]\n",
        "          out.append({\n",
        "              \"text\": ent_text,\n",
        "              \"label\": label,\n",
        "              \"start\": start,\n",
        "              \"end\": end,\n",
        "          })\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZZ5480vuOcvp",
      "metadata": {
        "id": "ZZ5480vuOcvp"
      },
      "source": [
        "Main function class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3klnPJ0iMbrG",
      "metadata": {
        "id": "3klnPJ0iMbrG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing data...\n",
            "Loaded 7660 annotated sentences. ('2 start restaurants with inside dining', {'entities': [(0, 7, 'Rating'), (25, 38, 'Amenity')]})\n",
            "Total examples: 7573, Training examples: 6058, Dev examples: 1515\n",
            "\n",
            "Setting up and training NER model...\n",
            "Added labels to NER: ['Amenity', 'Cuisine', 'Dish', 'Hours', 'Location', 'Price', 'Rating', 'Restaurant_Name']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 190it [00:08, 21.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 finished. Average NER Loss (last batch): 1566.4475 (Total Epoch: 15664.47)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 190it [00:08, 21.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 finished. Average NER Loss (last batch): 971.8929 (Total Epoch: 9718.93)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 190it [00:13, 14.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 finished. Average NER Loss (last batch): 799.6556 (Total Epoch: 7996.56)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 190it [00:09, 20.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 finished. Average NER Loss (last batch): 730.0250 (Total Epoch: 7300.25)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 190it [00:09, 20.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 finished. Average NER Loss (last batch): 656.1926 (Total Epoch: 6561.93)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 190it [00:08, 21.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 finished. Average NER Loss (last batch): 628.2819 (Total Epoch: 6282.82)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 190it [00:08, 21.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 finished. Average NER Loss (last batch): 597.5023 (Total Epoch: 5975.02)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 190it [00:08, 21.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 finished. Average NER Loss (last batch): 557.8219 (Total Epoch: 5578.22)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 190it [00:08, 21.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 finished. Average NER Loss (last batch): 534.5763 (Total Epoch: 5345.76)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 190it [00:08, 21.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 finished. Average NER Loss (last batch): 516.7979 (Total Epoch: 5167.98)\n",
            "Saved model to restaurant_ner_model_v2\n",
            "\n",
            "--- Loading and Testing Model ---\n",
            "Test text: 'I want a cheap italian restaurant nearby for dinner.'\n",
            "Entities found: [('cheap', 'Price'), ('italian', 'Cuisine'), ('nearby', 'Location'), ('dinner.', 'Restaurant_Name')]\n",
            "\n",
            "--- Evaluating on Dev Data (micro-averaged) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_6277/100565971.py:85: DeprecationWarning: The current evaluation method is deprecated and it will the output change in the next release.The output will change to a dictionary with the following keys: overall, entities, entity_results, overall_indices, entity_indices.\n",
            "  overall, per_type, _, _ = evaluator.evaluate()   # nervaluate ≥0.2.x\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall: {'ent_type': {'correct': 2478, 'incorrect': 250, 'partial': 0, 'missed': 251, 'spurious': 193, 'possible': 2979, 'actual': 2921, 'precision': 0.8483396097226977, 'recall': 0.8318227593152064, 'f1': 0.84}, 'partial': {'correct': 2342, 'incorrect': 0, 'partial': 386, 'missed': 251, 'spurious': 193, 'possible': 2979, 'actual': 2921, 'precision': 0.8678534748373845, 'recall': 0.850956696878147, 'f1': 0.8593220338983051}, 'strict': {'correct': 2218, 'incorrect': 510, 'partial': 0, 'missed': 251, 'spurious': 193, 'possible': 2979, 'actual': 2921, 'precision': 0.7593289969188634, 'recall': 0.7445451493789862, 'f1': 0.7518644067796609}, 'exact': {'correct': 2342, 'incorrect': 386, 'partial': 0, 'missed': 251, 'spurious': 193, 'possible': 2979, 'actual': 2921, 'precision': 0.8017802122560767, 'recall': 0.7861698556562605, 'f1': 0.7938983050847458}}\n",
            "Per-type: {'Amenity': {'ent_type': {'correct': 347, 'incorrect': 46, 'partial': 0, 'missed': 81, 'spurious': 53, 'possible': 474, 'actual': 446, 'precision': 0.7780269058295964, 'recall': 0.7320675105485233, 'f1': 0.7543478260869567}, 'partial': {'correct': 293, 'incorrect': 0, 'partial': 100, 'missed': 81, 'spurious': 53, 'possible': 474, 'actual': 446, 'precision': 0.7690582959641256, 'recall': 0.7236286919831224, 'f1': 0.7456521739130435}, 'strict': {'correct': 273, 'incorrect': 120, 'partial': 0, 'missed': 81, 'spurious': 53, 'possible': 474, 'actual': 446, 'precision': 0.6121076233183856, 'recall': 0.5759493670886076, 'f1': 0.5934782608695651}, 'exact': {'correct': 293, 'incorrect': 100, 'partial': 0, 'missed': 81, 'spurious': 53, 'possible': 474, 'actual': 446, 'precision': 0.6569506726457399, 'recall': 0.6181434599156118, 'f1': 0.6369565217391304}}, 'Cuisine': {'ent_type': {'correct': 475, 'incorrect': 48, 'partial': 0, 'missed': 19, 'spurious': 15, 'possible': 542, 'actual': 538, 'precision': 0.8828996282527881, 'recall': 0.8763837638376384, 'f1': 0.8796296296296297}, 'partial': {'correct': 484, 'incorrect': 0, 'partial': 39, 'missed': 19, 'spurious': 15, 'possible': 542, 'actual': 538, 'precision': 0.9358736059479554, 'recall': 0.9289667896678967, 'f1': 0.9324074074074075}, 'strict': {'correct': 457, 'incorrect': 66, 'partial': 0, 'missed': 19, 'spurious': 15, 'possible': 542, 'actual': 538, 'precision': 0.8494423791821561, 'recall': 0.8431734317343174, 'f1': 0.8462962962962963}, 'exact': {'correct': 484, 'incorrect': 39, 'partial': 0, 'missed': 19, 'spurious': 15, 'possible': 542, 'actual': 538, 'precision': 0.8996282527881041, 'recall': 0.8929889298892989, 'f1': 0.8962962962962964}}, 'Dish': {'ent_type': {'correct': 219, 'incorrect': 66, 'partial': 0, 'missed': 35, 'spurious': 12, 'possible': 320, 'actual': 297, 'precision': 0.7373737373737373, 'recall': 0.684375, 'f1': 0.7098865478119936}, 'partial': {'correct': 252, 'incorrect': 0, 'partial': 33, 'missed': 35, 'spurious': 12, 'possible': 320, 'actual': 297, 'precision': 0.9040404040404041, 'recall': 0.8390625, 'f1': 0.8703403565640194}, 'strict': {'correct': 209, 'incorrect': 76, 'partial': 0, 'missed': 35, 'spurious': 12, 'possible': 320, 'actual': 297, 'precision': 0.7037037037037037, 'recall': 0.653125, 'f1': 0.6774716369529984}, 'exact': {'correct': 252, 'incorrect': 33, 'partial': 0, 'missed': 35, 'spurious': 12, 'possible': 320, 'actual': 297, 'precision': 0.8484848484848485, 'recall': 0.7875, 'f1': 0.8168557536466775}}, 'Hours': {'ent_type': {'correct': 151, 'incorrect': 20, 'partial': 0, 'missed': 25, 'spurious': 25, 'possible': 196, 'actual': 196, 'precision': 0.7704081632653061, 'recall': 0.7704081632653061, 'f1': 0.7704081632653061}, 'partial': {'correct': 138, 'incorrect': 0, 'partial': 33, 'missed': 25, 'spurious': 25, 'possible': 196, 'actual': 196, 'precision': 0.7882653061224489, 'recall': 0.7882653061224489, 'f1': 0.7882653061224489}, 'strict': {'correct': 132, 'incorrect': 39, 'partial': 0, 'missed': 25, 'spurious': 25, 'possible': 196, 'actual': 196, 'precision': 0.673469387755102, 'recall': 0.673469387755102, 'f1': 0.673469387755102}, 'exact': {'correct': 138, 'incorrect': 33, 'partial': 0, 'missed': 25, 'spurious': 25, 'possible': 196, 'actual': 196, 'precision': 0.7040816326530612, 'recall': 0.7040816326530612, 'f1': 0.7040816326530612}}, 'Location': {'ent_type': {'correct': 640, 'incorrect': 23, 'partial': 0, 'missed': 34, 'spurious': 57, 'possible': 697, 'actual': 720, 'precision': 0.8888888888888888, 'recall': 0.9182209469153515, 'f1': 0.9033168666196189}, 'partial': {'correct': 569, 'incorrect': 0, 'partial': 94, 'missed': 34, 'spurious': 57, 'possible': 697, 'actual': 720, 'precision': 0.8555555555555555, 'recall': 0.8837876614060258, 'f1': 0.8694424841213831}, 'strict': {'correct': 556, 'incorrect': 107, 'partial': 0, 'missed': 34, 'spurious': 57, 'possible': 697, 'actual': 720, 'precision': 0.7722222222222223, 'recall': 0.7977044476327116, 'f1': 0.7847565278757939}, 'exact': {'correct': 569, 'incorrect': 94, 'partial': 0, 'missed': 34, 'spurious': 57, 'possible': 697, 'actual': 720, 'precision': 0.7902777777777777, 'recall': 0.8163558106169297, 'f1': 0.8031051517290049}}, 'Price': {'ent_type': {'correct': 129, 'incorrect': 13, 'partial': 0, 'missed': 10, 'spurious': 2, 'possible': 152, 'actual': 144, 'precision': 0.8958333333333334, 'recall': 0.8486842105263158, 'f1': 0.8716216216216217}, 'partial': {'correct': 128, 'incorrect': 0, 'partial': 14, 'missed': 10, 'spurious': 2, 'possible': 152, 'actual': 144, 'precision': 0.9375, 'recall': 0.8881578947368421, 'f1': 0.9121621621621622}, 'strict': {'correct': 123, 'incorrect': 19, 'partial': 0, 'missed': 10, 'spurious': 2, 'possible': 152, 'actual': 144, 'precision': 0.8541666666666666, 'recall': 0.8092105263157895, 'f1': 0.831081081081081}, 'exact': {'correct': 128, 'incorrect': 14, 'partial': 0, 'missed': 10, 'spurious': 2, 'possible': 152, 'actual': 144, 'precision': 0.8888888888888888, 'recall': 0.8421052631578947, 'f1': 0.8648648648648649}}, 'Rating': {'ent_type': {'correct': 181, 'incorrect': 12, 'partial': 0, 'missed': 11, 'spurious': 18, 'possible': 204, 'actual': 211, 'precision': 0.8578199052132701, 'recall': 0.8872549019607843, 'f1': 0.872289156626506}, 'partial': {'correct': 166, 'incorrect': 0, 'partial': 27, 'missed': 11, 'spurious': 18, 'possible': 204, 'actual': 211, 'precision': 0.8507109004739336, 'recall': 0.8799019607843137, 'f1': 0.8650602409638554}, 'strict': {'correct': 162, 'incorrect': 31, 'partial': 0, 'missed': 11, 'spurious': 18, 'possible': 204, 'actual': 211, 'precision': 0.7677725118483413, 'recall': 0.7941176470588235, 'f1': 0.7807228915662651}, 'exact': {'correct': 166, 'incorrect': 27, 'partial': 0, 'missed': 11, 'spurious': 18, 'possible': 204, 'actual': 211, 'precision': 0.7867298578199052, 'recall': 0.8137254901960784, 'f1': 0.7999999999999999}}, 'Restaurant_Name': {'ent_type': {'correct': 336, 'incorrect': 22, 'partial': 0, 'missed': 36, 'spurious': 11, 'possible': 394, 'actual': 369, 'precision': 0.9105691056910569, 'recall': 0.8527918781725888, 'f1': 0.8807339449541284}, 'partial': {'correct': 312, 'incorrect': 0, 'partial': 46, 'missed': 36, 'spurious': 11, 'possible': 394, 'actual': 369, 'precision': 0.907859078590786, 'recall': 0.850253807106599, 'f1': 0.8781127129750984}, 'strict': {'correct': 306, 'incorrect': 52, 'partial': 0, 'missed': 36, 'spurious': 11, 'possible': 394, 'actual': 369, 'precision': 0.8292682926829268, 'recall': 0.7766497461928934, 'f1': 0.8020969855832241}, 'exact': {'correct': 312, 'incorrect': 46, 'partial': 0, 'missed': 36, 'spurious': 11, 'possible': 394, 'actual': 369, 'precision': 0.8455284552845529, 'recall': 0.7918781725888325, 'f1': 0.817824377457405}}}\n",
            "Micro Precision: 75.93%\n",
            "Micro Recall:    74.45%\n",
            "Micro F1:        75.19%\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the NER training pipeline.\"\"\"\n",
        "    # --- 1. CONFIGURABLE PARAMETERS ---\n",
        "    n_iterations = 10\n",
        "    blank_model_name = \"en\"\n",
        "    bio_file_path = \"restauranttrain.bio\" # Make sure this file exists or change path\n",
        "    output_model_dir = \"./restaurant_ner_model_v2\"\n",
        "    dropout = 0.2\n",
        "    batch_size_train = 32 # Adjusted batch size\n",
        "\n",
        "\n",
        "    # --- 2. LOAD & PREPROCESS DATA ---\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data_processor = NerDataProcessor(bio_file_path=bio_file_path)\n",
        "    raw_train_data = data_processor.read_bio_file()\n",
        "    print(f\"Loaded {len(raw_train_data)} annotated sentences. {raw_train_data[0]}\")\n",
        "    if not raw_train_data:\n",
        "        print(\"No data loaded from BIO file. Exiting.\")\n",
        "        return\n",
        "\n",
        "    train_data_full = data_processor.preprocess_data(raw_train_data)\n",
        "    random.shuffle(train_data_full)\n",
        "\n",
        "    # Split data (e.g., 80% train, 20% dev/test)\n",
        "    split_ratio = 0.80\n",
        "    split_index = int(len(train_data_full) * split_ratio)\n",
        "\n",
        "    TRAIN_DATA = train_data_full[:split_index]\n",
        "    DEV_DATA = train_data_full[split_index:] # Using DEV_DATA for evaluation later (optional)\n",
        "\n",
        "    if not TRAIN_DATA:\n",
        "        print(\"Not enough data for training after split. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total examples: {len(train_data_full)}, Training examples: {len(TRAIN_DATA)}, Dev examples: {len(DEV_DATA)}\")\n",
        "\n",
        "    # --- 3. TRAIN MODEL ---\n",
        "    print(\"\\nSetting up and training NER model...\")\n",
        "    trainer = SpacyNerTrainer(\n",
        "        blank_model_name=blank_model_name,\n",
        "        n_iterations=n_iterations,\n",
        "        dropout_rate=dropout,\n",
        "        batch_size=batch_size_train,\n",
        "        output_dir=output_model_dir\n",
        "    )\n",
        "    trainer.train_model(TRAIN_DATA)\n",
        "\n",
        "    # --- 4. SAVE MODEL ---\n",
        "    trainer.save_model()\n",
        "\n",
        "    # --- 5. TEST MODEL ---\n",
        "    test_text_example = \"I want a cheap italian restaurant nearby for dinner.\"\n",
        "    trainer.test_model(test_text_example)\n",
        "\n",
        "    if DEV_DATA:\n",
        "        print(\"\\n--- Evaluating on Dev Data (micro-averaged) ---\")\n",
        "\n",
        "        # 1) load model and get label set\n",
        "        nlp_loaded = spacy.load(output_model_dir)\n",
        "        tags = sorted(nlp_loaded.pipe_labels[\"ner\"])\n",
        "\n",
        "\n",
        "        # 2) accumulate references / predictions sentence-by-sentence\n",
        "        all_refs, all_preds = [], []\n",
        "        with open (\"dev_outputs.txt\",'w') as f:\n",
        "\n",
        "          for idx, sample in enumerate(DEV_DATA):\n",
        "              text,gold_ann= sample\n",
        "\n",
        "              doc = nlp_loaded(text)\n",
        "\n",
        "              gold_spans = gold_ann[\"entities\"]                         # (start,end,label)\n",
        "              pred_spans = [(ent.start_char, ent.end_char, ent.label_)  # same shape\n",
        "                            for ent in doc.ents]\n",
        "              f.write(f\"{idx+1}. Text: {text}\\n\")\n",
        "              f.write(f\"Gold Entities: {gold_spans}\\n\")\n",
        "              f.write(f\"Pred Entities: {pred_spans}\\n\")\n",
        "              f.write('\\n')\n",
        "              # trainer.make_dicts_from_offsets → ♥ keeps your helper\n",
        "              all_refs.append(trainer.make_dicts_from_offsets(text, gold_spans))\n",
        "              all_preds.append(trainer.make_dicts_from_offsets(text, pred_spans))\n",
        "\n",
        "          # 4) one evaluation pass, micro-averaged over all sentences\n",
        "          evaluator = Evaluator(all_refs, all_preds, tags=tags, loader=\"default\")\n",
        "          overall, per_type, _, _ = evaluator.evaluate()   # nervaluate ≥0.2.x\n",
        "          print(\"Overall:\", overall)\n",
        "          print(\"Per-type:\", per_type)\n",
        "\n",
        "          prec = overall[\"strict\"][\"precision\"]\n",
        "          rec  = overall[\"strict\"][\"recall\"]\n",
        "          f1   = overall[\"strict\"][\"f1\"]\n",
        "\n",
        "          print(f\"Micro Precision: {prec:.2%}\")\n",
        "          print(f\"Micro Recall:    {rec:.2%}\")\n",
        "          print(f\"Micro F1:        {f1:.2%}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9704f8-cc01-4159-841f-de6d7973e92b",
      "metadata": {
        "id": "4b9704f8-cc01-4159-841f-de6d7973e92b"
      },
      "source": [
        "## **References**\n",
        "\n",
        "\n",
        "1. https://www.kaggle.com/code/remakia/introduction-to-ner-part-i-rule-based\n",
        "2. https://spacy.io/\n",
        "3. https://www.kaggle.com/code/remakia/introduction-to-ner-part-i-rule-based\n",
        "4. https://www.nltk.org/\n",
        "5. https://medium.com/data-science/conditional-random-fields-explained-e5b8256da776\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7c0266-7f5b-4247-ac36-c6b44f90bf03",
      "metadata": {
        "id": "8b7c0266-7f5b-4247-ac36-c6b44f90bf03"
      },
      "source": [
        "# **Facilitator(s) Details**\n",
        "\n",
        "**Facilitator(s):**\n",
        "\n",
        "*   Name: [FELIX TETTEH AKWERH,Adwoa Asantewaa Bremang]\n",
        "*   Email: [felix.akwerh@knust.edu.gh,adwoabremang@gmail.com]\n",
        "*   LinkedIn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7257b3a4-6572-417d-8b22-b53efb4bd6b2",
      "metadata": {
        "id": "7257b3a4-6572-417d-8b22-b53efb4bd6b2"
      },
      "source": [
        "# **Reviewer’s Name**\n",
        "\n",
        "*   Name: [Reviewer’s Name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22f7673-72d0-4bf2-bc87-f9c7c2c4b32c",
      "metadata": {
        "id": "c22f7673-72d0-4bf2-bc87-f9c7c2c4b32c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
