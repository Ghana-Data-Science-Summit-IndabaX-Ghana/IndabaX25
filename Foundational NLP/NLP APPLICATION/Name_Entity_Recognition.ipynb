{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8bd3a486-36c6-4c20-94d7-a30271ad011d",
      "metadata": {
        "id": "8bd3a486-36c6-4c20-94d7-a30271ad011d"
      },
      "source": [
        "# **NAME ENTITY RECOGNITION**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4348ff2-2146-4305-8999-191a8e9c4cc7",
      "metadata": {
        "id": "a4348ff2-2146-4305-8999-191a8e9c4cc7"
      },
      "source": [
        "# **Table of Contents**\n",
        "\n",
        "1.   [Introduction](#Introduction)\n",
        "2.   [Prerequisites](#Prerequisites)\n",
        "3.   [Applications](#Applications)\n",
        "4.   [Rule-based-model](#Rule-based-models)\n",
        "5.   [Machinelearning-model](#Machinelearning-model)\n",
        "6.   [References](#References)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b8a541-a2e4-4886-b42e-4126d9fb9d7a",
      "metadata": {
        "id": "a1b8a541-a2e4-4886-b42e-4126d9fb9d7a"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "Name Entity recognition(NER) is a subtask of Natural language process(NLP) which focuses on identifying and grouping entities within a text or document.\n",
        "Entities present specific objects or names such as Persons, organizations, dates and times, countries, drugs, and various unique information within a document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b84706-8034-464e-aeb9-3a40249370df",
      "metadata": {
        "id": "70b84706-8034-464e-aeb9-3a40249370df"
      },
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "Before diving into coding exercises and examples, one should have basic knowledge in the following:\n",
        "1. python programming\n",
        "2. writing and creating algorithms\n",
        "3. problem solving, critical thinking and creative skills"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a37af23-9129-4e73-88a0-b8714f639416",
      "metadata": {
        "id": "3a37af23-9129-4e73-88a0-b8714f639416"
      },
      "source": [
        "<a id='guide'></a>\n",
        "## **Applications**\n",
        "\n",
        "NER is applied in various sectors of our daily lives; so of these applied areas are:\n",
        "1. Spam detection in emails\n",
        "2. Use for search engines\n",
        "3. Finance and Banking\n",
        "4. Chatbots generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ceb56a",
      "metadata": {
        "id": "26ceb56a"
      },
      "source": [
        "<a id='guide'></a>\n",
        "## **Types-of-NER-models**\n",
        "\n",
        "NER is applied in various sectors of our daily lives; so of these applied areas are:\n",
        "1. Rule based NER models\n",
        "2. Machine learning (ML) models\n",
        "3. Deep learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b82436ec-e867-4841-966b-9c3fd6280850",
      "metadata": {
        "id": "b82436ec-e867-4841-966b-9c3fd6280850"
      },
      "source": [
        "## **Rule-based-model**\n",
        "A rule-based NER model is a system that relies on manually crafted linguistic rules, such as regular expressions, token patterns, and lexicons, to identify and classify named entities in text without the need for training data.\n",
        "\n",
        "Examples of rule based approaches\n",
        "\n",
        "a. Spacy Entity\n",
        "\n",
        "b. NLTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **code-examples-for-NLTK**"
      ],
      "metadata": {
        "id": "mohSp-SK2kVw"
      },
      "id": "mohSp-SK2kVw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V5AdgzpmCEjM",
      "metadata": {
        "id": "V5AdgzpmCEjM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install svgling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e1a00c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "52e1a00c",
        "outputId": "52c8ad42-8b4a-4149-c106-4ffc7297580f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.'), ('can', 'MD'), ('arthur', 'VB'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), Tree('ORGANIZATION', [('Ghana', 'NNP')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,976.0,168.0\" width=\"976px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"3.27869%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">At</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.63934%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.7377%\" x=\"3.27869%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">eight</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.14754%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.37705%\" x=\"9.01639%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">o'clock</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.7049%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"16.3934%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.0328%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.19672%\" x=\"19.6721%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Thursday</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.7705%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.37705%\" x=\"27.8689%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">morning</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.5574%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.55738%\" x=\"35.2459%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Arthur</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.5246%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"41.8033%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">did</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.8525%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"45.9016%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">n't</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.9508%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">feel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.459%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"54.918%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">very</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.377%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"59.8361%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">good</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.2951%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.45902%\" x=\"64.7541%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.9836%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"67.2131%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">can</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.2623%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.55738%\" x=\"71.3115%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">arthur</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.5902%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"77.8689%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">go</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5082%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"81.1475%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.7869%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"84.4262%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.4754%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.4754%\" x=\"88.5246%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Ghana</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.2623%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "import svgling\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good. can arthur go to the Ghana\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "\n",
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ef8ce6",
      "metadata": {
        "id": "f3ef8ce6"
      },
      "source": [
        "### **code-examples-for-Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90da5ca",
      "metadata": {
        "id": "c90da5ca"
      },
      "outputs": [],
      "source": [
        "#installations\n",
        "%%capture\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b16d546",
      "metadata": {
        "id": "8b16d546"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "# import EntityRuler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53eed00",
      "metadata": {
        "id": "d53eed00"
      },
      "outputs": [],
      "source": [
        "\n",
        "def spacy_rb_ner(patterns,text,model_name='en'):\n",
        "\n",
        "  #create a blank model\n",
        "  nlp = spacy.blank(model_name)\n",
        "\n",
        "  #create an new entity to for NER\n",
        "  ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "  ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "  #extract components from the text\n",
        "  doc = nlp(text)\n",
        "  print(doc)\n",
        "  for ent in doc.ents:\n",
        "      print(ent.text, ent.label_)\n",
        "\n",
        "  displacy.render(doc, style=\"ent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a1b4a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "f8a1b4a7",
        "outputId": "8ce70886-b801-4fa8-8143-bd9ea1b633c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John is 25 years old\n",
            "25 years old AGE\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">John is \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    25 years old\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">AGE</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "patterns = [{\"label\": \"AGE\", \"pattern\": [{\"like_num\": True}, {\"lower\": \"years\"}, {\"lower\": \"old\"}]}]\n",
        "text = \"John is 25 years old\"\n",
        "spacy_rb_ner(patterns,text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80aedc0f",
      "metadata": {
        "id": "80aedc0f"
      },
      "source": [
        "### **Exercises 1**\n",
        "\n",
        "Please list some advantages and disadvantages as you try out these rule based name entity recognition models.\n",
        "\n",
        "Advantages\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "1.\n",
        "\n",
        "2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9cd2de",
      "metadata": {
        "id": "1d9cd2de"
      },
      "source": [
        "### **Exercises 2**\n",
        "\n",
        "Let's try our hands on sample exercises for better understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388b0feb",
      "metadata": {
        "id": "388b0feb"
      },
      "outputs": [],
      "source": [
        "# Easy\n",
        "#customize your own pattern and provide your text for testing\n",
        "pattern =[]\n",
        "text = \"\"\n",
        "spacy_rb_ner(pattern,text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1cfecc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "9d1cfecc",
        "outputId": "30daa6ac-3d62-4a06-aaef-bd3ccad6bfff",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SpacyNEREntity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f5ff27e80385>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-f5ff27e80385>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/entities_dataset.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mspacy_enr_entity\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mSpacyNEREntity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mpattern_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_enr_entity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SpacyNEREntity' is not defined"
          ]
        }
      ],
      "source": [
        "#Hard\n",
        "\n",
        "#1. load the json dictionary\n",
        "import json\n",
        "def read_json(json_file):\n",
        "\n",
        "  return 0\n",
        "\n",
        "#2. convert the pattern dictionary into patterns\n",
        "def generate_pattern(collection):\n",
        "\n",
        "  return 0\n",
        "\n",
        "#3. test out generated pattern in main\n",
        "def main():\n",
        "  json_file = \"/content/entities_dataset.json\"\n",
        "\n",
        "\n",
        "  pattern_dict = read_json(json_file)\n",
        "  pattern =generate_pattern(pattern_dict)\n",
        "  text = \"Asantewaa went to kotoka international airport\"\n",
        "  text = text.lower()\n",
        "  spacy_rb_ner(pattern,text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "solution"
      ],
      "metadata": {
        "id": "Rk9kas6ztSAV"
      },
      "id": "Rk9kas6ztSAV"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class SpacyNEREntity:\n",
        "  def __init__(self,json_file):\n",
        "    self.json_file = json_file\n",
        "\n",
        "  def read_json(self):\n",
        "\n",
        "    try:\n",
        "        with open(self.json_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            # print(data)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'file.json' not found. Make sure the file exists in the correct path.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error: Could not decode JSON from the file. Make sure the file is a valid JSON.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "  def generate_pattern(self,collections):\n",
        "    pattern=[]\n",
        "    patterns=[]\n",
        "    for collection in collections.items():\n",
        "      for i in collection[1]:\n",
        "        components = i.split()\n",
        "        if len(components)>1:\n",
        "          pattern=[{\"lower\":items.lower()} for items in components]\n",
        "        elif len(components)==1:\n",
        "          pattern=[{\"lower\":components[0].lower()}]\n",
        "        patterns.append({\"label\":collection[0],\"pattern\":pattern})\n",
        "    print(patterns)\n",
        "    return patterns\n",
        "\n",
        "def main():\n",
        "    json_file = \"/content/entities_dataset.json\"\n",
        "    spacy_enr_entity =SpacyNEREntity(json_file)\n",
        "\n",
        "    pattern_dict = spacy_enr_entity.read_json()\n",
        "    pattern =spacy_enr_entity.generate_pattern(pattern_dict)\n",
        "    text = \"Asantewaa went to kotoka international airport\"\n",
        "    text = text.lower()\n",
        "    spacy_rb_ner(pattern,text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "XRBcZT4-tT3C",
        "outputId": "4980f786-3f2a-455d-d7f4-b763ee2d8f36"
      },
      "id": "XRBcZT4-tT3C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'GPE01', 'pattern': [{'lower': 'ghana'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'nigeria'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'kenya'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'france'}]}, {'label': 'GPE01', 'pattern': [{'lower': 'united'}, {'lower': 'states'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'sahara'}, {'lower': 'desert'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'mount'}, {'lower': 'kilimanjaro'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'lake'}, {'lower': 'victoria'}]}, {'label': 'LOC01', 'pattern': [{'lower': 'atlantic'}, {'lower': 'ocean'}]}, {'label': 'PER01', 'pattern': [{'lower': 'kwame'}, {'lower': 'nkrumah'}]}, {'label': 'PER01', 'pattern': [{'lower': 'ngũgĩ'}, {'lower': 'wa'}, {'lower': \"thiong'o\"}]}, {'label': 'PER01', 'pattern': [{'lower': 'chinua'}, {'lower': 'achebe'}]}, {'label': 'PER01', 'pattern': [{'lower': 'yaa'}, {'lower': 'asantewaa'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'united'}, {'lower': 'nations'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'world'}, {'lower': 'health'}, {'lower': 'organization'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'google'}]}, {'label': 'ORG01', 'pattern': [{'lower': 'ashesi'}, {'lower': 'university'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'world'}, {'lower': 'war'}, {'lower': 'ii'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'african'}, {'lower': 'union'}, {'lower': 'summit'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'cop27'}]}, {'label': 'EVENT01', 'pattern': [{'lower': 'olympic'}, {'lower': 'games'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'swahili'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'zulu'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'yoruba'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'igbo'}]}, {'label': 'LANG01', 'pattern': [{'lower': 'twi'}]}, {'label': 'WOA01', 'pattern': [{'lower': 'things'}, {'lower': 'fall'}, {'lower': 'apart'}]}, {'label': 'WOA01', 'pattern': [{'lower': 'petals'}, {'lower': 'of'}, {'lower': 'blood'}]}, {'label': 'WOA01', 'pattern': [{'lower': 'the'}, {'lower': 'beautiful'}, {'lower': 'ones'}, {'lower': 'are'}, {'lower': 'not'}, {'lower': 'yet'}, {'lower': 'born'}]}, {'label': 'LAW01', 'pattern': [{'lower': 'data'}, {'lower': 'protection'}, {'lower': 'act'}]}, {'label': 'LAW01', 'pattern': [{'lower': 'patriot'}, {'lower': 'act'}]}, {'label': 'LAW01', 'pattern': [{'lower': 'right'}, {'lower': 'to'}, {'lower': 'information'}, {'lower': 'bill'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'iphone'}, {'lower': '14'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'samsung'}, {'lower': 'galaxy'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'tesla'}, {'lower': 'model'}, {'lower': 's'}]}, {'label': 'PROD01', 'pattern': [{'lower': 'playstation'}, {'lower': '5'}]}, {'label': 'FAC01', 'pattern': [{'lower': 'kotoka'}, {'lower': 'international'}, {'lower': 'airport'}]}, {'label': 'FAC01', 'pattern': [{'lower': 'accra'}, {'lower': 'sports'}, {'lower': 'stadium'}]}, {'label': 'FAC01', 'pattern': [{'lower': 'nairobi'}, {'lower': 'terminal'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'independence'}, {'lower': 'day'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'christmas'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'africa'}, {'lower': 'day'}]}, {'label': 'DATE01', 'pattern': [{'lower': 'new'}, {'lower': \"year's\"}, {'lower': 'day'}]}]\n",
            "asantewaa went to kotoka international airport\n",
            "kotoka international airport FAC01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">asantewaa went to \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    kotoka international airport\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC01</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18658ab7",
      "metadata": {
        "id": "18658ab7"
      },
      "source": [
        "## **Machinelearning-model**\n",
        "\n",
        "1. Conditional Random Fields - Conditional Random Fields is a probalistic model that determines or predicts labels based on a given sequence/Input.\n",
        "\n",
        "It models the conditional probability of a label sequence Y, given an input sequence X:\n",
        "\n",
        "P(Y | X)\n",
        "\n",
        "\n",
        "2. A **Support Vector Machine (SVM)** is a supervised machine learning algorithm that finds the best hyperplane to separate data points into different classes.\n",
        "\n",
        "In **Named Entity Recognition (NER)**, SVM classifies each token (word) independently, based on its features.\n",
        "Unlike **Conditional Random Fields (CRFs)**, SVMs do **not** consider the previous or future tags when making predictions.\n",
        "This can limit their ability to model the context and dependencies between labels in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce78b08",
      "metadata": {
        "id": "0ce78b08"
      },
      "source": [
        "### **Example-code**\n",
        "\n",
        "Below is an example code of Spacy, a machine learning NER model train with the theory of conditional random fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab200a63",
      "metadata": {
        "id": "ab200a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12ff88b-fb1a-464f-f333-53a4c68233a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27-years-old 9 21 DATE\n",
            "Kwame 144 149 PERSON\n",
            "Ghana 216 221 GPE\n",
            "Kigali 223 229 GPE\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import requests\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "content =\"Esi is a 27-years-old individual who came back home from school. she is meant to go back to school soon to see her friends. Have you heard from Kwame because the last time i spoke to him, he said he was going to the Ghana, Kigali\"\n",
        "\n",
        "doc = nlp(content)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3da591e",
      "metadata": {
        "id": "b3da591e"
      },
      "source": [
        "### **Exercise 1**\n",
        "\n",
        "We will train our own Spacy model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installations & downloads"
      ],
      "metadata": {
        "id": "NuCd8_f8OKtX"
      },
      "id": "NuCd8_f8OKtX"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nervaluate\n",
        "!wget https://sls.csail.mit.edu/downloads/restaurant/restauranttest.bio\n",
        "!wget https://sls.csail.mit.edu/downloads/restaurant/restauranttrain.bio\n"
      ],
      "metadata": {
        "id": "LEw2MBgJLhwj"
      },
      "id": "LEw2MBgJLhwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importations"
      ],
      "metadata": {
        "id": "g6xMlsKJONeS"
      },
      "id": "g6xMlsKJONeS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ezl1qmGSui3G",
      "metadata": {
        "id": "Ezl1qmGSui3G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import random\n",
        "from pathlib import Path\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training.example import Example # Correct import for Example\n",
        "from spacy.util import minibatch\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from nervaluate import Evaluator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper class"
      ],
      "metadata": {
        "id": "RB73DWNtOSb5"
      },
      "id": "RB73DWNtOSb5"
    },
    {
      "cell_type": "code",
      "source": [
        "# anything that is **not** a letter or digit → replace\n",
        "chars_to_ignore_regex = r\"[^A-Za-z0-9]+\"\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"\n",
        "    Replace non-alphanumeric runs with a single space and lowercase the result.\n",
        "    \"\"\"\n",
        "    cleaned = re.sub(chars_to_ignore_regex, \" \", text)   # keep letters & digits\n",
        "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)               # squeeze repeated spaces\n",
        "    return cleaned.lower().strip()"
      ],
      "metadata": {
        "id": "wfcW4xhaOSF0"
      },
      "id": "wfcW4xhaOSF0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing class"
      ],
      "metadata": {
        "id": "V-4EPi8BOWIZ"
      },
      "id": "V-4EPi8BOWIZ"
    },
    {
      "cell_type": "markdown",
      "id": "nKr39DsLSFaO",
      "metadata": {
        "id": "nKr39DsLSFaO"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Sample format for a spacy NER dataset\n",
        "TRAIN_DATA = [\n",
        "    (\n",
        "        \"a four star restaurant with a bar\",\n",
        "        {\n",
        "            \"entities\": [\n",
        "                (2, 11, \"Rating\"),   # \"four star\"\n",
        "                (23, 29, \"Location\"), # \"with a\"\n",
        "                (30, 33, \"Amenity\")   # \"bar\"\n",
        "            ]\n",
        "        }\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "{'O': {'start': 12, 'end': 23},\n",
        "'Rating': {'start': 2, 'end': 12, 'text': 'a four star '},\n",
        "'Location': {'start': 23, 'end': 30},\n",
        " 'Amenity': {'start': 30, 'end': 34}}\n",
        "---------------------------------------------------------------------------\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset preprocessing"
      ],
      "metadata": {
        "id": "UQc-BVD61THh"
      },
      "id": "UQc-BVD61THh"
    },
    {
      "cell_type": "code",
      "source": [
        "class NerDataProcessor:\n",
        "    \"\"\"\n",
        "    Class to handle reading and processing of .bio (CoNLL-style) NER data.\n",
        "    \"\"\"\n",
        "    def __init__(self, bio_file_path):\n",
        "        self.bio_file_path = Path(bio_file_path)\n",
        "        if not self.bio_file_path.is_file():\n",
        "            raise FileNotFoundError(f\"BIO file not found at: {self.bio_file_path}\")\n",
        "\n",
        "\n",
        "    def read_bio_file(self):\n",
        "      \"\"\"\n",
        "      Reads a CoNLL-style .bio file and returns a list of\n",
        "      (text, {\"entities\": [(start, end, label), …]}) tuples.\n",
        "      \"\"\"\n",
        "      TRAIN_DATA , entities =[], []\n",
        "      entity_start_char_time, entity_end_char_time = 0,0\n",
        "      sentence ,tag_idx=\"\", \"\"\n",
        "      stored_entities={}\n",
        "\n",
        "      with open(self.bio_file_path, mode='r',encoding=\"utf8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "\n",
        "      for num,line in enumerate(lines):\n",
        "        line= line.strip()\n",
        "\n",
        "        part =line.split()\n",
        "        if len(part)==2:\n",
        "\n",
        "          token, tag = line.split()\n",
        "          sentence = sentence + tag + \" \"\n",
        "\n",
        "          if tag:\n",
        "            entity_end_char_time= entity_start_char_time + len(tag.strip())\n",
        "            if token.startswith(\"O\"):\n",
        "              entity_start_char_time=entity_end_char_time+1\n",
        "              continue\n",
        "            elif token.split(\"-\"):\n",
        "              idx= token.split(\"-\")[0]\n",
        "\n",
        "              if token.split(\"-\")[1] in stored_entities:\n",
        "                stored_entities[token.split(\"-\")[1]][\"end\"]=entity_end_char_time\n",
        "                if idx==\"B\": #new changes\n",
        "                    stored_entities[token.split(\"-\")[1]][\"start\"]=entity_start_char_time\n",
        "\n",
        "              else:\n",
        "                  stored_entities[token.split(\"-\")[1]]={\"start\":entity_start_char_time,\"end\":entity_end_char_time}\n",
        "\n",
        "              entity_start_char_time=entity_end_char_time+1\n",
        "        if not line: #this means if it reads an empty line\n",
        "          entity_start_char_time, entity_end_char_time = 0,0\n",
        "\n",
        "\n",
        "          entities= [(values[1]['start'],values[1]['end'],values[0]) for values in stored_entities.items()]\n",
        "\n",
        "          #we normalize the text before passing it\n",
        "          sentence = remove_special_characters(sentence)\n",
        "          TRAIN_DATA.append((sentence.strip(),{\"entities\":entities}))\n",
        "          sentence ,tag_idx=\"\", \"\"\n",
        "\n",
        "          stored_entities={}\n",
        "\n",
        "      return TRAIN_DATA\n",
        "\n",
        "    def preprocess_data(self, raw_data):\n",
        "        \"\"\"\n",
        "        Placeholder for any further filtering or augmentation.\n",
        "        For now, just returns it unchanged.\n",
        "        Can be extended to, e.g., filter out examples with no entities.\n",
        "        \"\"\"\n",
        "        # Example: Filter out sentences with no entities\n",
        "        processed_data = [(text, annots) for text, annots in raw_data if annots.get(\"entities\")]\n",
        "        return processed_data\n",
        "\n"
      ],
      "metadata": {
        "id": "JIJkc78cOD4p"
      },
      "id": "JIJkc78cOD4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy trainer class"
      ],
      "metadata": {
        "id": "pVLxPmruOZyf"
      },
      "id": "pVLxPmruOZyf"
    },
    {
      "cell_type": "code",
      "source": [
        "class SpacyNerTrainer:\n",
        "    \"\"\"\n",
        "    Class to handle training a spaCy NER model.\n",
        "    \"\"\"\n",
        "    def __init__(self, blank_model_name=\"en\", n_iterations=20, dropout_rate=0.2, batch_size=128, output_dir=\"./custom_ner_model\"):\n",
        "        self.blank_model_name = blank_model_name\n",
        "        self.n_iterations = n_iterations\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.nlp = spacy.blank(self.blank_model_name)\n",
        "        self._setup_ner_pipe()\n",
        "\n",
        "    def _setup_ner_pipe(self):\n",
        "        \"\"\"Initializes or gets the NER pipe.\"\"\"\n",
        "        if \"ner\" not in self.nlp.pipe_names:\n",
        "            self.ner = self.nlp.add_pipe(\"ner\", last=True)\n",
        "        else:\n",
        "            self.ner = self.nlp.get_pipe(\"ner\")\n",
        "\n",
        "    def _add_labels_to_ner(self, train_data):\n",
        "        \"\"\"Adds all unique entity labels from training data to the NER component.\"\"\"\n",
        "        all_labels = set()\n",
        "        for _, annotations in train_data:\n",
        "            for ent_start, ent_end, label in annotations.get(\"entities\", []):\n",
        "                all_labels.add(label)\n",
        "        for label in all_labels:\n",
        "            self.ner.add_label(label)\n",
        "        print(f\"Added labels to NER: {sorted(list(all_labels))}\")\n",
        "\n",
        "    def train_model(self, train_data):\n",
        "        \"\"\"Trains the NER model.\"\"\"\n",
        "        self._add_labels_to_ner(train_data)\n",
        "\n",
        "        # Prepare example data for initialization (can use a subset)\n",
        "        # Ensure TRAIN_DATA is not empty\n",
        "        if not train_data:\n",
        "            print(\"Error: TRAIN_DATA is empty. Cannot initialize model.\")\n",
        "            return\n",
        "\n",
        "        example_data_for_init = [\n",
        "            Example.from_dict(self.nlp.make_doc(text), annots)\n",
        "            for text, annots in train_data[:min(100, len(train_data))] # Use a subset or all\n",
        "        ]\n",
        "        if not example_data_for_init:\n",
        "            print(\"Error: Could not create example data for initialization from TRAIN_DATA.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != \"ner\"]\n",
        "        with self.nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "            optimizer = self.nlp.initialize(get_examples=lambda: example_data_for_init)\n",
        "\n",
        "            for itn in range(self.n_iterations):\n",
        "                random.shuffle(train_data)\n",
        "                epoch_losses = {}\n",
        "                batches = minibatch(train_data, size=self.batch_size)\n",
        "\n",
        "                for batch in tqdm(batches, desc=f\"Epoch {itn + 1}/{self.n_iterations}\"):\n",
        "                    examples_for_update = []\n",
        "                    for text, annotations in batch:\n",
        "                        doc = self.nlp.make_doc(text)\n",
        "                        examples_for_update.append(Example.from_dict(doc, annotations))\n",
        "\n",
        "                    if examples_for_update:\n",
        "                        self.nlp.update(\n",
        "                            examples_for_update,\n",
        "                            drop=self.dropout_rate,\n",
        "                            sgd=optimizer,\n",
        "                            losses=epoch_losses\n",
        "                        )\n",
        "                if 'ner' in epoch_losses and examples_for_update: # Check if examples_for_update is not empty\n",
        "                    avg_loss = epoch_losses['ner'] / len(examples_for_update) # More accurate avg per batch step in last batch\n",
        "                    print(f\"Epoch {itn + 1} finished. Average NER Loss (last batch): {avg_loss:.4f} (Total Epoch: {epoch_losses['ner']:.2f})\")\n",
        "                else:\n",
        "                    print(f\"Epoch {itn + 1} finished. Losses: {epoch_losses}\")\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Saves the trained model to the output directory.\"\"\"\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.nlp.to_disk(self.output_dir)\n",
        "        print(f\"Saved model to {self.output_dir}\")\n",
        "\n",
        "    def test_model(self, test_text):\n",
        "        \"\"\"Loads the saved model and tests it on a sample text.\"\"\"\n",
        "        print(\"\\n--- Loading and Testing Model ---\")\n",
        "        try:\n",
        "            nlp_loaded = spacy.load(self.output_dir)\n",
        "            doc = nlp_loaded(test_text)\n",
        "            print(f\"Test text: '{test_text}'\")\n",
        "            print(\"Entities found:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or testing model: {e}\")\n",
        "    def make_dicts_from_offsets(self,text, spans):\n",
        "      \"\"\"\n",
        "      Convert a span list to the dictionaries nervaluate expects.\n",
        "      \"\"\"\n",
        "      out = []\n",
        "\n",
        "      for span in spans:\n",
        "\n",
        "          start, end, label = span\n",
        "          ent_text = text[start:end]\n",
        "          out.append({\n",
        "              \"text\": ent_text,\n",
        "              \"label\": label,\n",
        "              \"start\": start,\n",
        "              \"end\": end,\n",
        "          })\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "q_xMklUwM5oa"
      },
      "id": "q_xMklUwM5oa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function class"
      ],
      "metadata": {
        "id": "ZZ5480vuOcvp"
      },
      "id": "ZZ5480vuOcvp"
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the NER training pipeline.\"\"\"\n",
        "    # --- 1. CONFIGURABLE PARAMETERS ---\n",
        "    n_iterations = 10\n",
        "    blank_model_name = \"en\"\n",
        "    bio_file_path = \"restauranttrain.bio\" # Make sure this file exists or change path\n",
        "    output_model_dir = \"./restaurant_ner_model_v2\"\n",
        "    dropout = 0.2\n",
        "    batch_size_train = 32 # Adjusted batch size\n",
        "\n",
        "\n",
        "    # --- 2. LOAD & PREPROCESS DATA ---\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    data_processor = NerDataProcessor(bio_file_path=bio_file_path)\n",
        "    raw_train_data = data_processor.read_bio_file()\n",
        "    print(f\"Loaded {len(raw_train_data)} annotated sentences. {raw_train_data[0]}\")\n",
        "    if not raw_train_data:\n",
        "        print(\"No data loaded from BIO file. Exiting.\")\n",
        "        return\n",
        "\n",
        "    train_data_full = data_processor.preprocess_data(raw_train_data)\n",
        "    random.shuffle(train_data_full)\n",
        "\n",
        "    # Split data (e.g., 80% train, 20% dev/test)\n",
        "    split_ratio = 0.80\n",
        "    split_index = int(len(train_data_full) * split_ratio)\n",
        "\n",
        "    TRAIN_DATA = train_data_full[:split_index]\n",
        "    DEV_DATA = train_data_full[split_index:] # Using DEV_DATA for evaluation later (optional)\n",
        "\n",
        "    if not TRAIN_DATA:\n",
        "        print(\"Not enough data for training after split. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total examples: {len(train_data_full)}, Training examples: {len(TRAIN_DATA)}, Dev examples: {len(DEV_DATA)}\")\n",
        "\n",
        "    # --- 3. TRAIN MODEL ---\n",
        "    print(\"\\nSetting up and training NER model...\")\n",
        "    trainer = SpacyNerTrainer(\n",
        "        blank_model_name=blank_model_name,\n",
        "        n_iterations=n_iterations,\n",
        "        dropout_rate=dropout,\n",
        "        batch_size=batch_size_train,\n",
        "        output_dir=output_model_dir\n",
        "    )\n",
        "    trainer.train_model(TRAIN_DATA)\n",
        "\n",
        "    # --- 4. SAVE MODEL ---\n",
        "    trainer.save_model()\n",
        "\n",
        "    # --- 5. TEST MODEL ---\n",
        "    test_text_example = \"I want a cheap italian restaurant nearby for dinner.\"\n",
        "    trainer.test_model(test_text_example)\n",
        "\n",
        "    if DEV_DATA:\n",
        "        print(\"\\n--- Evaluating on Dev Data (micro-averaged) ---\")\n",
        "\n",
        "        # 1) load model and get label set\n",
        "        nlp_loaded = spacy.load(output_model_dir)\n",
        "        tags = sorted(nlp_loaded.pipe_labels[\"ner\"])\n",
        "\n",
        "\n",
        "        # 2) accumulate references / predictions sentence-by-sentence\n",
        "        all_refs, all_preds = [], []\n",
        "        with open (\"dev_outputs.txt\",'w') as f:\n",
        "\n",
        "          for idx, sample in enumerate(DEV_DATA):\n",
        "              text,gold_ann= sample\n",
        "\n",
        "              doc = nlp_loaded(text)\n",
        "\n",
        "              gold_spans = gold_ann[\"entities\"]                         # (start,end,label)\n",
        "              pred_spans = [(ent.start_char, ent.end_char, ent.label_)  # same shape\n",
        "                            for ent in doc.ents]\n",
        "              f.write(f\"{idx+1}. Text: {text}\\n\")\n",
        "              f.write(f\"Gold Entities: {gold_spans}\\n\")\n",
        "              f.write(f\"Pred Entities: {pred_spans}\\n\")\n",
        "              f.write('\\n')\n",
        "              # trainer.make_dicts_from_offsets → ♥ keeps your helper\n",
        "              all_refs.append(trainer.make_dicts_from_offsets(text, gold_spans))\n",
        "              all_preds.append(trainer.make_dicts_from_offsets(text, pred_spans))\n",
        "\n",
        "          # 4) one evaluation pass, micro-averaged over all sentences\n",
        "          evaluator = Evaluator(all_refs, all_preds, tags=tags, loader=\"default\")\n",
        "          overall, per_type, _, _ = evaluator.evaluate()   # nervaluate ≥0.2.x\n",
        "          print(\"Overall:\", overall)\n",
        "          print(\"Per-type:\", per_type)\n",
        "\n",
        "          prec = overall[\"strict\"][\"precision\"]\n",
        "          rec  = overall[\"strict\"][\"recall\"]\n",
        "          f1   = overall[\"strict\"][\"f1\"]\n",
        "\n",
        "          print(f\"Micro Precision: {prec:.2%}\")\n",
        "          print(f\"Micro Recall:    {rec:.2%}\")\n",
        "          print(f\"Micro F1:        {f1:.2%}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "3klnPJ0iMbrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a563217-4163-4db6-e69d-0459e1d5cc4e"
      },
      "id": "3klnPJ0iMbrG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Loaded 7660 annotated sentences. ('2 start restaurants with inside dining', {'entities': [(0, 7, 'Rating'), (25, 38, 'Amenity')]})\n",
            "Total examples: 7573, Training examples: 6058, Dev examples: 1515\n",
            "\n",
            "Setting up and training NER model...\n",
            "Added labels to NER: ['Amenity', 'Cuisine', 'Dish', 'Hours', 'Location', 'Price', 'Rating', 'Restaurant_Name']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 190it [00:19,  9.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 finished. Average NER Loss (last batch): 1596.3876 (Total Epoch: 15963.88)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 190it [00:11, 16.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 finished. Average NER Loss (last batch): 988.4486 (Total Epoch: 9884.49)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 190it [00:11, 16.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 finished. Average NER Loss (last batch): 804.9422 (Total Epoch: 8049.42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 190it [00:10, 17.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 finished. Average NER Loss (last batch): 705.1609 (Total Epoch: 7051.61)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 190it [00:10, 17.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 finished. Average NER Loss (last batch): 651.8730 (Total Epoch: 6518.73)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 190it [00:11, 16.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 finished. Average NER Loss (last batch): 612.8253 (Total Epoch: 6128.25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 190it [00:11, 16.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 finished. Average NER Loss (last batch): 578.4261 (Total Epoch: 5784.26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 190it [00:11, 16.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 finished. Average NER Loss (last batch): 558.5384 (Total Epoch: 5585.38)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 190it [00:12, 15.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 finished. Average NER Loss (last batch): 518.2911 (Total Epoch: 5182.91)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 190it [00:11, 16.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 finished. Average NER Loss (last batch): 510.9116 (Total Epoch: 5109.12)\n",
            "Saved model to restaurant_ner_model_v2\n",
            "\n",
            "--- Loading and Testing Model ---\n",
            "Test text: 'I want a cheap italian restaurant nearby for dinner.'\n",
            "Entities found: [('cheap', 'Price'), ('italian', 'Cuisine'), ('nearby', 'Location'), ('dinner.', 'Hours')]\n",
            "\n",
            "--- Evaluating on Dev Data (micro-averaged) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f5a2c096bb27>:85: DeprecationWarning: The current evaluation method is deprecated and it will the output change in the next release.The output will change to a dictionary with the following keys: overall, entities, entity_results, overall_indices, entity_indices.\n",
            "  overall, per_type, _, _ = evaluator.evaluate()   # nervaluate ≥0.2.x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall: {'ent_type': {'correct': 2482, 'incorrect': 326, 'partial': 0, 'missed': 173, 'spurious': 231, 'possible': 2981, 'actual': 3039, 'precision': 0.8167160250082264, 'recall': 0.8326065078832606, 'f1': 0.8245847176079734}, 'partial': {'correct': 2376, 'incorrect': 0, 'partial': 432, 'missed': 173, 'spurious': 231, 'possible': 2981, 'actual': 3039, 'precision': 0.8529121421520237, 'recall': 0.8695068768869507, 'f1': 0.8611295681063125}, 'strict': {'correct': 2200, 'incorrect': 608, 'partial': 0, 'missed': 173, 'spurious': 231, 'possible': 2981, 'actual': 3039, 'precision': 0.723922342875946, 'recall': 0.7380073800738007, 'f1': 0.7308970099667774}, 'exact': {'correct': 2376, 'incorrect': 432, 'partial': 0, 'missed': 173, 'spurious': 231, 'possible': 2981, 'actual': 3039, 'precision': 0.7818361303060217, 'recall': 0.7970479704797048, 'f1': 0.7893687707641196}}\n",
            "Per-type: {'Amenity': {'ent_type': {'correct': 318, 'incorrect': 63, 'partial': 0, 'missed': 58, 'spurious': 66, 'possible': 439, 'actual': 447, 'precision': 0.7114093959731543, 'recall': 0.724373576309795, 'f1': 0.7178329571106096}, 'partial': {'correct': 278, 'incorrect': 0, 'partial': 103, 'missed': 58, 'spurious': 66, 'possible': 439, 'actual': 447, 'precision': 0.7371364653243848, 'recall': 0.7505694760820045, 'f1': 0.7437923250564334}, 'strict': {'correct': 256, 'incorrect': 125, 'partial': 0, 'missed': 58, 'spurious': 66, 'possible': 439, 'actual': 447, 'precision': 0.5727069351230425, 'recall': 0.5831435079726651, 'f1': 0.5778781038374716}, 'exact': {'correct': 278, 'incorrect': 103, 'partial': 0, 'missed': 58, 'spurious': 66, 'possible': 439, 'actual': 447, 'precision': 0.6219239373601789, 'recall': 0.6332574031890661, 'f1': 0.6275395033860044}}, 'Cuisine': {'ent_type': {'correct': 470, 'incorrect': 97, 'partial': 0, 'missed': 8, 'spurious': 24, 'possible': 575, 'actual': 591, 'precision': 0.7952622673434856, 'recall': 0.8173913043478261, 'f1': 0.8061749571183534}, 'partial': {'correct': 507, 'incorrect': 0, 'partial': 60, 'missed': 8, 'spurious': 24, 'possible': 575, 'actual': 591, 'precision': 0.9086294416243654, 'recall': 0.9339130434782609, 'f1': 0.9210977701543739}, 'strict': {'correct': 446, 'incorrect': 121, 'partial': 0, 'missed': 8, 'spurious': 24, 'possible': 575, 'actual': 591, 'precision': 0.754653130287648, 'recall': 0.7756521739130435, 'f1': 0.7650085763293311}, 'exact': {'correct': 507, 'incorrect': 60, 'partial': 0, 'missed': 8, 'spurious': 24, 'possible': 575, 'actual': 591, 'precision': 0.8578680203045685, 'recall': 0.8817391304347826, 'f1': 0.869639794168096}}, 'Dish': {'ent_type': {'correct': 208, 'incorrect': 66, 'partial': 0, 'missed': 21, 'spurious': 18, 'possible': 295, 'actual': 292, 'precision': 0.7123287671232876, 'recall': 0.7050847457627119, 'f1': 0.7086882453151617}, 'partial': {'correct': 237, 'incorrect': 0, 'partial': 37, 'missed': 21, 'spurious': 18, 'possible': 295, 'actual': 292, 'precision': 0.875, 'recall': 0.8661016949152542, 'f1': 0.8705281090289607}, 'strict': {'correct': 191, 'incorrect': 83, 'partial': 0, 'missed': 21, 'spurious': 18, 'possible': 295, 'actual': 292, 'precision': 0.6541095890410958, 'recall': 0.6474576271186441, 'f1': 0.6507666098807496}, 'exact': {'correct': 237, 'incorrect': 37, 'partial': 0, 'missed': 21, 'spurious': 18, 'possible': 295, 'actual': 292, 'precision': 0.8116438356164384, 'recall': 0.8033898305084746, 'f1': 0.807495741056218}}, 'Hours': {'ent_type': {'correct': 150, 'incorrect': 17, 'partial': 0, 'missed': 29, 'spurious': 25, 'possible': 196, 'actual': 192, 'precision': 0.78125, 'recall': 0.7653061224489796, 'f1': 0.7731958762886597}, 'partial': {'correct': 136, 'incorrect': 0, 'partial': 31, 'missed': 29, 'spurious': 25, 'possible': 196, 'actual': 192, 'precision': 0.7890625, 'recall': 0.7729591836734694, 'f1': 0.7809278350515463}, 'strict': {'correct': 127, 'incorrect': 40, 'partial': 0, 'missed': 29, 'spurious': 25, 'possible': 196, 'actual': 192, 'precision': 0.6614583333333334, 'recall': 0.6479591836734694, 'f1': 0.654639175257732}, 'exact': {'correct': 136, 'incorrect': 31, 'partial': 0, 'missed': 29, 'spurious': 25, 'possible': 196, 'actual': 192, 'precision': 0.7083333333333334, 'recall': 0.6938775510204082, 'f1': 0.7010309278350516}}, 'Location': {'ent_type': {'correct': 710, 'incorrect': 26, 'partial': 0, 'missed': 23, 'spurious': 51, 'possible': 759, 'actual': 787, 'precision': 0.9021601016518425, 'recall': 0.9354413702239789, 'f1': 0.9184993531694695}, 'partial': {'correct': 635, 'incorrect': 0, 'partial': 101, 'missed': 23, 'spurious': 51, 'possible': 759, 'actual': 787, 'precision': 0.8710292249047014, 'recall': 0.9031620553359684, 'f1': 0.8868046571798188}, 'strict': {'correct': 621, 'incorrect': 115, 'partial': 0, 'missed': 23, 'spurious': 51, 'possible': 759, 'actual': 787, 'precision': 0.7890724269377383, 'recall': 0.8181818181818182, 'f1': 0.8033635187580855}, 'exact': {'correct': 635, 'incorrect': 101, 'partial': 0, 'missed': 23, 'spurious': 51, 'possible': 759, 'actual': 787, 'precision': 0.806861499364676, 'recall': 0.836627140974967, 'f1': 0.8214747736093143}}, 'Price': {'ent_type': {'correct': 124, 'incorrect': 16, 'partial': 0, 'missed': 9, 'spurious': 7, 'possible': 149, 'actual': 147, 'precision': 0.8435374149659864, 'recall': 0.8322147651006712, 'f1': 0.8378378378378378}, 'partial': {'correct': 124, 'incorrect': 0, 'partial': 16, 'missed': 9, 'spurious': 7, 'possible': 149, 'actual': 147, 'precision': 0.8979591836734694, 'recall': 0.8859060402684564, 'f1': 0.8918918918918919}, 'strict': {'correct': 117, 'incorrect': 23, 'partial': 0, 'missed': 9, 'spurious': 7, 'possible': 149, 'actual': 147, 'precision': 0.7959183673469388, 'recall': 0.785234899328859, 'f1': 0.7905405405405406}, 'exact': {'correct': 124, 'incorrect': 16, 'partial': 0, 'missed': 9, 'spurious': 7, 'possible': 149, 'actual': 147, 'precision': 0.8435374149659864, 'recall': 0.8322147651006712, 'f1': 0.8378378378378378}}, 'Rating': {'ent_type': {'correct': 172, 'incorrect': 20, 'partial': 0, 'missed': 10, 'spurious': 24, 'possible': 202, 'actual': 216, 'precision': 0.7962962962962963, 'recall': 0.8514851485148515, 'f1': 0.8229665071770336}, 'partial': {'correct': 163, 'incorrect': 0, 'partial': 29, 'missed': 10, 'spurious': 24, 'possible': 202, 'actual': 216, 'precision': 0.8217592592592593, 'recall': 0.8787128712871287, 'f1': 0.8492822966507179}, 'strict': {'correct': 157, 'incorrect': 35, 'partial': 0, 'missed': 10, 'spurious': 24, 'possible': 202, 'actual': 216, 'precision': 0.7268518518518519, 'recall': 0.7772277227722773, 'f1': 0.7511961722488039}, 'exact': {'correct': 163, 'incorrect': 29, 'partial': 0, 'missed': 10, 'spurious': 24, 'possible': 202, 'actual': 216, 'precision': 0.7546296296296297, 'recall': 0.806930693069307, 'f1': 0.7799043062200958}}, 'Restaurant_Name': {'ent_type': {'correct': 330, 'incorrect': 21, 'partial': 0, 'missed': 15, 'spurious': 16, 'possible': 366, 'actual': 367, 'precision': 0.8991825613079019, 'recall': 0.9016393442622951, 'f1': 0.9004092769440654}, 'partial': {'correct': 296, 'incorrect': 0, 'partial': 55, 'missed': 15, 'spurious': 16, 'possible': 366, 'actual': 367, 'precision': 0.8814713896457765, 'recall': 0.8838797814207651, 'f1': 0.8826739427012277}, 'strict': {'correct': 285, 'incorrect': 66, 'partial': 0, 'missed': 15, 'spurious': 16, 'possible': 366, 'actual': 367, 'precision': 0.776566757493188, 'recall': 0.7786885245901639, 'f1': 0.7776261937244202}, 'exact': {'correct': 296, 'incorrect': 55, 'partial': 0, 'missed': 15, 'spurious': 16, 'possible': 366, 'actual': 367, 'precision': 0.8065395095367848, 'recall': 0.8087431693989071, 'f1': 0.8076398362892224}}}\n",
            "Micro Precision: 72.39%\n",
            "Micro Recall:    73.80%\n",
            "Micro F1:        73.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b9704f8-cc01-4159-841f-de6d7973e92b",
      "metadata": {
        "id": "4b9704f8-cc01-4159-841f-de6d7973e92b"
      },
      "source": [
        "## **References**\n",
        "\n",
        "\n",
        "1. https://www.kaggle.com/code/remakia/introduction-to-ner-part-i-rule-based\n",
        "2. https://spacy.io/\n",
        "3. https://www.kaggle.com/code/remakia/introduction-to-ner-part-i-rule-based\n",
        "4. https://www.nltk.org/\n",
        "5. https://medium.com/data-science/conditional-random-fields-explained-e5b8256da776\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7c0266-7f5b-4247-ac36-c6b44f90bf03",
      "metadata": {
        "id": "8b7c0266-7f5b-4247-ac36-c6b44f90bf03"
      },
      "source": [
        "# **Facilitator(s) Details**\n",
        "\n",
        "**Facilitator(s):**\n",
        "\n",
        "*   Name: [FELIX TETTEH AKWERH,Adwoa Asantewaa Bremang]\n",
        "*   Email: [felix.akwerh@knust.edu.gh,adwoabremang@gmail.com]\n",
        "*   LinkedIn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7257b3a4-6572-417d-8b22-b53efb4bd6b2",
      "metadata": {
        "id": "7257b3a4-6572-417d-8b22-b53efb4bd6b2"
      },
      "source": [
        "# **Reviewer’s Name**\n",
        "\n",
        "*   Name: [Reviewer’s Name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22f7673-72d0-4bf2-bc87-f9c7c2c4b32c",
      "metadata": {
        "id": "c22f7673-72d0-4bf2-bc87-f9c7c2c4b32c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
