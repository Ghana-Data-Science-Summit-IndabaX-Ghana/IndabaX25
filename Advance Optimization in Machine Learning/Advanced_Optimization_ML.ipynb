{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Optimisation Techniques for Machine Learning - Part 3\n",
    "\n",
    "#### Program: `Deep Learning Indabax, Ghana, 2025` . \n",
    "#### üè´ Institution: AIMS-RIC and ACITY \n",
    "#### üìÖ Date: `*16 th June, 2025`\n",
    "\n",
    "---\n",
    "\n",
    "##### üë®‚Äçüè´ Facilitator: Ishaya, Jeremiah Ayock & Toufiq Musah       \n",
    "\n",
    "**Lecturer and Researcher in Machine Learning**  \n",
    "\n",
    "‚úâÔ∏è Email: [jeremiah Ayock Ishaya](ayockishaya1029@gmail.com)  \n",
    "üîó LinkedIn: [jeremiah](https://www.linkedin.com/in/jeremiah-ayock-ishaya-a49a9999/)  \n",
    "\n",
    "‚úâÔ∏è Email: [toufiq Musah](toufiqmusah32@gmail.com)  \n",
    "üîó LinkedIn: [toufiq](https://www.linkedin.com/in/toufiqmusah/) \n",
    "\n",
    "---\n",
    "\n",
    "> üí° *Optimization is not just math, it is the engine behind breakthroughs in modern AI.*\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Tools and  Frameworks used  \n",
    "\n",
    "- Python 3.x . \n",
    "- TensorFlow 2.x / Keras\n",
    "- Optuna for Hyperparameter Tuning\n",
    "- Matplotlib / Seaborn for Visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Objectives\n",
    "\n",
    "By the end of this session, participants will be able to:\n",
    "\n",
    "\n",
    "1. **Understand the Role of Optimization in Deep Learning**  \n",
    "\n",
    "   - Explain how different optimizers (SGD, Momentum, RMSProp, Adam) affect training dynamics.  \n",
    "   - Recognize the trade-offs between convergence speed, stability, and generalization.\n",
    "\n",
    "2. **Compare and Evaluate Optimizers Experimentally**  \n",
    "\n",
    "   - Implement multiple optimizers in TensorFlow/Keras and assess their performance using loss/accuracy curves.  \n",
    "   - Diagnose underfitting/overfitting and interpret training behavior based on optimizer choice.\n",
    "\n",
    "3. **Apply Learning Rate Scheduling Strategies**  \n",
    "\n",
    "   - Integrate learning rate schedules such as `ExponentialDecay`, `CosineDecay`, and `OneCycleLR` in training pipelines.  \n",
    "   - Visualize and interpret how learning rate dynamics impact convergence and final performance.\n",
    "\n",
    "4. **Use Regularization Techniques to Improve Generalization**  \n",
    "\n",
    "   - Apply dropout and L2 regularization in CNNs to reduce overfitting.  \n",
    "   - Analyze the effect of regularization strategies on validation accuracy and loss.\n",
    "\n",
    "5. **Perform Hyperparameter Tuning Using Optuna**  \n",
    "\n",
    "   - Define an objective function and search space for tuning optimizers and regularization parameters.  \n",
    "   - Run automated hyperparameter optimization and analyze results.\n",
    "\n",
    "6. **Engage in Hands-on Practice**  \n",
    "\n",
    "   - Complete guided exercises to solidify understanding of each concept.  \n",
    "   - Collaborate on an integrated optimization challenge using best practices.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Areas in Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| ``Area``                | ``Description ``                                                 |\n",
    "|---------------------|--------------------------------------------------------------|\n",
    "| Optimizer Choice     | Affects convergence speed and generalization                 |\n",
    "| Learning Rate        | Most sensitive hyperparameter; needs careful tuning or scheduling |\n",
    "| Batch Size           | Impacts training stability and generalization               |\n",
    "| Weight Initialization| Affects early training dynamics                             |\n",
    "| Regularization       | Prevents overfitting via L1, L2, Dropout                    |\n",
    "| Early Stopping       | Reduces overfitting and computational cost                  |\n",
    "| Architecture Tuning  | Number of layers, units, filters                            |\n",
    "| Data Augmentation    | Improves generalization from limited data                  |\n",
    "| Gradient Clipping    | Stabilizes training by limiting extreme updates             |\n",
    "| Normalization Layers | E.g., ``BatchNorm`` speeds up training and improves stability   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing Libraries of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import SGD, Adam  \n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy  \n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Load and preprocess CIFAR-10 dataset .  \n",
    "\n",
    "**Reason:** CIFAR-10 is widely used for image classification benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples the training set\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.xlabel(y_train[i][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize using mean and standard deviation \n",
    "mean = np.mean(x_train, axis=(0, 1, 2),\n",
    "               keepdims=True)\n",
    "std = np.std(x_train, axis=(0, 1, 2),\n",
    "             keepdims=True)\n",
    "\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Define the CNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Define a deep neural network using Keras for CIFAR-10 image classification.\n",
    "\n",
    "**Includes:**\n",
    "\n",
    "- CNN architecture\n",
    "- Loss function: Cross-entropy\n",
    "- Evaluation metric: Accuracy\n",
    "\n",
    "**RaTational:** Establish a controlled baseline model to evaluate how different optimizers, scheduler, and regularization techniques affect training.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), \n",
    "                      activation='relu', \n",
    "                      padding='same', \n",
    "                      input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), \n",
    "                      activation='relu',\n",
    "                      padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, \n",
    "                     activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, \n",
    "                     activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "metric = SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Task 1**\n",
    "\n",
    "Modify the model above to:\n",
    "\n",
    "1. Add one more `Conv2D` + `BatchNorm` block. \n",
    "\n",
    "2. Use `ReLU` for all activations. \n",
    "\n",
    "4. Use `Dropout=0.3` instead of `0.25` in the first dropout layer. \n",
    "\n",
    "5. Try to understand how these changes may affect learning capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block of code here \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimization Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Compare different optimizers: SGD, Momentum, Adam, RMSProp.\n",
    "\n",
    "**Rational:** Different optimizers converge at different rates and reach different local minima. Professionals must know which optimizer fits their task best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjective to imports \n",
    "\n",
    "optimizers_dict = {\n",
    "    \"SGD\": optimizers.SGD(learning_rate=0.01),\n",
    "    \"Momentum\": optimizers.SGD(learning_rate=0.01, \n",
    "                               momentum=0.9),\n",
    "    \"Adam\": optimizers.Adam(learning_rate=0.001),\n",
    "    \"RMSprop\": optimizers.RMSprop(learning_rate=0.001) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "for name, opt in optimizers_dict.items():\n",
    "    print(f\"Training with {name}\")\n",
    "    model = build_base_model()\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=loss_fn, \n",
    "                  metrics=[metric])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_split=0.1, \n",
    "                        epochs=10, \n",
    "                        batch_size=64, \n",
    "                        verbose=0)\n",
    "    histories[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in histories.items():\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'],\n",
    "             label=name)\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the curves above,\n",
    "\n",
    "1. Which optimizer converges fastest?  \n",
    "\n",
    "2. Which optimizer leads to highest validation accuracy?  \n",
    "\n",
    "3. Suggest reasons for these behaviors based on your prior experience.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Improve learning dynamics using learning rate schedulers ``(Exponential Decay, Cosine Annealing).``  \n",
    "\n",
    "**Rational:** Fixed learning rates may prevent reaching optimal weights.Scheduling can ``accelerate convergence`` and ``improve generalization.``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical Formulation of Cosine Annealing\n",
    "\n",
    "The learning rate schedule follows:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{\\pi t}{T}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta_t$: Learning rate at epoch $t$\n",
    "- $\\eta_{max}$: Maximum learning rate (0.01 in your implementation)\n",
    "- $\\eta_{min}$: Minimum learning rate (1e-5 in your implementation)\n",
    "- $T$: Period length (20 in your implementation)\n",
    "- $t$: Current epoch number\n",
    "\n",
    "##### Boundary Conditions:\n",
    "\n",
    "- At $t = 0$: $\\cos(0) = 1 \\Rightarrow \\eta_0 = \\eta_{max}$\n",
    "- At $t = T$: $\\cos(\\pi) = -1 \\Rightarrow \\eta_T = \\eta_{min}$\n",
    "\n",
    "##### Phase Interpretation:\n",
    "\n",
    "For $t \\in [0, T]$:\n",
    "1. The $\\cos$ term decreases monotonically from 1 to -1\n",
    "2. The learning rate decays smoothly from $\\eta_{max}$ to $\\eta_{min}$\n",
    "3. The curve follows the first half-period of a cosine wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_annealing(epoch, lr):\n",
    "    max_lr = 0.01\n",
    "    min_lr = 1e-5\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * epoch / 20))\n",
    "\n",
    "lrs = [cosine_annealing(e, 0) for e in range(20)]\n",
    "plt.plot(lrs)\n",
    "plt.title(\"Cosine Annealing Schedule\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the cosine scheduler above in a model training loop.  \n",
    "\n",
    "2. Observe and log how the learning rate changes during training using the callback:  \n",
    "    `tf.keras.callbacks.LearningRateScheduler(cosine_annealing)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3 Implementation here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Control overfitting using ``Dropout`` and ``L2 Regularization.``\n",
    "\n",
    "**Rational:** Complex models often overfit. Regularization increases generalization without sacrificing model capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a regularized model with Dropout and L2 Regularization\n",
    "# Use 0.3 dropout for the first two layers and 0.5 for the last dropout layer\n",
    "# Use L2 regularization with a lambda of 0.001 for all layers \n",
    "# Use ReLU for all activations \n",
    "# Use BatchNormalization after each Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regularized_model():     \n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001), input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Task 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experiment with different L2 values: `0.0001`, `0.001`, `0.01`.  \n",
    "\n",
    "2. Track validation accuracy and loss. Which setting overfits least?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 Implementation here  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Use Optuna to find the best learning rate and dropout combination.\n",
    "\n",
    "**Rational:** Manual tuning is inefficient. Tools like Optuna accelerate experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    dropout_rate = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss=loss_fn, metrics=[metric])\n",
    "    history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=0)\n",
    "    return max(history.history['val_sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change `Adam` to `SGD` in the objective function. Run another study with `momentum` as an additional hyperparameter.  \n",
    "\n",
    "2. Observe how the optimal values shift.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5 Implementation here  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Wrap-up Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Discussion:**\n",
    "\n",
    "- How does choice of optimizer affect final accuracy?  \n",
    "\n",
    "- What happens when you use an overly aggressive learning rate?  \n",
    "\n",
    "- What combination of techniques gave best performance?  \n",
    "\n",
    "- Would you use the same setup in production?  \n",
    "\n",
    "- How can you log, monitor, and scale these experiments using tools like Weights & Biases?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** \n",
    "Theory guides intuition, but experiments validate solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button onclick=\"var sol=document.getElementById('sol1'); sol.style.display=sol.style.display==='none'?'block':'none'\">Show/Hide Solution</button>\n",
    "<div id=\"sol1\" style=\"display:none\">\n",
    "\n",
    "```python\n",
    "# Solution would go here\n",
    "print(\"2 + 2 =\", 4)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayadata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
