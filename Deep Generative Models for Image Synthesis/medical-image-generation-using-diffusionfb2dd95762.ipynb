{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1LfsGqCPgqXTb47fHjD1krY8qSZTIqTsf","timestamp":1746033866095},{"file_id":"https://github.com/reshalfahsi/medical-image-generation/blob/master/Medical_Image_Generation_Using_Diffusion_Model.ipynb","timestamp":1745384577218}]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1099232,"sourceType":"datasetVersion","datasetId":614679}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":211.123357,"end_time":"2025-05-26T16:54:29.811007","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-26T16:50:58.687650","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0d2b2a977cbd4e718d34cea0c5b377ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3e3f936918e1400c9e8a5b93e1396d2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447e3cdd35b74ab3afc9bd0da2d3a5c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7de24e9998af4f5bb515409ab0f33c9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e3f936918e1400c9e8a5b93e1396d2c","placeholder":"â€‹","style":"IPY_MODEL_e802b91b6f5a4f04b284c592d82c50a0","value":"â€‡0/2â€‡[00:00&lt;?,â€‡?it/s]"}},"9831bac7d8de463ca7bfeae8a1210b7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_447e3cdd35b74ab3afc9bd0da2d3a5c7","placeholder":"â€‹","style":"IPY_MODEL_c056b2af805d4a279e1f597938230838","value":"Sanityâ€‡Checkingâ€‡DataLoaderâ€‡0:â€‡â€‡â€‡0%"}},"c056b2af805d4a279e1f597938230838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cce2910ed1c543be9b2a90c8b9483da4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9831bac7d8de463ca7bfeae8a1210b7e","IPY_MODEL_fde30a38b8624541aa9891eb89e1b731","IPY_MODEL_7de24e9998af4f5bb515409ab0f33c9b"],"layout":"IPY_MODEL_0d2b2a977cbd4e718d34cea0c5b377ad"}},"d3810aa0d19b4d12899e4b5ee2d8df6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddfe9b8f3ea94ed9a46da0092cb6bdb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e802b91b6f5a4f04b284c592d82c50a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fde30a38b8624541aa9891eb89e1b731":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddfe9b8f3ea94ed9a46da0092cb6bdb5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3810aa0d19b4d12899e4b5ee2d8df6c","value":0}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Medical Image Generation with Diffusion Models**","metadata":{"id":"nxLWNsrydEIG","papermill":{"duration":0.015066,"end_time":"2025-05-26T16:51:05.241584","exception":false,"start_time":"2025-05-26T16:51:05.226518","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Table of Contents\n1. [Introduction to Diffusion Models](#introduction)\n2. [Prerequisites](#prerequisites)\n3. [Dataset Preparation](#dataset)\n4. [Coding the Diffusion Model](#coding-diffusion)\n5. [Training](#training)\n6. [Testing](#testing)\n7. [Inference](#inference)","metadata":{}},{"cell_type":"markdown","source":"## ğŸ§ª Introduction to Diffusion Models\n<a id=\"introduction\"></a>\n\nIn recent years, **diffusion models** have emerged as a powerful class of generative models, rivaling and even surpassing traditional methods like GANs and VAEs in image synthesis tasks. These models generate high-quality images by learning to gradually reverse a noise processâ€”starting from random noise and iteratively denoising it to recover realistic data samples.\n\nThis notebook will walk you through the **foundational components of a basic diffusion model**, starting from the **forward noising process**, moving to the **denoising network architecture**, and finally arriving at the **sampling process** used to generate new images. You will get hands-on experience coding the key parts of a DDPM (Denoising Diffusion Probabilistic Model), helping you build both intuition and implementation skills.\n\nWe'll begin with a refresher on earlier generative models like GANs and VAEs, and then progressively build toward training a basic diffusion model that can generate images from scratch.\n\nBy the end of this session, you will have:\n- Understood the theoretical principles behind diffusion models.\n- Implemented core components such as the noise scheduler and denoising network.\n- Trained a model to generate new samples.\n- Explored possible extensions, such as class-conditioning and improved sampling strategies.\n\n> Let's dive into the world of generative AIâ€”one noisy step at a time!","metadata":{}},{"cell_type":"markdown","source":"## ğŸ“š Prerequisites\n<a id=\"prerequisites\"></a>\n\nBefore we begin building diffusion models, itâ€™s important to ensure you're comfortable with the following concepts and tools. This will help you follow along and get the most out of the hands-on coding experience.\n\n### âœ… Technical Knowledge\n- **Basic understanding of neural networks** and deep learning workflows.\n- Familiarity with **convolutional neural networks (CNNs)**.\n- Knowledge of **PyTorch** (especially `nn.Module`, data loaders, training loops).\n- Understanding of **loss functions** like MSE, and optimizers like Adam.\n\n### ğŸ§  Conceptual Foundations\n- What are **generative models**? (How they learn to model a data distribution)\n- Basic knowledge of **GANs (Generative Adversarial Networks)** and **VAEs (Variational Autoencoders)**.\n- Familiarity with **probability distributions** (especially Gaussian/Normal distributions).\n- Concept of **adding noise** to data and recovering it through learning.\n\nIf you're not familiar with some of these, don't worryâ€”brief refreshers will be provided along the way.\n\n> Letâ€™s now take a quick look at traditional generative models (GANs and VAEs) before introducing diffusion models.\n","metadata":{}},{"cell_type":"markdown","source":"## Building Diffusion Models\n### Code Implementation","metadata":{}},{"cell_type":"markdown","source":"### Setting Up\n* Install and import required libraries\n* Define utility function to enable us visualize images\n* Define important variables","metadata":{}},{"cell_type":"code","source":"# Install required Python packages using pip.\n# - lightning: for building and training PyTorch models easily\n# - torchmetrics: to calculate performance metrics like accuracy, FID, etc.\n# - torch-fidelity: for evaluating generative models (e.g., FID score)\n\n# Note: '--no-cache-dir' prevents using cached versions of the packages\n!pip install -q --no-cache-dir lightning torchmetrics torch-fidelity","metadata":{"execution":{"iopub.status.busy":"2025-06-16T19:18:10.451673Z","iopub.execute_input":"2025-06-16T19:18:10.451981Z","iopub.status.idle":"2025-06-16T19:20:08.756489Z","shell.execute_reply.started":"2025-06-16T19:18:10.451960Z","shell.execute_reply":"2025-06-16T19:20:08.755032Z"},"papermill":{"duration":203.854755,"end_time":"2025-05-26T16:54:29.138879","exception":false,"start_time":"2025-05-26T16:51:05.284124","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import Lightning and PyTorch Lightning utilities\nimport lightning as L\nfrom lightning.pytorch import Trainer, seed_everything\nfrom lightning.pytorch.callbacks import ModelCheckpoint  # For saving the best model\n\n# Import FID metric from TorchMetrics to evaluate generative models\nfrom torchmetrics.image.fid import FrechetInceptionDistance\n\n# Import a helper from Google Colab to display images with OpenCV\nfrom google.colab.patches import cv2_imshow\n\n# Import core PyTorch libraries\nimport torch\nimport torch.nn as nn  # For defining neural networks\nimport torch.optim as optim  # For optimization algorithms like Adam\nimport torch.utils.data as data  # For data loading and batching\nimport torch.nn.functional as F  # For functions like relu, softmax, etc.\n\n# Import torchvision tools for data transformation and augmentation\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, Resize\n\n# PIL for image handling\nfrom PIL import Image\n\n# tqdm for showing progress bars\nfrom tqdm.auto import tqdm\n\n# Numpy and matplotlib for numerical operations and visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Standard libraries\nimport os\nimport cv2\nimport random\nimport math\n\n# Ignore warning messages to keep the output clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plot settings: inline display and aesthetic customization\n%matplotlib inline\nplt.rcParams['axes.facecolor'] = 'lightgray'  # Set background color of plots\nplt.rcParams['mathtext.fontset'] = 'cm'  # Use Computer Modern font for math\nplt.rcParams['font.family'] = 'STIXGeneral'  # Use STIX font for consistency","metadata":{"execution":{"iopub.status.busy":"2025-06-16T19:20:08.758866Z","iopub.execute_input":"2025-06-16T19:20:08.759180Z","iopub.status.idle":"2025-06-16T19:20:25.256362Z","shell.execute_reply.started":"2025-06-16T19:20:08.759147Z","shell.execute_reply":"2025-06-16T19:20:25.255179Z"},"papermill":{"duration":0.121395,"end_time":"2025-05-26T16:54:29.273322","exception":true,"start_time":"2025-05-26T16:54:29.151927","status":"failed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def show_tensor_image(image):\n    # Define a series of reverse transforms to convert a normalized tensor image\n    # back to a format that can be displayed as a proper image\n    reverse_transforms = Compose([\n        # Undo normalization from [-1, 1] to [0, 1]\n        Lambda(lambda t: (t + 1) / 2),\n\n        # Change shape from [C, H, W] to [H, W, C] for display\n        Lambda(lambda t: t.permute(1, 2, 0)),\n\n        # Scale pixel values from [0, 1] to [0, 255]\n        Lambda(lambda t: t * 255.),\n\n        # Move tensor to CPU, convert to NumPy array, and change dtype to uint8 for image display\n        Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n\n        # Convert the NumPy array to a PIL Image\n        ToPILImage(),\n\n        # Resize the image to a fixed size (e.g., for consistent display)\n        Resize(IMAGE_SIZE),\n    ])\n\n    # If the input is a batch of images (4D tensor), take the first one only\n    if len(image.shape) == 4:\n        image = image[0, :, :, :]  # Select the first image in the batch\n\n    # Turn off axis labels and ticks\n    plt.axis('off')\n\n    # Apply reverse transforms and display the image using matplotlib\n    plt.imshow(reverse_transforms(image))\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T19:20:25.257390Z","iopub.execute_input":"2025-06-16T19:20:25.257963Z","iopub.status.idle":"2025-06-16T19:20:25.264707Z","shell.execute_reply.started":"2025-06-16T19:20:25.257931Z","shell.execute_reply":"2025-06-16T19:20:25.263872Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"MAX_EPOCH = 30\nDIFFUSION_STEP = 1000\nBATCH_SIZE = 20\nLR = 3e-4\nCHECKPOINT_DIR = os.getcwd()\nIMAGE_SIZE = 48\nSCALE_DOWN = 2\nN_CHANNEL = 3 #INFO[FLAG]['n_channels']\nBETA_START = 1e-4\nBETA_END = 2e-2\nN_CLASSES = 2# len(INFO[FLAG]['label'])","metadata":{"execution":{"iopub.status.busy":"2025-06-16T19:20:25.266441Z","iopub.execute_input":"2025-06-16T19:20:25.266815Z","iopub.status.idle":"2025-06-16T19:20:25.288905Z","shell.execute_reply.started":"2025-06-16T19:20:25.266784Z","shell.execute_reply":"2025-06-16T19:20:25.287886Z"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1747755891211,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"WCjIc6r__B8G","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## ğŸ—‚ï¸ Dataset Preparation","metadata":{}},{"cell_type":"markdown","source":"### ğŸ“¥ Adding the Medical MNIST Dataset to Your Kaggle Notebook\n\nBefore we begin training our model, we need to make sure the **Medical MNIST** dataset is available in our working environment.\n\nSince we're using **Kaggle Notebooks**, adding a dataset is straightforward and does not require any download or upload. Follow the steps below:\n\n#### âœ… Steps to Add the Dataset from the Notebook\n\n1. **Open Your Kaggle Notebook**\n   Make sure you're inside the notebook you'll be working in.\n\n2. **Click on the â€œAdd Inputâ€ Panel**\n   - Look to the **right side** of your notebook interface.\n   - Youâ€™ll see a sidebar labeled **â€œInput,â€ â€œOutput,â€ â€œTable of Contents,â€ etc.**\n   - Click on the **â€œ+ Add Inputâ€** button under the **â€œInputâ€** tab.\n\n3. **Search for the Dataset**\n   - In the search box that appears, type:  \n     ```\n     medical mnist\n     ```\n   - Locate the dataset titled **â€œMedical MNISTâ€** (uploaded by `Larxel`).\n\n4. **Attach the Dataset**\n   - Click the **â€œAddâ€ (+)** button next to the dataset name.\n   - This will attach the dataset to your notebook under the `/kaggle/input/` directory.\n","metadata":{}},{"cell_type":"markdown","source":"### Processing the Dataset\nIn this section, weâ€™ll walk through how to **prepare a custom dataset** for training a conditional diffusion model. The goal is to allow our model to learn to generate images from specific classes â€” *Chest* vs *Hand* X-ray images.\n\n1. **Image Preprocessing**  \n   We'll define a transformation pipeline that resizes, normalizes, and converts the images into tensors. This ensures that all images are in the same format and scale before being fed into our model.\n\n2. **Label Formatting (One-Hot Encoding)**  \n   Since weâ€™re working with two classes and using a class-conditioned diffusion model, weâ€™ll convert each class label into a **one-hot encoded tensor**. This allows the model to receive class information in a structured and consistent format during training.\n\n3. **Custom Dataset Class**  \n   To have more control over which classes are loaded from a dataset, weâ€™ll build a **custom dataset class** by extending `torchvision.datasets.DatasetFolder`. This will allow us to:\n   - Filter and load only specific subfolders (i.e., only the desired classes),\n   - Apply our transformations,\n   - Return both the image and its corresponding one-hot encoded label.\n","metadata":{"id":"PrhyQrbmdFCQ","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Define a transformation pipeline to preprocess images before feeding them into a model\nimage_transform = Compose([\n    # Resize the image to a fixed size (IMAGE_SIZE x IMAGE_SIZE)\n    Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\n    # Convert the image from PIL or numpy format to a PyTorch tensor\n    # This also scales pixel values from [0, 255] to [0.0, 1.0]\n    ToTensor(),\n\n    # Normalize the tensor values from [0.0, 1.0] to [-1.0, 1.0]\n    Lambda(lambda x: (x * 2) - 1),\n])\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.592866Z","iopub.execute_input":"2025-05-26T22:00:44.593144Z","iopub.status.idle":"2025-05-26T22:00:44.599148Z","shell.execute_reply.started":"2025-05-26T22:00:44.593121Z","shell.execute_reply":"2025-05-26T22:00:44.597841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This function converts a class label into a one-hot encoded tensor with 2 classes\ndef one_hot_encode(label):\n    # Convert the label (e.g., 0 or 1) to a PyTorch tensor\n    # Then apply one-hot encoding with 2 classes (e.g., [1, 0] for class 0, [0, 1] for class 1)\n    return F.one_hot(torch.tensor(label), num_classes=2).float()\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.600456Z","iopub.execute_input":"2025-05-26T22:00:44.600838Z","iopub.status.idle":"2025-05-26T22:00:44.605976Z","shell.execute_reply.started":"2025-05-26T22:00:44.600805Z","shell.execute_reply":"2025-05-26T22:00:44.604947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.datasets import DatasetFolder\nfrom torchvision.datasets.folder import default_loader\nfrom typing import List, Tuple, Dict\nimport os\n\n# A custom dataset class that extends torchvision's DatasetFolder\n# It filters and loads only images from specified subfolders (i.e., desired classes)\nclass CustomImageFolder(DatasetFolder):\n    def __init__(\n        self,\n        root: str,                          # Root directory containing class-named folders\n        desired_classes: List[str],         # List of class folder names to include\n        transform=None,                     # Optional image transformations\n        target_transform=None,              # Optional label transformations\n        loader=default_loader,              # Function to load an image file\n        is_valid_file=None                  # Optional custom file filter\n    ):\n        self.desired_classes = desired_classes  # Store desired class names\n        # Call the parent DatasetFolder constructor with appropriate settings\n        super().__init__(\n            root,\n            loader,\n            extensions=('jpg', 'jpeg', 'png'),  # Allowed image formats\n            transform=transform,\n            target_transform=target_transform,\n            is_valid_file=is_valid_file\n        )\n\n    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n        \"\"\"\n        Override the default class discovery to include only desired classes.\n\n        Args:\n            directory (str): Path to the root dataset directory.\n\n        Returns:\n            classes (List[str]): List of selected class folder names.\n            class_to_idx (Dict[str, int]): Mapping from class names to indices.\n        \"\"\"\n        # Scan the dataset directory and list all subfolders\n        all_classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n\n        # Filter only those that match the desired_classes list\n        classes = [cls for cls in all_classes if cls in self.desired_classes]\n\n        # Raise an error if none of the desired classes were found\n        if not classes:\n            raise FileNotFoundError(\n                f\"No matching class folders found in {directory}. \"\n                f\"Available: {all_classes}\"\n            )\n\n        # Create a dictionary mapping class names to integer indices\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n\n        return classes, class_to_idx\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.607080Z","iopub.execute_input":"2025-05-26T22:00:44.607517Z","iopub.status.idle":"2025-05-26T22:00:44.616698Z","shell.execute_reply.started":"2025-05-26T22:00:44.607483Z","shell.execute_reply":"2025-05-26T22:00:44.615653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = CustomImageFolder(\n    root=\"/kaggle/input/medical-mnist\",        # Path to the root directory of the dataset.\n                                                # This directory should contain subfolders named after class labels (e.g., \"CXR\", \"Hand\", etc.)\n\n    desired_classes=[\"CXR\", \"Hand\"],            # Only include images from the \"CXR\" and \"Hand\" subfolders (i.e., filter by desired classes).\n\n    transform=image_transform,                  # Apply preprocessing to each image:\n                                                # - Resize to IMAGE_SIZE x IMAGE_SIZE\n                                                # - Convert to tensor\n                                                # - Normalize from [0, 1] to [-1, 1]\n\n    target_transform=one_hot_encode             # Apply one-hot encoding to the class label (e.g., 0 -> [1, 0], 1 -> [0, 1])\n)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:01:41.017103Z","iopub.execute_input":"2025-05-26T22:01:41.017500Z","iopub.status.idle":"2025-05-26T22:02:23.967006Z","shell.execute_reply.started":"2025-05-26T22:01:41.017473Z","shell.execute_reply":"2025-05-26T22:02:23.965791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the list of class folder names included in the dataset\n# This will only include the classes specified in 'desired_classes' (e.g., ['CXR', 'Hand'])\nprint(dataset.classes)\n\n# Print the mapping of class names to numerical indices\n# This dictionary is used internally to assign numeric labels to each class\nprint(dataset.class_to_idx)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:23.969009Z","iopub.execute_input":"2025-05-26T22:02:23.969323Z","iopub.status.idle":"2025-05-26T22:02:23.974546Z","shell.execute_reply.started":"2025-05-26T22:02:23.969301Z","shell.execute_reply":"2025-05-26T22:02:23.973614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of samples per class in the dataset\n# dataset.targets is a list of class indices for each image (e.g., [0, 0, 1, 0, 1, 1, ...])\nclass_counts = torch.bincount(torch.tensor(dataset.targets))\n\n# Loop through each class and print its name and corresponding image count\nfor i, count in enumerate(class_counts):\n    # dataset.classes[i] gets the class name (e.g., 'CXR' or 'Hand')\n    # count.item() gets the number of images for that class\n    print(f\"Class: {dataset.classes[i]}, Count: {count.item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:23.975404Z","iopub.execute_input":"2025-05-26T22:02:23.975642Z","iopub.status.idle":"2025-05-26T22:02:23.990714Z","shell.execute_reply.started":"2025-05-26T22:02:23.975624Z","shell.execute_reply":"2025-05-26T22:02:23.989734Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Define the number of samples for validation and test sets\nval_size = 4        # Number of samples to reserve for validation\ntest_size = 4       # Number of samples to reserve for testing\n\n# Compute the number of samples for the training set\n# Total = len(dataset), so training = total - val - test\ntrain_size = len(dataset) - val_size - test_size\n\n# Step 3: Randomly split the dataset into training, validation, and test sets\nTrainDataset, ValDataset, TestDataset = data.random_split(\n    dataset,                         # The full dataset to split\n    [train_size, val_size, test_size],  # List of sizes for each split\n)","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:23.993154Z","iopub.execute_input":"2025-05-26T22:02:23.993494Z","iopub.status.idle":"2025-05-26T22:02:24.000863Z","shell.execute_reply.started":"2025-05-26T22:02:23.993470Z","shell.execute_reply":"2025-05-26T22:02:23.999586Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get one sample (image and label) from the training dataset\nimages = TrainDataset[0]  # This returns a tuple: (image_tensor, label_tensor)\n\n# Print the shape of the image tensor\nprint(images[0].shape)    # Should be something like torch.Size([3, H, W])\n\n# Extract the label from the tuple\nlabel = images[1]         # Can be a one-hot encoded tensor or class index depending on your target_transform\n\n# Print the type of the label (e.g., <class 'torch.Tensor'>)\nprint(type(label))\n\n# Print the actual label value (e.g., tensor([1., 0.]))\nprint(\"label\", label)\n\n# Another way to explicitly show the image shape\nprint(f\"Image shape: {images[0].shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.001893Z","iopub.execute_input":"2025-05-26T22:02:24.002148Z","iopub.status.idle":"2025-05-26T22:02:24.121123Z","shell.execute_reply.started":"2025-05-26T22:02:24.002131Z","shell.execute_reply":"2025-05-26T22:02:24.119898Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize an index counter\nindex = 0\n\n# Iterate through all samples (image, label) in the training dataset\nfor image, label in TrainDataset:\n    \n    # Check if the label matches [1., 0.] â€” i.e., class 0 in one-hot encoding\n    # (label == torch.tensor([1., 0.])) returns a boolean tensor, so sum(...) counts how many elements match\n    if sum(label == torch.tensor([1., 0.])) > 1:\n        \n        # If it's a match, increment the counter\n        index += 1\n\n        # Display the image using your custom image display function\n        show_tensor_image(image)\n\n        # Break after showing more than 5 matching images\n        if index > 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.122210Z","iopub.execute_input":"2025-05-26T22:02:24.122598Z","iopub.status.idle":"2025-05-26T22:02:24.466976Z","shell.execute_reply.started":"2025-05-26T22:02:24.122567Z","shell.execute_reply":"2025-05-26T22:02:24.466050Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Implementation\nIn this section, weâ€™ll begin building our **Diffusion Model** â€” a powerful generative model used for creating high-quality synthetic images. We'll break it down into understandable, modular components so that each part makes sense and can be reused or extended later.\n\n\n### ğŸ”§ Step-by-Step Breakdown\n\nWe'll proceed in the following order:\n\n1. **ğŸ“Š Utility Function (AverageMeter)**\n   - We'll define a simple utility class to keep track of metrics like loss during training.\n\n\n2. **ğŸŒ«ï¸ Noise Scheduler**\n   - The forward (diffusion) process starts here.\n   - This component defines how noise is added to clean images over time (timesteps), simulating the degradation process.\n\n\n3. **ğŸ§± Core U-Net Architecture**\n   - Our model will follow a **U-Net**-like structure, which is commonly used in denoising and image segmentation tasks.\n   - We'll implement it **block by block**, building from the ground up:\n     - **Self-Attention Block:** Helps the model learn long-range dependencies.\n     - **Double Convolution Block:** Applies two successive convolutional layers to capture features.\n     - **Downsampling Block:** Reduces spatial resolution and increases feature abstraction.\n     - **Upsampling Block:** Restores spatial resolution, guided by features from earlier layers.\n     - **Positional Embeddings:** Encodes timestep and context information into the model.\n\n\n4. **ğŸ§© Assembling the U-Net**\n   - All components from step 3 are brought together to construct the full U-Net.\n\n5. **ğŸŒ€ The Diffusion Model**\n   - Here we combine:\n     - The **Noise Scheduler** from step 2\n     - The **U-Net** model from step 4\n   - This results in a complete diffusion system that learns how to **reverse the noise process**, gradually transforming pure noise into meaningful images conditioned on class labels or time.\n\n","metadata":{"id":"M9bvhMihdFNH","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"class AvgMeter(object):\n    def __init__(self, num=40):\n        \"\"\"\n        Initialize the average meter.\n\n        Args:\n            num (int): Number of recent values to keep for computing the average.\n        \"\"\"\n        self.num = num  # Limit to the most recent `num` values\n        self.reset()    # Initialize/reset the scores list\n\n    def reset(self):\n        \"\"\"\n        Clear the stored scores.\n        \"\"\"\n        self.scores = []\n\n    def update(self, val):\n        \"\"\"\n        Add a new value to the score list.\n\n        Args:\n            val (float or tensor): The new value to track.\n        \"\"\"\n        self.scores.append(val)\n\n    def show(self):\n        \"\"\"\n        Compute and return the average of the most recent `num` values.\n\n        Returns:\n            torch.Tensor: The mean of the recent values as a torch scalar.\n        \"\"\"\n        # Only use the last `num` values for averaging\n        out = torch.mean(\n            torch.stack(\n                self.scores[np.maximum(len(self.scores) - self.num, 0):]  # Get last `num` elements\n            )\n        )\n        return out\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.468118Z","iopub.execute_input":"2025-05-26T22:02:24.468481Z","iopub.status.idle":"2025-05-26T22:02:24.474886Z","shell.execute_reply.started":"2025-05-26T22:02:24.468458Z","shell.execute_reply":"2025-05-26T22:02:24.473906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Forward Diffusion - Noise Scheduler\n* The forward step is a Markov chain where the current noisy image is dependent or computed from the previous noisy image. Remember that in diffusion, we gradually destroy the image by adding noise to the image at each timestep. The conditional probability distribution of $x_t$ given $x_t-1$ is represented by,\n    $$\n        q(x_t \\mid x_{t-1}) = \\mathcal{N}\\left( \\sqrt{\\alpha_t} x_{t-1}, \\, (1 - \\alpha_t) \\cdot \\mathbf{I} \\right)   ------- (1)\n    $$\n\n  Where,\n\n  $\\alpha_t$ is the noise scaling factor or signal retention factor at timestep $t$, defined as $Î±_t$ = $1âˆ’\\beta_t$ , where $\\beta_t$ is the noise variance schedule or how much noise to add. \n\n  $x_t$ is the new noisy image at timestep $t$\n\n  $x_t-1$ is the previous image at timestep $t-1$\n\n  $q(x_t \\mid x_{t-1})$ is the conditional probability of $x_t$ given $x_t-1$\nTo actually sample and generate $x_t$ from $x_t-1$ by adding noise, we use\n    $$\n        x_t = \\sqrt{1 - \\beta_t} \\, x_{t-1} + \\sqrt{\\beta_t} \\, \\mathcal{N}(0, \\mathbf{I})                            ------- (2)\n    $$\n\n\n* This directly means that, we need to simulate the entire Markov Chain during training. Think computational expenses? ğŸ¤­. Iterating up to `t` steps per training example.\n\n* Fortunately, we can use the closed-form formula to directly sample `x_t` (noisy image at any arbitrary timestep) from `x_0` (the original undisturbed image), without needing to sequentially apply all the intermediate steps. It is given by:\n    $$\n        x_t=\\ \\sqrt{{\\bar{\\alpha}}_t}\\ .\\ x_0\\ +\\ \\sqrt{1\\ -\\ {\\bar{\\alpha}}_t}\\ .\\ \\epsilon\\                         ------- (3)\n    $$\n\n  Where $$ \\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s $$\n* Compute the comulative product of the values in the tensor sel.alpha across timesteps.","metadata":{"id":"OUP90Trjij2r","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"class NoiseScheduler(nn.Module):\n    def __init__(\n        self,\n        T=DIFFUSION_STEP,          # Total number of diffusion steps (e.g., 1000)\n        beta_start=BETA_START,     # Starting value of beta (e.g., 1e-4)\n        beta_end=BETA_END          # Ending value of beta (e.g., 0.02)\n    ):\n        super().__init__()\n\n        # Number of timesteps in the diffusion process\n        self.T = T\n\n        # Linearly spaced beta schedule: controls the amount of noise added at each step\n        self.beta = torch.linspace(beta_start, beta_end, T).to(\n            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n\n        # Task 0: Compute alpha, which represents the amount of original signal preserved at each step\n        ### START CODE HERE\n        self.alpha = None  # Replace None with your code\n        ### END CODE HERE\n\n        # alpha_hat = cumulative product of alpha over time\n        # This represents the overall preservation of the original signal up to time t\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n    def forward(self, x, t):\n        \"\"\"\n        Add noise to input `x` based on timestep `t`.\n\n        Args:\n            x (Tensor): Original clean image tensor [B, C, H, W]\n            t (Tensor): Timestep tensor [B] containing values in [0, T)\n\n        Returns:\n            x_noisy (Tensor): Noisy version of the input image\n            noise (Tensor): The random noise that was added\n        \"\"\"\n\n        # Task 1: Calculate the square root of alpha_hat\n        ### START CODE HERE\n        sqrt_alpha_hat = None # Replace None with your code \n        ### END CODE HERE\n\n        # Task 2: Calculate the square root of one_minus_alpha_hat\n        ### START CODE HERE\n        sqrt_one_minus_alpha_hat = None # Replace None with your code\n        ### END CODE HERE\n\n        ### DO NOT EDIT THIS CODE\n        # Reshape alpha_hat[t] to [B, 1, 1, 1] to match image dimensions\n        sqrt_alpha_hat = sqrt_alpha_hat[:, None, None, None]\n        sqrt_one_minus_alpha_hat = sqrt_one_minus_alpha_hat.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n        ### DO NOT EDIT THIS CODE\n        \n\n        # Task 3: Generate standard Gaussian noise of the same shape as x\n        ### START CODE HERE\n        noise = None # Replace None with your code\n        ### END CODE HERE\n\n        # Task 4: Create the noisy image using the reparameterization formula\n        ### START CODE HERE\n        x_noisy = None # Replace None with your code\n        ### END CODE HERE\n        \n        return x_noisy, noise\n\nFORWARD = NoiseScheduler","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.475849Z","iopub.execute_input":"2025-05-26T22:02:24.476101Z","iopub.status.idle":"2025-05-26T22:02:24.483934Z","shell.execute_reply.started":"2025-05-26T22:02:24.476081Z","shell.execute_reply":"2025-05-26T22:02:24.482811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class NoiseScheduler(nn.Module):\n#     def __init__(\n#         self,\n#         T=DIFFUSION_STEP,          # Total number of diffusion steps (e.g., 1000)\n#         beta_start=BETA_START,     # Starting value of beta (e.g., 1e-4)\n#         beta_end=BETA_END          # Ending value of beta (e.g., 0.02)\n#     ):\n#         super().__init__()\n\n#         # Number of timesteps in the diffusion process\n#         self.T = T\n\n#         # Linearly spaced beta schedule: controls the amount of noise added at each step\n#         self.beta = torch.linspace(beta_start, beta_end, T).to(\n#             \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#         )\n\n#         # alpha = 1 - beta, which represents the amount of original signal preserved at each step\n#         self.alpha = 1. - self.beta\n\n#         # alpha_hat = cumulative product of alpha over time\n#         # This represents the overall preservation of the original signal up to time t\n#         self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n#     def forward(self, x, t):\n#         \"\"\"\n#         Add noise to input `x` based on timestep `t`.\n\n#         Args:\n#             x (Tensor): Original clean image tensor [B, C, H, W]\n#             t (Tensor): Timestep tensor [B] containing values in [0, T)\n\n#         Returns:\n#             x_noisy (Tensor): Noisy version of the input image\n#             noise (Tensor): The random noise that was added\n#         \"\"\"\n#         # Reshape alpha_hat[t] to [B, 1, 1, 1] to match image dimensions\n#         sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n#         sqrt_one_minus_alpha_hat = torch.sqrt(\n#             1 - self.alpha_hat[t]\n#         ).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n\n#         # Generate standard Gaussian noise of the same shape as x\n#         noise = torch.randn_like(x)\n\n#         # Create the noisy image using the reparameterization formula:\n#         # x_noisy = sqrt(alpha_hat) * x + sqrt(1 - alpha_hat) * noise\n#         x_noisy = sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise\n\n#         return x_noisy, noise\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **ğŸ” Backward Diffusion â€“ The UNet Architecture**\n\nIn the backward diffusion process, the goal is to denoise an image step by step until we recover a clean sample. The UNet architecture is used to predict the noise present in a noisy image at each timestep.\n\nThe network takes in:\n\n* A **noisy image** `xâ‚œ`\n* The **current timestep** `t`\n* An optional **class or context label** `y`\n\nIt then outputs an estimate of the noise added at that step.\n\nThe UNet follows an encoderâ€“decoder structure:\n\n* The **encoder** captures features and reduces the spatial size of the input.\n* The **bottleneck** processes compressed features.\n* The **decoder** upsamples the features and uses skip connections from the encoder to preserve details.\n* **Self-attention layers** help the model focus on important regions.\n* **Time and context embeddings** are injected into several blocks to guide the denoising process based on timestep and class.\n\nBy learning to predict the noise, the UNet helps reverse the diffusion process and gradually reconstruct clean images from pure noise.\n","metadata":{"id":"uWcPMvglioOY","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"#### ğŸ§  Self-Attention Layer\n\nThe self-attention layer helps the model focus on the most important regions of the image by allowing each pixel to consider information from all other pixels. This improves the modelâ€™s ability to capture global patterns and relationships, leading to better feature understanding and image generation.","metadata":{}},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, channels, size):\n        super(SelfAttention, self).__init__()\n        \n        self.channels = channels  # Number of channels in the input (e.g., feature maps)\n        self.size = size          # Spatial size (height/width) of the feature map\n\n        # Multi-Head Attention: operates on flattened spatial features\n        self.mha = nn.MultiheadAttention(\n            embed_dim=channels,     # Each token has 'channels' features\n            num_heads=4,            # Number of attention heads\n            batch_first=True        # Input/output shape: (B, N, C) instead of default (N, B, C)\n        )\n\n        # LayerNorm before attention\n        self.ln = nn.LayerNorm([channels])\n\n        # Feed-forward block after attention\n        self.ff_self = nn.Sequential(\n            nn.LayerNorm([channels]),\n            nn.Linear(channels, channels),\n            nn.GELU(),\n            nn.Linear(channels, channels),\n        )\n\n    def forward(self, x):\n        # Input x: [B, C, H, W]\n        \n        # Reshape to [B, N, C] where N = H*W (flatten spatial dims), suitable for attention\n        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)  # => [B, N, C]\n\n        # Apply LayerNorm before attention (standard transformer practice)\n        x_ln = self.ln(x)\n\n        # Apply multi-head self-attention: query, key, value are all x_ln\n        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n\n        # Residual connection: Add input x to attention output\n        attention_value = attention_value + x\n\n        # Feed-forward network with residual connection\n        attention_value = self.ff_self(attention_value) + attention_value\n\n        # Reshape back to original spatial dimensions: [B, C, H, W]\n        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.493151Z","iopub.execute_input":"2025-05-26T22:02:24.493498Z","iopub.status.idle":"2025-05-26T22:02:24.501608Z","shell.execute_reply.started":"2025-05-26T22:02:24.493471Z","shell.execute_reply":"2025-05-26T22:02:24.500485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ğŸ§± **Building Block 1 - The Convolutional Block**\nThe convolutional block is the basic unit of the UNet. It applies convolutional layers, normalization, and activation functions to extract features from the input image. This helps the network learn patterns like edges, textures, and shapes at different levels.\n\n#### **ğŸ§ª Task: Complete the Convolutional Block**\nYour job is to fill in the None values in the code below with the correct PyTorch layers.\n\nThis block is made up of:\n\n* A convolutional layer that changes the number of channels from `in_channels` to `mid_channels`.\n\n* A group normalization layer with num_groups=1 and num_channels=mid_channels.\n\n* A GeLU activation function.\n\n* Another convolutional layer that changes the number of channels from `mid_channels` to `out_channels`.\n\n* A group normalization layer with `num_groups=1` and `num_channels=out_channels`.\n\nYou can use:\n\n* [nn.Conv2d(...)](hhttps://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for convolution layers (this is already provided to you).\n\n* [nn.GroupNorm(...)](https://docs.pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html) for group normalization.\n\n* [nn.GELU()](https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html) for activation.","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, mid_channels=None, residual=False,\n    ):\n        super().__init__()\n        self.residual = residual  # Whether to use residual connection\n\n        # If no mid_channels is provided, set it equal to out_channels\n        if not mid_channels:\n            mid_channels = out_channels\n\n        kernel_size = 3    # Kernel_size of convolutional layer\n        padding = 1        # padding for convolutional layer\n        bias = False\n\n        # Task 0: Define a sequence of two convolutional layers with normalization and GELU activation\n        #### START CODE HERE\n        self.double_conv = nn.Sequential(\n            None,   # Conv. Layer(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n            None,   # A group normalization layer with number of groups = 1, and number channels in input = mid_channels\n            None,   # A GeLU activation\n            None,   # Conv. Layer(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n                                          # Second Conv: mid_channels â†’ out_channels\n            None,   # A group normalization layer with number of groups = 1, and number channels in input = out_channels\n        )\n        ### END CODE HERE\n\n\n    def forward(self, x):\n        # If residual is enabled, add input `x` to the convolutional output before GELU\n        if self.residual:\n            return F.gelu(x + self.double_conv(x))\n        else:\n            return self.double_conv(x)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.503043Z","iopub.execute_input":"2025-05-26T22:02:24.503417Z","iopub.status.idle":"2025-05-26T22:02:24.510907Z","shell.execute_reply.started":"2025-05-26T22:02:24.503386Z","shell.execute_reply":"2025-05-26T22:02:24.510029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# class DoubleConv(nn.Module):\n#     def __init__(\n#         self, in_channels, out_channels, mid_channels=None, residual=False,\n#     ):\n#         super().__init__()\n#         self.residual = residual  # Whether to use residual connection\n\n#         # If no mid_channels is provided, set it equal to out_channels\n#         if not mid_channels:\n#             mid_channels = out_channels\n\n#         # Define a sequence of two convolutional layers with normalization and GELU activation\n#         self.double_conv = nn.Sequential(\n#             nn.Conv2d(\n#                 in_channels, mid_channels, kernel_size=3, padding=1, bias=False\n#             ),                              # First Conv: in_channels â†’ mid_channels\n#             nn.GroupNorm(1, mid_channels),  # Group normalization with 1 group (acts like LayerNorm)\n#             nn.GELU(),                      # GELU activation (smoother than ReLU)\n\n#             nn.Conv2d(\n#                 mid_channels, out_channels, kernel_size=3, padding=1, bias=False\n#             ),                              # Second Conv: mid_channels â†’ out_channels\n#             nn.GroupNorm(1, out_channels),  # Another GroupNorm\n#         )\n\n#     def forward(self, x):\n#         # If residual is enabled, add input `x` to the convolutional output before GELU\n#         if self.residual:\n#             return F.gelu(x + self.double_conv(x))\n#         else:\n#             return self.double_conv(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ğŸ§± **Building Block 2 - The UNet Encoder Block**\n\nIn this section, you will build the **encoder** part of the UNet architecture.\n\nThe encoder is responsible for:\n\n* **Capturing important features** from the input image\n* **Reducing spatial size** (height and width) while increasing the number of channels (depth)\n\nEach encoder block typically includes:\n\n1. A **downsampling step** using `nn.MaxPool2d` to reduce image size by half\n2. One or more **convolutional blocks** to extract and refine features\n   (You will use the `DoubleConv` module you implemented earlier.)\n\nYouâ€™ll stack several of these blocks to form the full encoder. Later, the features will be passed to the decoder through **skip connections** to help the model recover fine details.\n\n\n##### ğŸ§  **Incorporating Embeddings**\n\nTo make the model time-aware and support conditioning (e.g. class-conditional generation), we also include two embedding layers in the encoder block:\n\n1. **Time Embedding Layer**\n   Projects a vector representing the current **diffusion timestep** into a shape that matches the feature map. This helps the model understand *where* in the denoising process it is.\n\n2. **Context Embedding Layer**\n   Projects an optional **class or context label** into the same shape, enabling conditional generation (e.g., generating a specific class of images).\n\nBoth layers use:\n\n* A `SiLU` activation for smooth non-linearity\n* A `Linear` layer to project the input embedding to `out_channels`\n","metadata":{}},{"cell_type":"markdown","source":"#### ğŸ§ª **Task 0: Build the Downsampling Block**\nIn this task, you need to complete the maxpool_conv block by replacing each None with the correct PyTorch layer or block.\n\nThe block should:\n\n* Downsample the input using `nn.MaxPool2d` with a kernel size of 2 (this reduces height and width by half).\n\n* Apply a DoubleConv block with `residual=True` that keeps the same number of channels (in_channels â†’ in_channels).\n\n* Apply another DoubleConv block to change the number of channels from `in_channels` to `out_channels`.\n\nâœ… Use the provided `DoubleConv` module. Just focus on filling in the right values for each block.\n\n---\n\n#### ğŸ§ª **Task 1: Build the Time Embedding Layer**\n\nThis layer takes in the **time step embedding** and projects it into the same shape as the output channels of your block.\n\nYour job is to:\n\n1. Add a [`nn.SiLU()`](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html) activation function (a smoother version of ReLU)\n2. Add a [`nn.Linear(...)`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer that projects from `time_emb_dim` â†’ `out_channels`\n\n> âœ… This helps inject time-related information into the model.\n\n---\n\n#### ğŸ§ª **Task 2: Build the Context Embedding Layer**\n\nThis layer handles the **context or class label embedding** (e.g., for conditional generation).\n\nYour job is to:\n\n1. Add a [`nn.SiLU()`](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html) activation\n2. Add a [`nn.Linear(...)`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer to project from `context_emb_dim` â†’ `out_channels`\n\n> âœ… This allows the model to adapt its behavior based on class or conditioning input.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim, context_emb_dim=N_CLASSES):\n        super().__init__()\n\n        # Task 0: Build a downsampling block\n        #### START CODE HERE\n        self.maxpool_conv = nn.Sequential(\n            None,  # Replace with a MaxPool2d layer that downsamples by a factor of 2\n            None,  # Replace with a DoubleConv block with residual=True and same in/out channels\n            None,  # Replace with a DoubleConv block from in_channels to out_channels (no residual)\n        )\n        #### END CODE HERE\n\n        # Task 1: Linear layer to embed the time step into a feature map with 'out_channels' dimensions\n        #### START CODE HERE\n        self.time_emb_layer = nn.Sequential(\n            None,  # Activation function (Sigmoid-weighted ReLU)\n            None,  # A Linear layer with in_features=time_emb_dim and out_features=out_channels\n        )\n        #### END CODE HERE\n\n        # Task 2: Linear layer to embed the class/context label into the same shape\n        #### START CODE HERE\n        self.context_emb_layer = nn.Sequential(\n            None,   # Activation function (Sigmoid-weighted ReLU)\n            None,  # A Linear layer with in_features=context_emb_dim and out_features=out_channels\n        )\n        #### END CODE HERE\n        \n        \n\n    def forward(self, x, y, t):\n        \"\"\"\n        Args:\n            x: Input feature map (B, C, H, W)\n            y: Context embedding input (e.g., class label vector) â†’ shape (B, context_emb_dim)\n            t: Time embedding input (e.g., timestep vector) â†’ shape (B, time_emb_dim)\n\n        Returns:\n            A feature map with both time and context conditioning applied.\n        \"\"\"\n\n        #### DO NOT EDIT THIS CODE\n        # Apply downsampling and convolutional operations\n        x = self.maxpool_conv(x)  # Output shape: (B, out_channels, H/2, W/2)\n\n        # Process time embedding and reshape to broadcast over spatial dimensions\n        t_emb = self.time_emb_layer(t)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n        \n        # Process context (e.g., class embedding) and reshape to broadcast\n        y_emb = self.context_emb_layer(y)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n        #### DO NOT EDIT THIS CODE\n\n        #TASK 3: Apply conditioning - modulate x with y_emb (multiplicative) and t_emb (additive)\n        ### START CODE HERE\n        conditioned_x = None \n        ### END CODE HERE\n\n        return conditioned_x\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T19:49:18.227047Z","iopub.execute_input":"2025-06-16T19:49:18.228360Z","iopub.status.idle":"2025-06-16T19:49:18.238045Z","shell.execute_reply.started":"2025-06-16T19:49:18.228307Z","shell.execute_reply":"2025-06-16T19:49:18.237096Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class Down(nn.Module):\n#     def __init__(self, in_channels, out_channels, time_emb_dim, context_emb_dim=N_CLASSES):\n#         super().__init__()\n\n       \n#         self.maxpool_conv = nn.Sequential(\n#             None,  # Replace with a MaxPool2d layer that downsamples by a factor of 2\n#             None,  # Replace with a DoubleConv block with residual=True and same in/out channels\n#             None,  # Replace with a DoubleConv block from in_channels to out_channels (no residual)\n#         )\n\n#         # Linear layer to embed the time step into a feature map with 'out_channels' dimensions\n#         self.time_emb_layer = nn.Sequential(\n#             nn.SiLU(),  # Activation function (Sigmoid-weighted ReLU)\n#             nn.Linear(time_emb_dim, out_channels),  # Time embedding projection\n#         )\n\n#         # Linear layer to embed the class/context label into the same shape\n#         self.context_emb_layer = nn.Sequential(\n#             nn.SiLU(),  # Activation function\n#             nn.Linear(context_emb_dim, out_channels),  # Context embedding projection\n#         )\n\n#     def forward(self, x, y, t):\n#         \"\"\"\n#         Args:\n#             x: Input feature map (B, C, H, W)\n#             y: Context embedding input (e.g., class label vector) â†’ shape (B, context_emb_dim)\n#             t: Time embedding input (e.g., timestep vector) â†’ shape (B, time_emb_dim)\n\n#         Returns:\n#             A feature map with both time and context conditioning applied.\n#         \"\"\"\n\n#         # Apply downsampling and convolutional operations\n#         x = self.maxpool_conv(x)  # Output shape: (B, out_channels, H/2, W/2)\n\n#         # Process time embedding and reshape to broadcast over spatial dimensions\n#         t_emb = self.time_emb_layer(t)[:, :, None, None].repeat(\n#             1, 1, x.shape[-2], x.shape[-1]\n#         )\n\n#         # Process context (e.g., class embedding) and reshape to broadcast\n#         y_emb = self.context_emb_layer(y)[:, :, None, None].repeat(\n#             1, 1, x.shape[-2], x.shape[-1]\n#         )\n\n#         #TASK 0: Apply conditioning - modulate x with y_emb (multiplicative) and t_emb (additive)\n#         ### START CODE HERE\n#         conditioned_x = None\n#         ### END CODE HERE\n\n#         return conditioned_x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **ğŸ§± Building Block 2 - The UNet Decoder Block**\n\nIn this section, you will build the **decoder** part of the UNet architecture.\n\nRemember that the decoder is responsible for:\n\n* **Upsampling** the feature maps to recover the original image size\n* **Combining** the upsampled features with the corresponding features from the encoder via skip connections\n* **Refining** the combined features through convolutional layers\n\nEach decoder block usually has:\n\n* An **upsampling step** (using interpolation or transposed convolution)\n* A **concatenation** step to merge encoder features with upsampled features\n* One or more **convolutional blocks** (like `DoubleConv`) to process the combined features\n\nThis helps the network recover spatial details while still using the rich features learned during encoding.\n","metadata":{}},{"cell_type":"markdown","source":"#### ğŸ§ª **Task: Build the Upsampling Block**\n\nIn this task, you will build a decoder block that includes upsampling and feature refinement.\n\nThe block should:\n\n1. **Upsample** the input using `nn.Upsample(...)` method\n2. **Concatenate** the upsampled feature map with the corresponding encoder feature map (you will handle this in the `forward()` method)\n3. Apply a `DoubleConv` block to combine and refine the features\n\nâœ… Use the provided `DoubleConv` module and make sure your channels match when concatenating.","metadata":{}},{"cell_type":"code","source":"class Up(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim, context_emb_dim=N_CLASSES):\n        super().__init__()\n\n        # Upsampling operation to increase spatial resolution by a factor of 2\n        self.up = nn.Upsample(\n            scale_factor=2,\n            mode=\"bilinear\",        # Bilinear interpolation for smoother upscaling\n            align_corners=True,\n        )\n\n        # Double convolution block after upsampling and skip connection concatenation\n        self.conv = nn.Sequential(\n            DoubleConv(in_channels, in_channels, residual=True),       # Residual double conv\n            DoubleConv(in_channels, out_channels, in_channels // 2),   # Standard double conv\n        )\n\n        # Project time embedding to match the number of output channels\n        self.time_emb_layer = nn.Sequential(\n            nn.SiLU(),                      # Activation function\n            nn.Linear(time_emb_dim, out_channels),  # Map time embedding â†’ channel space\n        )\n\n        # Project context (e.g. class label) embedding to output channels\n        self.context_emb_layer = nn.Sequential(\n            nn.SiLU(),                             # Activation\n            nn.Linear(context_emb_dim, out_channels),  # Map context â†’ channel space\n        )\n\n    def forward(self, x, skip_x, y, t):\n        \"\"\"\n        Forward pass of the Up block.\n\n        Args:\n            x (Tensor): Input feature map from the previous layer [B, C, H, W]\n            skip_x (Tensor): Feature map from the encoder for skip connection [B, C_skip, H, W]\n            y (Tensor): Context embedding [B, context_emb_dim]\n            t (Tensor): Time embedding [B, time_emb_dim]\n\n        Returns:\n            Tensor: Output feature map with conditioning applied\n        \"\"\"\n\n        #### START CODE HERE\n        # Task 1: Upsample the input\n        x = self.up(None) # Pass the input through the upsampling layer. Replace None with the  appropraite input.\n\n        # Task 2: Concatenate the skip connection from encoder (along channel dimension)\n        x = torch.cat([None, None], dim=1)  # Concatenate the output of the upsampling layer with skip_X. Replace None and None with the two features we want to concatenate\n\n        # Task 3: Apply convolutional layers after the concatenation operation\n        x = self.conv(x)\n        #### END CODE HERE\n\n        #### DO NOT EDIT THIS CODE\n        # Expand time embedding to spatial dimensions and match channels\n        t_emb = self.time_emb_layer(t)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n        \n        # Expand context embedding to spatial dimensions\n        y_emb = self.context_emb_layer(y)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n        ### DO NOT EDIT THIS CODE\n\n        # Apply context as multiplicative modulation, and time as additive bias\n        return (y_emb * x) + t_emb\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.522229Z","iopub.execute_input":"2025-05-26T22:02:24.522610Z","iopub.status.idle":"2025-05-26T22:02:24.532328Z","shell.execute_reply.started":"2025-05-26T22:02:24.522579Z","shell.execute_reply":"2025-05-26T22:02:24.531050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class Up(nn.Module):\n#     def __init__(self, in_channels, out_channels, time_emb_dim, context_emb_dim=N_CLASSES):\n#         super().__init__()\n\n#         # Upsampling operation to increase spatial resolution by a factor of 2\n#         self.up = nn.Upsample(\n#             scale_factor=2,\n#             mode=\"bilinear\",        # Bilinear interpolation for smoother upscaling\n#             align_corners=True,\n#         )\n\n#         # Double convolution block after upsampling and skip connection concatenation\n#         self.conv = nn.Sequential(\n#             DoubleConv(in_channels, in_channels, residual=True),       # Residual double conv\n#             DoubleConv(in_channels, out_channels, in_channels // 2),   # Standard double conv\n#         )\n\n#         # Project time embedding to match the number of output channels\n#         self.time_emb_layer = nn.Sequential(\n#             nn.SiLU(),                      # Activation function\n#             nn.Linear(time_emb_dim, out_channels),  # Map time embedding â†’ channel space\n#         )\n\n#         # Project context (e.g. class label) embedding to output channels\n#         self.context_emb_layer = nn.Sequential(\n#             nn.SiLU(),                             # Activation\n#             nn.Linear(context_emb_dim, out_channels),  # Map context â†’ channel space\n#         )\n\n#     def forward(self, x, skip_x, y, t):\n#         \"\"\"\n#         Forward pass of the Up block.\n\n#         Args:\n#             x (Tensor): Input feature map from the previous layer [B, C, H, W]\n#             skip_x (Tensor): Feature map from the encoder for skip connection [B, C_skip, H, W]\n#             y (Tensor): Context embedding [B, context_emb_dim]\n#             t (Tensor): Time embedding [B, time_emb_dim]\n\n#         Returns:\n#             Tensor: Output feature map with conditioning applied\n#         \"\"\"\n\n#         # Upsample the input\n#         x = self.up(x)\n\n#         # Concatenate the skip connection from encoder (along channel dimension)\n#         x = torch.cat([skip_x, x], dim=1)  # Shape: [B, C_skip + C, H, W]\n\n#         # Apply convolutional layers\n#         x = self.conv(x)\n\n#         # Expand time embedding to spatial dimensions and match channels\n#         t_emb = self.time_emb_layer(t)[:, :, None, None].repeat(\n#             1, 1, x.shape[-2], x.shape[-1]\n#         )\n\n#         # Expand context embedding to spatial dimensions\n#         y_emb = self.context_emb_layer(y)[:, :, None, None].repeat(\n#             1, 1, x.shape[-2], x.shape[-1]\n#         )\n\n#         # Apply context as multiplicative modulation, and time as additive bias\n#         return (y_emb * x) + t_emb\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(\n        self,\n        channels,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    ):\n        super().__init__()\n        self.channels = channels  # Dimensionality of the embedding vector\n        self.device = device      # Device to place tensors on (e.g., \"cuda\" or \"cpu\")\n\n    def forward(self, time):\n        \"\"\"\n        Converts a scalar time tensor into a sinusoidal positional embedding.\n\n        Args:\n            time (Tensor): A tensor of shape [B, 1] or [B] representing timesteps.\n\n        Returns:\n            Tensor: A positional embedding of shape [B, channels]\n        \"\"\"\n\n        # Compute inverse frequencies for sinusoidal functions:\n        # inv_freq = [1/10000^(2i/channels) for i in (0, 2, 4, ...)]\n        inv_freq = 1.0 / (\n            10000 ** (\n                torch.arange(0, self.channels, 2, device=self.device).float() / self.channels\n            )\n        )\n\n        # Make sure time has shape [B, 1] for broadcasting\n        time = time.unsqueeze(1) if time.dim() == 1 else time\n\n        # Repeat time values to match the number of frequencies\n        # Apply sin to even indices, cos to odd indices\n        pos_enc_a = torch.sin(time * inv_freq)  # shape: [B, channels//2]\n        pos_enc_b = torch.cos(time * inv_freq)  # shape: [B, channels//2]\n\n        # Concatenate sin and cos components to get the full positional embedding\n        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)  # shape: [B, channels]\n\n        return pos_enc\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.533414Z","iopub.execute_input":"2025-05-26T22:02:24.533689Z","iopub.status.idle":"2025-05-26T22:02:24.541406Z","shell.execute_reply.started":"2025-05-26T22:02:24.533670Z","shell.execute_reply":"2025-05-26T22:02:24.540433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸ”§ Putting it ALL Together â€“ The Full UNet Architecture\n\nNow that you've built all the essential parts of the UNetâ€”downsampling blocks, upsampling blocks, self-attention, and embedding layersâ€”it's time to connect everything in the `forward()` method.\n\nThis is where your model receives an input image, along with its time step and optional context label, and processes it through the full UNet pipeline:\n\n#### The forward pass consists of:\n\n* **Downsampling path:**\n\n  * Applies initial convolution (`self.input_conv`)\n  * Then passes the data through a series of downsampling blocks (`self.down1`, `self.down2`, `self.down3`)\n  * After each downsampling, a self-attention layer (`self.sa1`, `self.sa2`, `self.sa3`) helps refine features\n\n* **Bottleneck (bridge):**\n\n  * Further processes the deepest features with three convolutional layers (`self.bridge1`, `self.bridge2`, `self.bridge3`)\n\n* **Upsampling path:**\n\n  * Gradually reconstructs the image using `self.up1`, `self.up2`, and `self.up3`\n  * Each upsampling step is followed by its own self-attention layer (`self.sa4`, `self.sa5`, `self.sa6`)\n  * Each upsampling also receives skip connections from earlier encoder blocks (`x3`, `x2`, `x1`)\n\nYour job is to complete this forward pass by filling in each `None` with the correct operations using the modules defined in your model.\n\nLetâ€™s bring everything together!","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(\n        self,\n        c_in=N_CHANNEL,                          # Number of input channels (e.g., 3 for RGB)\n        c_out=N_CHANNEL,                         # Number of output channels\n        time_dim=256 // SCALE_DOWN,              # Dimensionality of time embeddings\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ):\n        super().__init__()\n\n        # Initial convolutional block\n        self.input_conv = DoubleConv(c_in, 64 // SCALE_DOWN)\n\n        # Encoder (Downsampling path)\n        self.down1 = Down(64 // SCALE_DOWN, 128 // SCALE_DOWN, time_dim)\n        self.sa1 = SelfAttention(128 // SCALE_DOWN, IMAGE_SIZE // 2)  # Self-attention after down1\n\n        self.down2 = Down(128 // SCALE_DOWN, 256 // SCALE_DOWN, time_dim)\n        self.sa2 = SelfAttention(256 // SCALE_DOWN, IMAGE_SIZE // 4)\n\n        self.down3 = Down(256 // SCALE_DOWN, 256 // SCALE_DOWN, time_dim)\n        self.sa3 = SelfAttention(256 // SCALE_DOWN, IMAGE_SIZE // 8)\n\n        # Bridge (bottleneck)\n        self.bridge1 = DoubleConv(256 // SCALE_DOWN, 512 // SCALE_DOWN)\n        self.bridge2 = DoubleConv(512 // SCALE_DOWN, 512 // SCALE_DOWN)\n        self.bridge3 = DoubleConv(512 // SCALE_DOWN, 256 // SCALE_DOWN)\n\n        # Decoder (Upsampling path)\n        self.up1 = Up(512 // SCALE_DOWN, 128 // SCALE_DOWN, time_dim)\n        self.sa4 = SelfAttention(128 // SCALE_DOWN, IMAGE_SIZE // 4)\n\n        self.up2 = Up(256 // SCALE_DOWN, 64 // SCALE_DOWN, time_dim)\n        self.sa5 = SelfAttention(64 // SCALE_DOWN, IMAGE_SIZE // 2)\n\n        self.up3 = Up(128 // SCALE_DOWN, 64 // SCALE_DOWN, time_dim)\n        self.sa6 = SelfAttention(64 // SCALE_DOWN, IMAGE_SIZE)\n\n        # Final convolution to produce output image\n        self.out_conv = nn.Conv2d(64 // SCALE_DOWN, c_out, kernel_size=1)\n\n        # Time positional encoding (used to condition the model on timestep `t`)\n        self.pos_encoding = PositionalEmbedding(time_dim, device)\n\n    def forward(self, x, y, t):\n        \"\"\"\n        Forward pass of the conditional U-Net.\n\n        Args:\n            x: Input image tensor (B, C, H, W)\n            y: Class embedding or one-hot vector (B, N_CLASSES)\n            t: Time step tensor (B)\n\n        Returns:\n            Tensor: Output image tensor (B, C_out, H, W)\n        \"\"\"\n        # Expand and encode the time embedding\n        t = t.unsqueeze(-1).type(torch.float)     # Ensure shape [B, 1]\n        t = self.pos_encoding(t)                  # Get time embedding [B, time_dim]\n\n        # Downsampling path\n        x1 = self.input_conv(x)                   # Initial conv\n        x2 = self.down1(x1, y, t)\n        x2 = self.sa1(x2)\n\n        x3 = self.down2(x2, y, t)\n        x3 = self.sa2(x3)\n\n        x4 = self.down3(x3, y, t)\n        x4 = self.sa3(x4)\n\n        # Bottleneck (bridge)\n        x4 = self.bridge1(x4)\n        x4 = self.bridge2(x4)\n        x4 = self.bridge3(x4)\n\n        # Upsampling path with skip connections\n        x = self.up1(x4, x3, y, t)\n        x = self.sa4(x)\n\n        x = self.up2(x, x2, y, t)\n        x = self.sa5(x)\n\n        x = self.up3(x, x1, y, t)\n        x = self.sa6(x)\n\n        # Final output layer\n        output = self.out_conv(x)\n\n        return output\n\n        # Task 1: \n        #### START CODE HERE\n       \n        #### END CODE HERE ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.542323Z","iopub.execute_input":"2025-05-26T22:02:24.542686Z","iopub.status.idle":"2025-05-26T22:02:24.554949Z","shell.execute_reply.started":"2025-05-26T22:02:24.542655Z","shell.execute_reply":"2025-05-26T22:02:24.553950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BACKWARD = UNet","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.555891Z","iopub.execute_input":"2025-05-26T22:02:24.556114Z","iopub.status.idle":"2025-05-26T22:02:24.560028Z","shell.execute_reply.started":"2025-05-26T22:02:24.556097Z","shell.execute_reply":"2025-05-26T22:02:24.559158Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1747758141286,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"f8Y-sm8FmlrI","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Wrapper**","metadata":{"id":"tn_nP8Uqmi6b","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"class DiffusionModel(L.LightningModule):\n    def __init__(self, forward_model, backward_model, batch_size, lr, max_epoch):\n        super().__init__()\n        \n        # Diffusion models\n        self.forward_model = forward_model  # Noise scheduler\n        self.backward_model = backward_model  # Denoising UNet\n\n        # Training parameters\n        self.batch_size = batch_size\n        self.lr = lr\n        self.max_epoch = max_epoch\n\n        # Manual optimization\n        self.automatic_optimization = False\n\n        # Metrics\n        self.model_loss = []\n        self.val_fid = []\n\n        self.model_loss_recorder = AvgMeter()\n        self.val_fid_recorder = AvgMeter()\n\n        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._T = self.forward_model.T\n\n        # FID computation\n        self.fid = FrechetInceptionDistance(\n            feature=64,\n            input_img_size=(N_CHANNEL, IMAGE_SIZE, IMAGE_SIZE),\n            normalize=True\n        )\n\n    def forward(self, x=None, y=None, t=None):\n        # Training: return MSE loss between predicted and true noise\n        if self.training:\n            x_noisy, noise = self.forward_model(x, t)\n            noise_pred = self.backward_model(x_noisy, y, t)\n            return F.mse_loss(noise, noise_pred)\n        else:\n            # Sampling mode during validation or inference\n            return self.sample(progress=False, verbose=True)\n\n    def sample(self, n=1, context=None, progress=False, verbose=False, n_progress=5):\n        self.backward_model.eval()\n\n        # If no context is provided, randomly generate one-hot class context\n        context = F.one_hot(torch.tensor(np.random.randint(0, 2), dtype=torch.int64),\n                            num_classes=N_CLASSES).float() if context is None else context\n\n        progress_image = [] if progress else None\n\n        with torch.no_grad():\n            x = torch.randn((n, N_CHANNEL, IMAGE_SIZE, IMAGE_SIZE)).to(self.device)\n\n            if progress:\n                progress_image.append(x.detach().cpu())\n\n            iteration = tqdm(reversed(range(0, self._T)), position=0) if verbose else reversed(range(0, self._T))\n\n            for i in iteration:\n                t = (torch.ones(n) * i).long().to(self.device)\n\n                noise_pred = self.backward_model(x, context, t)\n\n                alpha = self.forward_model.alpha[t][:, None, None, None]\n                alpha_hat = self.forward_model.alpha_hat[t][:, None, None, None]\n                beta = self.forward_model.beta[t][:, None, None, None]\n\n                noise = torch.randn_like(x) if i else torch.zeros_like(x)\n\n                # Reverse process (denoising step)\n                x = (\n                    1 / torch.sqrt(alpha) *\n                    (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * noise_pred) +\n                    torch.sqrt(beta) * noise\n                )\n                x = torch.clamp(x, -1.0, 1.0)\n\n                if progress and i % (self._T // n_progress) == 0:\n                    progress_image.append(x.detach().cpu())\n\n            if progress:\n                progress_image.pop(n_progress // 2)\n                return progress_image\n\n        return x\n\n    def on_train_epoch_start(self):\n        self.fid.reset()\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n\n        self.fid.update((x + 1.0) / 2.0, real=True)\n\n        t = torch.randint(0, self._T, (self.batch_size,), device=self._device).long()\n        loss = self(x, y, t)\n\n        opt = self.optimizers()\n        opt.zero_grad()\n        self.manual_backward(loss)\n        opt.step()\n\n        self.log(\"model_loss\", loss, prog_bar=True)\n        self.model_loss_recorder.update(loss.data)\n\n    def on_train_epoch_end(self):\n        self.model_loss.append(self.model_loss_recorder.show().data.cpu().numpy())\n        self.model_loss_recorder = AvgMeter()\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        self.fid.update((x + 1.0) / 2.0, real=True)\n\n        _x = self.sample(x.shape[0], y)\n        self.fid.update((_x + 1.0) / 2.0, real=False)\n\n        fid = self.fid.compute().data.cpu()\n        self.log(\"val_fid\", fid, prog_bar=True)\n        self.val_fid_recorder.update(fid)\n\n    def on_validation_epoch_end(self):\n        self.val_fid.append(self.val_fid_recorder.show().data.cpu().numpy())\n        self.val_fid_recorder = AvgMeter()\n\n    def test_step(self, batch, batch_nb):\n        x, y = batch\n        self.fid.update((x + 1.0) / 2.0, real=True)\n\n        _x = self.sample(x.shape[0], y)\n        self.fid.update((_x + 1.0) / 2.0, real=False)\n\n    def on_test_epoch_end(self):\n        fid = self.fid.compute().data.cpu()\n        self.log(\"test_fid\", fid, prog_bar=False, logger=True)\n\n    def on_train_end(self):\n        # Plot training loss\n        loss_img_file = f\"/content/{MODEL_NAME}_loss_plot.png\"\n        plt.plot(self.model_loss, color=\"r\")\n        plt.title(\"Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.grid()\n        plt.savefig(loss_img_file)\n        plt.clf()\n        img = cv2.imread(loss_img_file)\n        cv2_imshow(img)\n\n        # Plot FID metric over epochs\n        evaluation_metric_img_file = f\"/content/{MODEL_NAME}_fid_plot.png\"\n        plt.plot(self.val_fid[1:], color=\"b\")\n        plt.title(\"FID Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"FID\")\n        plt.grid()\n        plt.savefig(evaluation_metric_img_file)\n        plt.clf()\n        img = cv2.imread(evaluation_metric_img_file)\n        cv2_imshow(img)\n\n    def train_dataloader(self):\n        return data.DataLoader(\n            dataset=TrainDataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            drop_last=True\n        )\n\n    def val_dataloader(self):\n        return data.DataLoader(\n            dataset=ValDataset,\n            batch_size=2,\n            shuffle=True,\n            drop_last=True\n        )\n\n    def test_dataloader(self):\n        return data.DataLoader(\n            dataset=TestDataset,\n            batch_size=2,\n            shuffle=True,\n            drop_last=True\n        )\n\n    def configure_optimizers(self):\n        return [optim.AdamW(self.parameters(), lr=self.lr)]\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:09:06.989588Z","iopub.execute_input":"2025-05-26T22:09:06.989938Z","iopub.status.idle":"2025-05-26T22:09:07.014933Z","shell.execute_reply.started":"2025-05-26T22:09:06.989912Z","shell.execute_reply":"2025-05-26T22:09:07.013933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = DiffusionModel.__name__","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:09:10.494550Z","iopub.execute_input":"2025-05-26T22:09:10.495296Z","iopub.status.idle":"2025-05-26T22:09:10.499232Z","shell.execute_reply.started":"2025-05-26T22:09:10.495269Z","shell.execute_reply":"2025-05-26T22:09:10.498439Z"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1747758142802,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"9cngPQKTtyGV","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Task 1: Replace with the appropraiting\n### START CODE HERE\nforward_model = None # Replace None with your code \nbackward_model = None # Replace None with your code \n### END CODE HERE","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"load_pretrained = False  # Set to True to load from a saved checkpoint\ncheckpoint_path = None\n\nif load_pretrained:\n    print(\"Loading pre-weights...\")\n\n    # Load the pretrained model from a checkpoint file using PyTorch Lightning\n    model = DiffusionModel.load_from_checkpoint(\n        checkpoint_path=checkpoint_path,  # Path to the saved checkpoint\n\n        # Map the model to the appropriate device (GPU if available, else CPU)\n        map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n\n        # Required arguments for rebuilding the model instance (must match constructor)\n        forward_model=FORWARD(),         # Forward diffusion scheduler\n        backward_model=BACKWARD(),       # Backward (UNet) model\n        batch_size=BATCH_SIZE,           # Batch size during training/inference\n        lr=LR,                           # Learning rate\n        max_epoch=MAX_EPOCH,             # Total training epochs\n    )\nelse:\n    # Fresh initialization of the model from scratch\n    model = DiffusionModel(FORWARD(), BACKWARD(), 32, LR, MAX_EPOCH)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:09:11.802832Z","iopub.execute_input":"2025-05-26T22:09:11.803147Z","iopub.status.idle":"2025-05-26T22:09:12.429342Z","shell.execute_reply.started":"2025-05-26T22:09:11.803129Z","shell.execute_reply":"2025-05-26T22:09:12.428423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Training**","metadata":{"id":"boRJQCOJdFXa","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Define a checkpoint callback to save the best model based on validation FID\ncheckpoint = ModelCheckpoint(\n    monitor='val_fid',            # Metric to monitor\n    dirpath=CHECKPOINT_DIR,       # Directory to save checkpoint files\n    mode='min',                   # Lower FID = better\n)\n\n# Create a Lightning Trainer object with desired settings\ntrainer = Trainer(\n    accelerator=\"auto\",           # Automatically choose GPU/CPU\n    devices=1,                    # Use 1 GPU or 1 CPU\n    max_epochs=30,                # Total training epochs\n    callbacks=[checkpoint],       # Use ModelCheckpoint callback\n    log_every_n_steps=5,          # Log metrics every 5 steps\n    check_val_every_n_epoch=5,    # Run validation every 5 epochs\n)\n\n# Begin training the model\ntrainer.fit(model)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:09:20.367595Z","iopub.execute_input":"2025-05-26T22:09:20.368105Z","execution_failed":"2025-05-27T05:34:04.277Z"},"id":"96vkBWERjAwH","outputId":"0824364f-0cf2-4a95-c210-ab8d220e7daf","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Testing**","metadata":{"id":"rx7WK_KxdFhM","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Rename the best model checkpoint to a custom file name for easier identification\nos.rename(\n    checkpoint.best_model_path,                                     # Original path (auto-named by PyTorch Lightning)\n    os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")         # New path with custom name\n)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:06:31.479855Z","iopub.status.idle":"2025-05-26T22:06:31.480135Z","shell.execute_reply.started":"2025-05-26T22:06:31.480011Z","shell.execute_reply":"2025-05-26T22:06:31.480022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set model to evaluation mode to disable dropout, batchnorm updates, etc.\nmodel.eval()\n\n# Run the test step using the best saved checkpoint\ntrainer.test(\n    ckpt_path=os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")\n)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:06:31.481181Z","iopub.status.idle":"2025-05-26T22:06:31.481524Z","shell.execute_reply.started":"2025-05-26T22:06:31.481388Z","shell.execute_reply":"2025-05-26T22:06:31.481403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Inference**","metadata":{"id":"JfvehFoAiW16","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"### **Utils**","metadata":{"id":"9U0lYLLUR23C","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"CHECKPOINT_INFERENCE = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:06:31.482167Z","iopub.status.idle":"2025-05-26T22:06:31.482571Z","shell.execute_reply.started":"2025-05-26T22:06:31.482376Z","shell.execute_reply":"2025-05-26T22:06:31.482394Z"},"id":"ndWwtFfmJZ_F","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_SAMPLE = 5","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:06:31.483383Z","iopub.status.idle":"2025-05-26T22:06:31.483762Z","shell.execute_reply.started":"2025-05-26T22:06:31.483563Z","shell.execute_reply":"2025-05-26T22:06:31.483582Z"},"id":"srJUwJm8ktjs","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the pretrained diffusion model from a checkpoint\nmodel = DiffusionModel.load_from_checkpoint(\n    checkpoint_path=CHECKPOINT_INFERENCE,  # Path to your saved .ckpt file\n    map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\",  # Load onto the correct device\n    forward_model=FORWARD(),         # Reconstruct the forward noise scheduler\n    backward_model=BACKWARD(),       # Reconstruct the backward UNet\n    batch_size=BATCH_SIZE,           # Must match what was used during training\n    lr=LR,                           # Learning rate used at training\n    max_epoch=MAX_EPOCH              # Total epochs used in training\n)\n\n# Set model to evaluation mode to disable training-specific behavior (e.g., dropout)\nmodel.eval()\n\nprint(\"done\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:06:31.485953Z","iopub.status.idle":"2025-05-26T22:06:31.486250Z","shell.execute_reply.started":"2025-05-26T22:06:31.486119Z","shell.execute_reply":"2025-05-26T22:06:31.486132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize","metadata":{"id":"ym6UVoVoR6NE","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"samples_per_class = 10  # Number of samples to generate per class\n\nfor class_id in range(0, 2):  # Loop through all classes (e.g., 0 and 1)\n    # Create one-hot class label and move to CUDA\n    class_label = F.one_hot(torch.tensor(class_id, dtype=torch.int64), num_classes=N_CLASSES).float().to(\"cuda\")\n    class_label = class_label.unsqueeze(0)  # Shape: [1, N_CLASSES]\n    \n    print(f\"\\nClass {class_id} one-hot:\", class_label)\n\n    model.eval()  # Ensure model is in inference mode\n\n    for sample_idx in range(samples_per_class):\n        plt.figure(figsize=(5, 5))  # Set figure size for clarity\n\n        # Generate 1 sample with the given class condition\n        generated = model.sample(\n            n=1,\n            context=class_label,\n            progress=False,\n            verbose=False,\n            n_progress=5\n        )\n\n        # Extract image from batch (shape: [1, C, H, W]) â†’ [C, H, W]\n        image = generated[0] if isinstance(generated, list) else generated[0, :, :, :]\n\n        # Show the generated image\n        show_tensor_image(image)","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:06:31.487163Z","iopub.status.idle":"2025-05-26T22:06:31.487494Z","shell.execute_reply.started":"2025-05-26T22:06:31.487310Z","shell.execute_reply":"2025-05-26T22:06:31.487321Z"},"id":"e9STip-Oh5sY","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}